{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69263294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import pickle, functools, operator\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import random #shuffle\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729ab05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d4ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CMIPdata(Xdata, Ydata, out , myform):\n",
    "    inp1 = Dataset(Xdata,'r')\n",
    "    inp2 = Dataset(Ydata,'r')\n",
    "    sst_1 = np.zeros((myform,140,12,24,72))\n",
    "\n",
    "    #首年序列\n",
    "    #i= 21个模式\n",
    "    for i in range(myform):\n",
    "        sst_1[i,:,:,:,:] = inp1.variables['sst1'][1+141*i:141+141*i,0:12,:,:]\n",
    "    #(21,140,12,24,72)\n",
    "    #首年序列\n",
    "\n",
    "    \n",
    "    #flatted扁平化                                             \n",
    "    sst_2 = np.zeros((myform,1680,24,72))\n",
    "    for i in range(myform):\n",
    "        for j in range(140):\n",
    "            sst_2[i,j*12:(j+1)*12,:,:] = sst_1[i,j,:,:,:]\n",
    "    \n",
    "        #(21,1680,24,72)\n",
    "        #每个模式 139*12+1 = 1680 个序列 序列长度为12\n",
    "\n",
    "    #丢弃1个月 1862.1\n",
    "    sst_2 = sst_2[:,1:,:,:]\n",
    "    #(form,1679,24,72)\n",
    "\n",
    "\n",
    "    winsize = 12\n",
    "    #winnum = 1679-winsize +1 #1668\n",
    "    \"\"\"跳跃六个月滑窗 = 6\"\"\"\n",
    "    ts=6 \n",
    "    #(1679-12)/6+1 =27\n",
    "    winnum=int((1679-12)/ts +1) \n",
    "    \n",
    "    sst_3 = np.zeros((myform,winnum,12,24,72))\n",
    "    for i in range(myform):\n",
    "        for j in range(winnum):\n",
    "            #print(j)\n",
    "            sst_3[i,j,:,:,:] = sst_2[i,(j*ts):(j*ts)+winsize,:,:]\n",
    "            \n",
    "\n",
    "    sst_4 = np.zeros((int(winnum*myform),12,24,72))\n",
    "    #t300_4 = np.zeros((int(winnum*myform),12,24,72))\n",
    "    for i in range(myform):\n",
    "        sst_4[i*winnum:(i+1)*winnum,:,:,:] = sst_3[i,:,:,:,:]\n",
    "\n",
    "\n",
    "    #sst_4 = sst_4.swapaxes(1, 3)\n",
    "    trX = sst_4\n",
    "    del sst_1,sst_2,sst_3,sst_4\n",
    "    #trX = trX.reshape(int(winnum*myform),12,1728)\n",
    "\n",
    "    #保存np数组\n",
    "    #(5859, 12, 24, 72, 2)\n",
    "    #np.save(\"./CMIPdata/CMIP_trX_21_ts6_out.npy\",trX) \n",
    "\n",
    "    #label\n",
    "    #平铺\n",
    "    pr_1 = np.zeros(((myform),1692))#21,1692\n",
    "    for i in range(myform):\n",
    "        for j in range(141):\n",
    "            pr_1[i,j*12:(j+1)*12] = inp2.variables['pr'][(141*i)+j,:,0,0]\n",
    "            \n",
    "    #out\n",
    "    pr_2 =  np.zeros((myform,winnum,out))#form,1668,1\n",
    "    for i in range(myform):\n",
    "        for j in range(winnum):\n",
    "            pr_2[i,j,:] = pr_1[i,(j*ts):(j*ts)+out]\n",
    "\n",
    "    trY = np.zeros((int(winnum*myform),out))#form*1668,1\n",
    "    for i in range(myform):\n",
    "        trY[i*winnum:(i+1)*winnum,:] = pr_2[i,:,:]\n",
    "\n",
    "    #np.save(\"./CMIPdata/CMIP_trY_21_ts6_out%s.npy\"%out,trY)\n",
    "    trY_decoder_input = np.zeros((int(winnum*myform),out))\n",
    "    trY_decoder_input[:,1:] = trY[:,:-1]\n",
    "\n",
    "    #return trX[1:,:,:] ,trY.reshape(-1,24,1)[1:,:,:] ,trY_decoder_input\n",
    "    return trX ,trY.reshape(-1,24,1) ,trY_decoder_input.reshape(-1,24,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55004bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GOSDAdata(Xdata, Ydata, out):\n",
    "    #test data\n",
    "    inp11 = Dataset(Xdata,'r')\n",
    "    inp22 = Dataset(Ydata,'r')\n",
    "                                              \n",
    "    sst_11 = np.zeros((33,12,24,72)) #1983-2015\n",
    "    #t300_11 = np.zeros((33,12,24,72))\n",
    "\n",
    "    sst_11[:,:,:,:] = inp11.variables['sst'][3:,0:12,:,:]\n",
    "    #t300_11[:,:,:,:] = inp11.variables['t300'][3:,0:12,:,:]\n",
    "    #(33,12,24,72)\n",
    "\n",
    "    sst_22 = np.zeros((396,24,72))\n",
    "    #t300_22 = np.zeros((396,24,72))\n",
    "\n",
    "    for i in range(33):\n",
    "        sst_22[i*12:(i+1)*12,:,:] = sst_11[i,:,:,:]\n",
    "        #t300_22[i*12:(i+1)*12,:,:] = t300_11[i,:,:,:]\n",
    "    #(396,24,72)\n",
    "    #丢弃一个月，便于制作滑窗序列\n",
    "    sst_22 = sst_22[1:,:,:]\n",
    "    #t300_22 = t300_22[1:,:,:]#(395,24,72)\n",
    "\n",
    "    #滑窗\n",
    "    winsize = 12\n",
    "    winnum = 395-winsize + 1 #384\n",
    "    sst_33 = np.zeros((winnum, winsize, 24, 72))#(384, 12, 24, 72)\n",
    "    #t300_33 = np.zeros((winnum, winsize, 24, 72))\n",
    "\n",
    "    for i in range(winnum):\n",
    "        sst_33[i,:,:,:] = sst_22[i:i+12,:,:]\n",
    "        #t300_33[i,:,:,:] = t300_22[i:i+12,:,:]\n",
    "    #(384,12,24,72)\n",
    "\n",
    "\n",
    "    #channel = 2\n",
    "    #testX = np.zeros((winnum,12,24,72,2))#384\n",
    "    #testX[:,:,:,:,0] = sst_33\n",
    "    #testX[:,:,:,:,1] = t300_33\n",
    "    #sst_33 = sst_33.swapaxes(1, 3)\n",
    "\n",
    "    testX = sst_33#384\n",
    "    #testX[:,:,:,1] = t300_33.reshape(384,12,1728)\n",
    "    del sst_11,sst_22,sst_33\n",
    "    #del t300_11,t300_22,t300_33\n",
    "    #testX = testX.reshape(384,12,3456)\n",
    "\n",
    "    #label\n",
    "    inpv22 = np.zeros((408))#2017-1984 +1=34\n",
    "    for i in range(34):\n",
    "        inpv22[i*12:(i+1)*12] = inp22.variables['pr'][i+2,:,0,0]# +2:从1984开始\n",
    "        #(408)\n",
    "\n",
    "    testY = np.zeros((winnum,out,1))\n",
    "    #out = 1\n",
    "    #滑窗\n",
    "    for i in range(winnum):\n",
    "        testY[i,:,0] = inpv22[i:i+out]#24\n",
    "    #(384,out,1)\n",
    "\n",
    "    return testX, testY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcffbbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trX, trY, trY_decoder_input = CMIPdata('/home/d/Q/saconvlstm/CMIP5.input.36mon.1861_2001.nc',\\\n",
    "                                          '/home/d/Q/saconvlstm/CMIP5.label.12mon.1863_2003.nc', 24,  21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a6d5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX, testY = GOSDAdata('/home/d/Q/saconvlstm/GODAS.input.36mon.1980_2015.nc',\\\n",
    "                         '/home/d/Q/saconvlstm/GODAS.label.12mon.1982_2017.nc', 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf00437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 12, 24, 72)\n",
      "(384, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1604192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5838, 12, 24, 72)\n",
      "(5838, 24, 1)\n",
      "(5838, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "print(trX.shape)\n",
    "print(trY.shape)\n",
    "print(trY_decoder_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8387fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "trX = np.around(trX, decimals=3)\n",
    "trY = np.around(trY, decimals=3)\n",
    "trY_decoder_input = np.around(trY_decoder_input, decimals=3)\n",
    "\n",
    "testX = np.around(testX, decimals=3)\n",
    "testY = np.around(testY, decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73ec4c",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75705169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-05 02:18:41.998814: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-05 02:18:42.018422: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3799900000 Hz\n",
      "2021-11-05 02:18:42.019135: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56257a3b4180 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-05 02:18:42.019161: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "input_cnn = Input(shape=(24, 72,1))\n",
    "\n",
    "x = layers.Conv2D(30, (8,4), activation='tanh', padding='SAME')(input_cnn)\n",
    "x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "x = layers.Conv2D(30, (4,2), activation='tanh', padding='SAME')(x)\n",
    "x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "x = layers.Conv2D(30, (4,2), activation='tanh', padding='SAME')(x)\n",
    "x = tf.reshape(x, [-1, 30 * 18 * 6])\n",
    "outputs_cnn = x\n",
    "model_cnn = Model(input_cnn,outputs_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90541bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = layers.Conv2D(50, (8,4), activation='tanh', padding='SAME')(input_cnn)\n",
    "x1 = tf.nn.max_pool(x1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "x1 = layers.Conv2D(50, (4,2), activation='tanh', padding='SAME')(x1)\n",
    "x1 = tf.nn.max_pool(x1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "x1 = layers.Conv2D(50, (4,2), activation='tanh', padding='SAME')(x1)\n",
    "x1 = tf.reshape(x1, [-1, 50 * 18 * 6])\n",
    "outputs_cnn1 = x1\n",
    "model_cnn1 = Model(input_cnn,outputs_cnn1)\n",
    "#model_cnn1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f335c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = Input(shape=(12, 24, 72))\n",
    "_input1 = tf.reshape(_input, [-1, 24, 72])\n",
    "x = model_cnn(_input1)\n",
    "#x1 = model_cnn1(_input1)\n",
    "output = tf.reshape(x, [-1,12, 30 * 18 * 6])\n",
    "#output1 = tf.reshape(x1, [-1,12, 50 * 18 * 6])\n",
    "#output = tf.concat([output,output1],2)#(None, 12, 6480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee41bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = Input(shape=(72,24,12))\n",
    "x1 = model_cnn(_input[:,:,:,0])\n",
    "x2 = model_cnn(_input[:,:,:,1])\n",
    "x3 = model_cnn(_input[:,:,:,2])\n",
    "x4 = model_cnn(_input[:,:,:,3])\n",
    "x5 = model_cnn(_input[:,:,:,4])\n",
    "x6 = model_cnn(_input[:,:,:,5])\n",
    "x7 = model_cnn(_input[:,:,:,6])\n",
    "x8 = model_cnn(_input[:,:,:,7])\n",
    "x9 = model_cnn(_input[:,:,:,8])\n",
    "x10 = model_cnn(_input[:,:,:,9])\n",
    "x11 = model_cnn(_input[:,:,:,10])\n",
    "x12 = model_cnn(_input[:,:,:,11])\n",
    "\n",
    "x1 = tf.reshape(x1, [-1, 1, 3240])\n",
    "x2 = tf.reshape(x2, [-1, 1, 3240])\n",
    "x3 = tf.reshape(x3, [-1, 1, 3240])\n",
    "x4 = tf.reshape(x4, [-1, 1, 3240])\n",
    "x5 = tf.reshape(x5, [-1, 1, 3240])\n",
    "x6 = tf.reshape(x6, [-1, 1, 3240])\n",
    "x7 = tf.reshape(x7, [-1, 1, 3240])\n",
    "x8 = tf.reshape(x8, [-1, 1, 3240])\n",
    "x9 = tf.reshape(x9, [-1, 1, 3240])\n",
    "x10 = tf.reshape(x10, [-1, 1, 3240])\n",
    "x11 = tf.reshape(x11, [-1, 1, 3240])\n",
    "x12 = tf.reshape(x12, [-1, 1, 3240])\n",
    "\n",
    "output = layers.concatenate([x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12], axis=1)#(None, 12,3240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c81471df",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps_encoder=12\n",
    "#num_encoder_tokens=3456\n",
    "num_encoder_tokens=3240\n",
    "latent_dim=512\n",
    "time_steps_decoder=24\n",
    "#time_steps_decoder=3\n",
    "num_decoder_tokens=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f79f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):# 可以在其中进行所有与输入无关的初始化，定义相关的层\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):# 知道输入张量的形状并可以进行其余的初始化\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        # 为该层创建一个可训练的权重变量。\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):# 在这里进行前向传播\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state \n",
    "                计算单个解码器状态能量的阶跃函数  \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors 整形张量所需的一些参数\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si] 计算 S.Wa\"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
    "            if verbose:\n",
    "                print('wa.s>',W_a_dot_s.shape)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>',U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei 利用ei计算ci的步骤函数 \"\"\"\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        def create_inital_state(inputs, hidden_size):\n",
    "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
    "            # 我们不使用初始状态，但需要传递一些东西给K.rnn函数\n",
    "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
    "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
    "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
    "            return fake_state\n",
    "\n",
    "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
    "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f0a8554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 12, 24, 72)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 24, 72)]     0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "functional_1 (Functional)       (None, 3240)         15450       tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 12, 3240)]   0           functional_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 12, 512), (N 7686144     tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, 12, 512), (N 2099200     lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, 24, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   [(None, 12, 512), (N 2099200     lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 24, 512), (N 1052672     decoder_inputs[0][0]             \n",
      "                                                                 lstm_8[0][1]                     \n",
      "                                                                 lstm_8[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, 24, 512), (N 524800      lstm_8[0][0]                     \n",
      "                                                                 decoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, 24, 1024)     0           decoder_lstm[0][0]               \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 24, 512)      524800      concat_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 24, 1)        513         time_distributed_4[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 14,002,779\n",
      "Trainable params: 14,002,779\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = output\n",
    "\"\"\"后为特征数，前为批次数量！！！\"\"\"\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(encoder_inputs)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder\n",
    "decoder_inputs = Input(shape=(time_steps_decoder, num_decoder_tokens), name= \"decoder_inputs\")\n",
    "# 我们设定我们的解码器回传整个输出的序列同时也回传内部的states参数\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "# 我们使用`encoder_states`来做为初始值(initial state) <-- 重要\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n",
    "\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "decoder_dense = TimeDistributed(Dense(latent_dim, activation ='tanh'))\n",
    "decoder_outputs1 = decoder_dense(decoder_concat_input)\n",
    "\n",
    "decoder_dense2 = TimeDistributed(Dense(num_decoder_tokens))\n",
    "decoder_outputs2 = decoder_dense2(decoder_outputs1)\n",
    "\n",
    "# 定义一个模型接收encoder_input_data` & `decoder_input_data`做为输入而输出`decoder_target_data`\n",
    "model = Model([_input, decoder_inputs], decoder_outputs2)\n",
    "model.summary()\n",
    "#plot_model(model, to_file='model_train.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a89e889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 12, 24, 72)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 24, 72)]     0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "functional_1 (Functional)       (None, 3240)         15450       tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 12, 3240)]   0           functional_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 12, 512), (N 7686144     tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, 12, 512), (N 2099200     lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, 24, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 12, 512), (N 2099200     lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, 24, 512), (N 1052672     decoder_inputs[0][0]             \n",
      "                                                                 lstm_6[0][1]                     \n",
      "                                                                 lstm_6[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, 24, 512), (N 524800      lstm_6[0][0]                     \n",
      "                                                                 lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, 24, 1024)     0           lstm_7[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 24, 512)      524800      concat_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 24, 1)        513         time_distributed_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 14,002,779\n",
      "Trainable params: 14,002,779\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = output\n",
    "# Encoder\n",
    "#encoder_inputs = Input(shape=(time_steps_encoder,num_encoder_tokens))\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(encoder_inputs)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(time_steps_decoder, num_decoder_tokens), name= \"decoder_inputs\")\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(decoder_inputs,initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(latent_dim))\n",
    "decoder_outputs11 = decoder_dense(decoder_concat_input)\n",
    "\n",
    "decoder_dense2 =  TimeDistributed(Dense(1))\n",
    "decoder_outputs = decoder_dense2(decoder_outputs11)\n",
    "\n",
    "# Define the model \n",
    "model = Model([_input, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary() \n",
    "#plot_model(model, to_file='model_train.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43e465c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custome_rmse_fn(y_true, y_pred):\n",
    "    \"\"\" custome loss function\n",
    "    The 24 series is not equally weighted, so log1p weighted is used.\n",
    "    This is just my initial try, still have further improvement space.\n",
    "\n",
    "    y_true: batch * 24\n",
    "    \"\"\" \n",
    "    #y_pred = y_pred.numpy().reshape(-1,24)\n",
    "    #y_true = y_true.numpy().reshape(-1,24)\n",
    "    diff = (y_pred - y_true) ** 2\n",
    "    predict_sequence_length = tf.shape(y_true)[-1]#获取序列长度\n",
    "    alpha = [np.log1p(i) for i in range(1, time_steps_decoder+1)]\n",
    "    #alpha = [np.log(i)*j for i,j in zip(range(1, predict_sequence_length+1), [0.65]*4+[1]*7+[1.2]*7+[1.5]*6)]\n",
    "    alpha = tf.reshape(tf.convert_to_tensor(alpha, tf.float32), (1, time_steps_decoder))\n",
    "    #reshape(1,24)\n",
    "    rmse = tf.sqrt(tf.reduce_mean(diff * alpha))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f5939b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "64/64 [==============================] - 58s 905ms/step - loss: 0.0874 - mae: 0.1886 - val_loss: 0.0206 - val_mae: 0.1033\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 57s 892ms/step - loss: 0.0187 - mae: 0.1014 - val_loss: 0.0134 - val_mae: 0.0858\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 57s 893ms/step - loss: 0.0133 - mae: 0.0863 - val_loss: 0.0093 - val_mae: 0.0724\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 57s 893ms/step - loss: 0.0102 - mae: 0.0758 - val_loss: 0.0081 - val_mae: 0.0667\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 57s 892ms/step - loss: 0.0088 - mae: 0.0706 - val_loss: 0.0072 - val_mae: 0.0630\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 57s 892ms/step - loss: 0.0078 - mae: 0.0668 - val_loss: 0.0066 - val_mae: 0.0606\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 57s 893ms/step - loss: 0.0074 - mae: 0.0648 - val_loss: 0.0061 - val_mae: 0.0581\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 57s 893ms/step - loss: 0.0068 - mae: 0.0623 - val_loss: 0.0065 - val_mae: 0.0592\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 57s 894ms/step - loss: 0.0065 - mae: 0.0610 - val_loss: 0.0062 - val_mae: 0.0586\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 57s 897ms/step - loss: 0.0060 - mae: 0.0588 - val_loss: 0.0059 - val_mae: 0.0569\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 57s 893ms/step - loss: 0.0058 - mae: 0.0583 - val_loss: 0.0059 - val_mae: 0.0569\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 57s 894ms/step - loss: 0.0058 - mae: 0.0581 - val_loss: 0.0060 - val_mae: 0.0573\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 57s 893ms/step - loss: 0.0057 - mae: 0.0579 - val_loss: 0.0060 - val_mae: 0.0572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd7cc148430>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=100\n",
    "# Early Stopping\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience = 3, restore_best_weights = True)\n",
    "\n",
    "# Tensorboard callback\n",
    "#logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "# Run training\n",
    "#opt = keras.optimizers.Adam(lr = 0.0001)\n",
    "#RMSprop\n",
    "#当学习停滞时，减少2倍或10倍的学习率常常能获得较好的效果。\n",
    "x = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,patience=2,verbose=0,mode=\"auto\")\n",
    "\n",
    "model.compile(metrics=['mae'], optimizer='Adam', loss='mse')\n",
    "\n",
    "model.fit([trX, trY_decoder_input], trY, batch_size=64, epochs=epochs, validation_split=0.3, verbose=1, callbacks=[earlystopping, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a26dc84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "# 对输入序列进行编码得到特征向量\n",
    "encoder_model = Model(_input,[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "# 下面的张量将保存上一个时间步的状态\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "#修改\n",
    "decoder_hidden_state_input = Input(shape=(time_steps_encoder,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "# 获取解码器序列的嵌入\n",
    "#dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))#注意是none (none, 1)\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "# 要预测序列中的下一个单词，请将初始状态设置为前一个时间步的状态\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_inputs, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "# 注意力 推理\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "# 用于生成概率分布的密集 softmax 层。 超过目标词汇\n",
    "decoder_outputs22 = decoder_dense(decoder_inf_concat)\n",
    "decoder_outputs = decoder_dense2(decoder_outputs22)\n",
    "\n",
    "# Final decoder model\n",
    "# 最终解码器模型\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs] + [state_h2, state_c2])\n",
    "\n",
    "\n",
    "#预测\n",
    "def decode_predict(input_seq):\n",
    "    # 将输入编码为状态向量。\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq.reshape(-1, 12, 24, 72))\n",
    "    #print(e_out.shape)\n",
    "    # 生成长度为 1 的空目标序列。\n",
    "    target_seq = np.zeros(( 1,1, num_decoder_tokens))#(1, 1, 1)\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    # 用起始词填充目标序列的第一个词。\n",
    "    target_seq[ 0,0, 0] = 0.\n",
    "\n",
    "    decoded_sentence = []\n",
    "    for i in range(24):\n",
    "        output, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        \n",
    "        decoded_sentence.append(output[0,0,0])\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        # 更新目标序列（长度为 1）。\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq = output\n",
    "        #print(target_seq)\n",
    "\n",
    "        # Update internal states\n",
    "        # 更新内部状态\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60075c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test No.1\n",
      "test No.2\n",
      "test No.3\n",
      "test No.4\n",
      "test No.5\n",
      "test No.6\n",
      "test No.7\n",
      "test No.8\n",
      "test No.9\n",
      "test No.10\n",
      "test No.11\n",
      "test No.12\n",
      "test No.13\n",
      "test No.14\n",
      "test No.15\n",
      "test No.16\n",
      "test No.17\n",
      "test No.18\n",
      "test No.19\n",
      "test No.20\n",
      "test No.21\n",
      "test No.22\n",
      "test No.23\n",
      "test No.24\n",
      "test No.25\n",
      "test No.26\n",
      "test No.27\n",
      "test No.28\n",
      "test No.29\n",
      "test No.30\n",
      "test No.31\n",
      "test No.32\n",
      "test No.33\n",
      "test No.34\n",
      "test No.35\n",
      "test No.36\n",
      "test No.37\n",
      "test No.38\n",
      "test No.39\n",
      "test No.40\n",
      "test No.41\n",
      "test No.42\n",
      "test No.43\n",
      "test No.44\n",
      "test No.45\n",
      "test No.46\n",
      "test No.47\n",
      "test No.48\n",
      "test No.49\n",
      "test No.50\n",
      "test No.51\n",
      "test No.52\n",
      "test No.53\n",
      "test No.54\n",
      "test No.55\n",
      "test No.56\n",
      "test No.57\n",
      "test No.58\n",
      "test No.59\n",
      "test No.60\n",
      "test No.61\n",
      "test No.62\n",
      "test No.63\n",
      "test No.64\n",
      "test No.65\n",
      "test No.66\n",
      "test No.67\n",
      "test No.68\n",
      "test No.69\n",
      "test No.70\n",
      "test No.71\n",
      "test No.72\n",
      "test No.73\n",
      "test No.74\n",
      "test No.75\n",
      "test No.76\n",
      "test No.77\n",
      "test No.78\n",
      "test No.79\n",
      "test No.80\n",
      "test No.81\n",
      "test No.82\n",
      "test No.83\n",
      "test No.84\n",
      "test No.85\n",
      "test No.86\n",
      "test No.87\n",
      "test No.88\n",
      "test No.89\n",
      "test No.90\n",
      "test No.91\n",
      "test No.92\n",
      "test No.93\n",
      "test No.94\n",
      "test No.95\n",
      "test No.96\n",
      "test No.97\n",
      "test No.98\n",
      "test No.99\n",
      "test No.100\n",
      "test No.101\n",
      "test No.102\n",
      "test No.103\n",
      "test No.104\n",
      "test No.105\n",
      "test No.106\n",
      "test No.107\n",
      "test No.108\n",
      "test No.109\n",
      "test No.110\n",
      "test No.111\n",
      "test No.112\n",
      "test No.113\n",
      "test No.114\n",
      "test No.115\n",
      "test No.116\n",
      "test No.117\n",
      "test No.118\n",
      "test No.119\n",
      "test No.120\n",
      "test No.121\n",
      "test No.122\n",
      "test No.123\n",
      "test No.124\n",
      "test No.125\n",
      "test No.126\n",
      "test No.127\n",
      "test No.128\n",
      "test No.129\n",
      "test No.130\n",
      "test No.131\n",
      "test No.132\n",
      "test No.133\n",
      "test No.134\n",
      "test No.135\n",
      "test No.136\n",
      "test No.137\n",
      "test No.138\n",
      "test No.139\n",
      "test No.140\n",
      "test No.141\n",
      "test No.142\n",
      "test No.143\n",
      "test No.144\n",
      "test No.145\n",
      "test No.146\n",
      "test No.147\n",
      "test No.148\n",
      "test No.149\n",
      "test No.150\n",
      "test No.151\n",
      "test No.152\n",
      "test No.153\n",
      "test No.154\n",
      "test No.155\n",
      "test No.156\n",
      "test No.157\n",
      "test No.158\n",
      "test No.159\n",
      "test No.160\n",
      "test No.161\n",
      "test No.162\n",
      "test No.163\n",
      "test No.164\n",
      "test No.165\n",
      "test No.166\n",
      "test No.167\n",
      "test No.168\n",
      "test No.169\n",
      "test No.170\n",
      "test No.171\n",
      "test No.172\n",
      "test No.173\n",
      "test No.174\n",
      "test No.175\n",
      "test No.176\n",
      "test No.177\n",
      "test No.178\n",
      "test No.179\n",
      "test No.180\n",
      "test No.181\n",
      "test No.182\n",
      "test No.183\n",
      "test No.184\n",
      "test No.185\n",
      "test No.186\n",
      "test No.187\n",
      "test No.188\n",
      "test No.189\n",
      "test No.190\n",
      "test No.191\n",
      "test No.192\n",
      "test No.193\n",
      "test No.194\n",
      "test No.195\n",
      "test No.196\n",
      "test No.197\n",
      "test No.198\n",
      "test No.199\n",
      "test No.200\n",
      "test No.201\n",
      "test No.202\n",
      "test No.203\n",
      "test No.204\n",
      "test No.205\n",
      "test No.206\n",
      "test No.207\n",
      "test No.208\n",
      "test No.209\n",
      "test No.210\n",
      "test No.211\n",
      "test No.212\n",
      "test No.213\n",
      "test No.214\n",
      "test No.215\n",
      "test No.216\n",
      "test No.217\n",
      "test No.218\n",
      "test No.219\n",
      "test No.220\n",
      "test No.221\n",
      "test No.222\n",
      "test No.223\n",
      "test No.224\n",
      "test No.225\n",
      "test No.226\n",
      "test No.227\n",
      "test No.228\n",
      "test No.229\n",
      "test No.230\n",
      "test No.231\n",
      "test No.232\n",
      "test No.233\n",
      "test No.234\n",
      "test No.235\n",
      "test No.236\n",
      "test No.237\n",
      "test No.238\n",
      "test No.239\n",
      "test No.240\n",
      "test No.241\n",
      "test No.242\n",
      "test No.243\n",
      "test No.244\n",
      "test No.245\n",
      "test No.246\n",
      "test No.247\n",
      "test No.248\n",
      "test No.249\n",
      "test No.250\n",
      "test No.251\n",
      "test No.252\n",
      "test No.253\n",
      "test No.254\n",
      "test No.255\n",
      "test No.256\n",
      "test No.257\n",
      "test No.258\n",
      "test No.259\n",
      "test No.260\n",
      "test No.261\n",
      "test No.262\n",
      "test No.263\n",
      "test No.264\n",
      "test No.265\n",
      "test No.266\n",
      "test No.267\n",
      "test No.268\n",
      "test No.269\n",
      "test No.270\n",
      "test No.271\n",
      "test No.272\n",
      "test No.273\n",
      "test No.274\n",
      "test No.275\n",
      "test No.276\n",
      "test No.277\n",
      "test No.278\n",
      "test No.279\n",
      "test No.280\n",
      "test No.281\n",
      "test No.282\n",
      "test No.283\n",
      "test No.284\n",
      "test No.285\n",
      "test No.286\n",
      "test No.287\n",
      "test No.288\n",
      "test No.289\n",
      "test No.290\n",
      "test No.291\n",
      "test No.292\n",
      "test No.293\n",
      "test No.294\n",
      "test No.295\n",
      "test No.296\n",
      "test No.297\n",
      "test No.298\n",
      "test No.299\n",
      "test No.300\n",
      "test No.301\n",
      "test No.302\n",
      "test No.303\n",
      "test No.304\n",
      "test No.305\n",
      "test No.306\n",
      "test No.307\n",
      "test No.308\n",
      "test No.309\n",
      "test No.310\n",
      "test No.311\n",
      "test No.312\n",
      "test No.313\n",
      "test No.314\n",
      "test No.315\n",
      "test No.316\n",
      "test No.317\n",
      "test No.318\n",
      "test No.319\n",
      "test No.320\n",
      "test No.321\n",
      "test No.322\n",
      "test No.323\n",
      "test No.324\n",
      "test No.325\n",
      "test No.326\n",
      "test No.327\n",
      "test No.328\n",
      "test No.329\n",
      "test No.330\n",
      "test No.331\n",
      "test No.332\n",
      "test No.333\n",
      "test No.334\n",
      "test No.335\n",
      "test No.336\n",
      "test No.337\n",
      "test No.338\n",
      "test No.339\n",
      "test No.340\n",
      "test No.341\n",
      "test No.342\n",
      "test No.343\n",
      "test No.344\n",
      "test No.345\n",
      "test No.346\n",
      "test No.347\n",
      "test No.348\n",
      "test No.349\n",
      "test No.350\n",
      "test No.351\n",
      "test No.352\n",
      "test No.353\n",
      "test No.354\n",
      "test No.355\n",
      "test No.356\n",
      "test No.357\n",
      "test No.358\n",
      "test No.359\n",
      "test No.360\n",
      "test No.361\n",
      "test No.362\n",
      "test No.363\n",
      "test No.364\n",
      "test No.365\n",
      "test No.366\n",
      "test No.367\n",
      "test No.368\n",
      "test No.369\n",
      "test No.370\n",
      "test No.371\n",
      "test No.372\n",
      "test No.373\n",
      "test No.374\n",
      "test No.375\n",
      "test No.376\n",
      "test No.377\n",
      "test No.378\n",
      "test No.379\n",
      "test No.380\n",
      "test No.381\n",
      "test No.382\n",
      "test No.383\n",
      "test No.384\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "for i in range(testX.shape[0]):\n",
    "    print(\"test No.\" + str(i+1))\n",
    "    out.append(decode_predict(testX[i,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9f74bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97437555  0.92884431  0.85354765  0.7583023   0.65203463  0.54571141\n",
      "  0.44398442  0.34470657  0.24869166  0.16245212  0.08885094  0.0250032\n",
      " -0.02977233 -0.07126178 -0.10342814 -0.12495972 -0.14000287 -0.15140603\n",
      " -0.16489891 -0.18016526 -0.1930057  -0.20139397 -0.20577378 -0.21199596]\n"
     ]
    }
   ],
   "source": [
    "cor = np.zeros((24))\n",
    "for i in range(24):\n",
    "    cor[i] = np.corrcoef(testY.reshape(384,24)[:,i],np.array(out)[:,i])[0,1]\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "753c875f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.85058784,\n",
       " -2.3470197,\n",
       " -4.9426928,\n",
       " -9.195482,\n",
       " -15.082142,\n",
       " -20.817394,\n",
       " -24.295744,\n",
       " -25.711657,\n",
       " -26.177704,\n",
       " -26.319181,\n",
       " -26.36103,\n",
       " -26.373312,\n",
       " -26.376911,\n",
       " -26.377964,\n",
       " -26.378271,\n",
       " -26.378363,\n",
       " -26.378387,\n",
       " -26.378399,\n",
       " -26.378405,\n",
       " -26.378405,\n",
       " -26.378405,\n",
       " -26.378405,\n",
       " -26.378405,\n",
       " -26.378405]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "173df7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.15875304],\n",
       "       [-1.15833545],\n",
       "       [-1.18053198],\n",
       "       [-1.17541635],\n",
       "       [-1.00110507],\n",
       "       [-0.80188906],\n",
       "       [-0.6772756 ],\n",
       "       [-0.52563614],\n",
       "       [-0.43219197],\n",
       "       [-0.36762857],\n",
       "       [-0.33536553],\n",
       "       [-0.28663447],\n",
       "       [-0.24307424],\n",
       "       [-0.35159171],\n",
       "       [-0.53687763],\n",
       "       [-0.68450022],\n",
       "       [-0.63430518],\n",
       "       [-0.45563629],\n",
       "       [-0.19504076],\n",
       "       [ 0.00893648],\n",
       "       [ 0.22462256],\n",
       "       [ 0.45296106],\n",
       "       [ 0.66400367],\n",
       "       [ 0.86413419]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2efe137b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均方误差(MSE)：536.300492165276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print(f\"均方误差(MSE)：{mean_squared_error(out[10], testY[10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c8dd8",
   "metadata": {},
   "source": [
    "## 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be8a5090",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = 'model_final'\n",
    "if not os.path.exists(save_model_path):\n",
    "    os.makedirs(save_model_path)\n",
    "\n",
    "# Saving encoder as in training\n",
    "encoder_model = Model(_input, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Saving decoder states and dense layer \n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(time_steps_encoder,latent_dim))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "#attention inference\n",
    "# 注意力 推理\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs, attn_out_inf])\n",
    "\n",
    "\n",
    "decoder_outputs11 = decoder_dense(decoder_inf_concat)\n",
    "decoder_outputs22 = decoder_dense2(decoder_outputs11)\n",
    "#modle(input, output)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs22] + decoder_states)\n",
    "\n",
    "#encoder_model.summary()\n",
    "#decoder_model.summary()\n",
    "encoder_model.save(os.path.join(save_model_path, 'encoder_model.h5'))\n",
    "decoder_model.save_weights(os.path.join(save_model_path, 'decoder_model_weights.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99068aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bffe1ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# inference encoder model\n",
    "save_model_path = 'model_final'\n",
    "inf_encoder_model = load_model(os.path.join(save_model_path, 'encoder_model.h5'))\n",
    "    \n",
    "# inference decoder model\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))#注意是none (none, 1)\n",
    "decoder_dense1 = Dense(latent_dim,activation ='tanh')\n",
    "decoder_dense = Dense(num_decoder_tokens)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs, attn_out_inf])\n",
    "\n",
    "decoder_outputs11 = decoder_dense1(decoder_inf_concat)\n",
    "decoder_outputs22 = decoder_dense(decoder_outputs11)\n",
    "\n",
    "inf_decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs22] + decoder_states)\n",
    "\n",
    "inf_decoder_model.load_weights(os.path.join(save_model_path, 'decoder_model_weights.h5'))\n",
    "\n",
    "def decode_predict(input_seq):\n",
    "    # 将输入编码为状态向量，states_value用于decoder\n",
    "    e_out, e_h, e_c = inf_encoder_model.predict(input_seq.reshape(-1, 12, 24, 72))\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))#(1, 1, 1)\n",
    "    target_seq[0, 0, 0] = 0.\n",
    "    decoded_sentence = []\n",
    "    \n",
    "    for i in range(24):\n",
    "        output, h, c = inf_decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        decoded_sentence.append(output[0,0,0])\n",
    "      \n",
    "      # 更新目标序列（长度为1）。\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq = output\n",
    "      \n",
    "      # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be9805ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将输入编码为状态向量，states_value用于decoder\n",
    "e_out, e_h, e_c = inf_encoder_model.predict(testX[1,:,:].reshape(-1, 12, 24, 72))\n",
    "target_seq = np.zeros((1, 1, num_decoder_tokens))#(1, 1, 1)\n",
    "target_seq[0, 0, 0] = 0.\n",
    "decoded_sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbe90b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.94294360e-02 -4.80716908e-03 -2.43398710e-03  2.57461201e-02\n",
      "   1.29896495e-02 -3.48316622e-03  2.53515411e-03  9.14517883e-03\n",
      "   2.73835100e-03  3.65677843e-04  3.84280905e-02  9.89520084e-03\n",
      "  -9.65179596e-03 -2.10680789e-03  1.35984300e-02 -2.21971665e-02\n",
      "   4.65500401e-03 -2.84637362e-02 -1.25992149e-02 -3.10753658e-02\n",
      "   9.89455148e-04 -8.79889540e-03  7.89037999e-03  1.54475163e-06\n",
      "  -1.70093216e-02  4.07776684e-02  2.14411095e-02  3.68490033e-02\n",
      "  -3.21448967e-03  3.70949012e-04  7.04183290e-03 -4.41080406e-02\n",
      "  -1.16891069e-02  5.73234074e-03 -6.44941404e-02  6.92679430e-04\n",
      "   2.04301011e-02 -1.48590365e-02  3.80915143e-02  4.74250736e-03\n",
      "   4.26264451e-04 -7.87364785e-03  2.51122732e-02 -1.69392638e-02\n",
      "  -1.51226399e-02  9.92778223e-03  9.25744884e-03  6.90290052e-03\n",
      "   4.06714249e-03  1.08147739e-02 -1.79369617e-02  2.14706585e-02\n",
      "  -2.50345189e-02  1.63761061e-02  9.18420218e-03  2.01003347e-02\n",
      "   1.64176058e-02  1.16255637e-02  2.78490204e-02 -1.81362834e-02\n",
      "  -2.34346762e-02 -2.86512380e-03  4.93442267e-03 -1.00983782e-02\n",
      "   8.45492759e-04 -9.37877595e-03 -3.28472769e-03  2.61253249e-02\n",
      "  -1.60894997e-04  1.38673349e-03 -1.68573111e-02  2.15612091e-02\n",
      "  -2.01182142e-02 -1.76670961e-02  1.17740380e-02 -1.96371805e-02\n",
      "   6.62182225e-04 -1.35458913e-02 -4.14909571e-02 -1.16430698e-02\n",
      "  -2.58957478e-03  1.18459156e-02 -6.14277134e-03  3.86629290e-05\n",
      "  -1.54752284e-02 -2.00184137e-02  2.37119179e-02  1.72825772e-02\n",
      "   3.20678912e-02  1.97442360e-02 -9.43523739e-03 -2.62235478e-02\n",
      "   3.78584340e-02 -2.25541033e-02  3.82855348e-02 -4.27901372e-02\n",
      "   1.22791529e-02  8.46641511e-03 -1.62819785e-03  1.75707173e-02\n",
      "  -3.91814508e-04  1.62982158e-02 -6.83148857e-03  5.21902591e-02\n",
      "  -3.21429633e-02 -1.30592044e-02 -4.15588357e-03  1.39084430e-02\n",
      "   1.20437052e-03 -6.10433053e-03 -1.13921156e-02  1.45584149e-02\n",
      "   1.52037083e-03 -1.39802210e-02  3.11261639e-02  2.29284968e-02\n",
      "   3.35822478e-02 -1.45146064e-02 -1.00659663e-02  2.73246914e-02\n",
      "   4.64094919e-04  7.12856092e-03  1.02630751e-02 -3.29646207e-02\n",
      "   1.27502000e-02  2.25828518e-03  3.61776426e-02  5.81182819e-03\n",
      "  -4.46248613e-03 -2.13201959e-02  6.30282424e-03  2.57148966e-02\n",
      "  -1.72068831e-02 -2.80771423e-02  2.51294374e-02  1.53729562e-02\n",
      "   7.57764746e-03 -1.34624904e-02 -2.54971273e-02  6.67029060e-03\n",
      "  -3.33520435e-02  2.07937714e-02  1.86885335e-02 -1.40396906e-02\n",
      "  -5.82267344e-03 -4.44896286e-03 -8.62179510e-03 -4.29685181e-03\n",
      "   2.95254067e-02 -7.10428786e-03 -7.19360635e-03  8.84869322e-03\n",
      "  -8.90990812e-03 -8.00659787e-03 -2.30458546e-02 -1.36475274e-02\n",
      "   3.14701203e-04  2.31150966e-02  6.69636903e-03 -5.90636395e-03\n",
      "   2.52726080e-04  2.30627507e-02 -4.68228525e-03  2.93699410e-02\n",
      "  -1.05122873e-03  6.39070943e-03 -4.95293830e-03  8.68148636e-03\n",
      "  -6.78982353e-03  5.77021670e-03 -1.65363401e-02  2.18109973e-02\n",
      "   3.33549492e-02  8.68745893e-03  4.54957932e-02  1.16234692e-02\n",
      "  -4.41902271e-03 -2.42835265e-02  4.02210699e-03 -2.20596492e-02\n",
      "   2.83921347e-03  2.21728217e-02 -1.55146029e-02  4.69247438e-02\n",
      "   2.00026706e-02 -3.11692096e-02 -4.35987627e-03 -1.54658891e-02\n",
      "  -1.62364114e-02  2.24577799e-03 -6.30849898e-02 -1.07855760e-02\n",
      "  -1.19442474e-02  5.80855971e-03  2.75466777e-02 -2.34042779e-02\n",
      "  -6.19627023e-03  2.44304184e-02 -3.33818644e-02 -2.20920946e-02\n",
      "   2.60905884e-02  1.75705831e-02  1.11767966e-02 -1.36279296e-02\n",
      "  -8.65104795e-03  9.25926957e-03  5.51484451e-02 -3.00818309e-02\n",
      "  -3.88028077e-03  3.14733386e-02 -1.89146399e-02 -7.35371746e-03\n",
      "  -2.50390228e-02  6.83691166e-03  1.50825726e-02  7.12613633e-04\n",
      "  -3.14894738e-03 -5.70314052e-03  1.04931789e-02  2.42735445e-02\n",
      "  -6.70649926e-04 -2.64570583e-02  1.23048164e-02  1.96033083e-02\n",
      "   1.03736075e-03  1.60958935e-02  9.04051587e-03  6.15680963e-02\n",
      "  -2.27179024e-02 -1.04781240e-03 -5.71699301e-03 -2.24640239e-02\n",
      "   1.75574049e-02  2.21885517e-02  1.21059716e-02 -1.22867490e-03\n",
      "   4.72972654e-02 -3.55397575e-02 -2.54978961e-03  3.01820133e-02\n",
      "  -1.51046822e-02  3.51426948e-04  1.85872391e-02 -3.38555649e-02\n",
      "  -1.12092111e-03 -1.29875494e-02 -4.04442661e-03  7.74034578e-03\n",
      "  -9.91839077e-03 -3.65011133e-02 -1.35603808e-02  1.26356902e-02\n",
      "   2.08744556e-02  2.93903379e-03 -1.31116826e-02  2.13225055e-02\n",
      "  -2.66347197e-03 -1.26242288e-04 -1.93034969e-02 -2.57553924e-02\n",
      "  -1.61544718e-02 -3.50489020e-02  1.53451134e-02 -5.28711732e-03\n",
      "  -4.27135779e-03 -5.92365046e-04  8.15688726e-03 -3.61171900e-03\n",
      "  -4.86856513e-03 -1.10579068e-02  8.39772634e-03  3.21578458e-02\n",
      "   9.13554430e-03  3.30746477e-03 -9.35074780e-03 -2.24365643e-03\n",
      "   2.41667684e-02  1.26903392e-02 -3.58030610e-02 -7.70391314e-04\n",
      "   2.56346390e-02  1.46578311e-03  1.68601924e-03  1.81985460e-02\n",
      "  -1.87906623e-02 -1.02968197e-02  2.43797582e-02  2.63135824e-02\n",
      "  -6.04920601e-03 -2.55497987e-04  1.35079706e-02  2.19687782e-02\n",
      "  -9.84530244e-03  2.08047275e-02  6.57797232e-03  7.45865330e-03\n",
      "   6.26352290e-03  1.61511898e-02  1.66278146e-02  5.19333757e-04\n",
      "  -1.74434856e-03  1.37910515e-03  2.62315217e-02  2.59008468e-03\n",
      "  -1.17366510e-02  1.25834728e-02  2.56795399e-02 -2.33874358e-02\n",
      "  -9.18550789e-03 -5.11083864e-02 -3.33635835e-04  6.32352056e-03\n",
      "  -6.18781662e-03  3.85039975e-03 -3.50810029e-03  2.10329387e-02\n",
      "  -4.34816582e-03 -2.38605868e-02 -4.76487577e-02 -2.57568117e-02\n",
      "   5.61282132e-03 -1.06492657e-02 -3.63356061e-03  9.35118180e-03\n",
      "   1.13068856e-02  1.30699826e-02 -1.85426250e-02 -8.26393906e-03\n",
      "  -3.53109767e-03  3.07388119e-02 -1.62994657e-02  2.02795700e-03\n",
      "   7.56396353e-03 -3.12328152e-02 -1.90045815e-02 -2.01080628e-02\n",
      "  -5.16968954e-04 -2.72804145e-02  9.08351596e-03 -1.92779079e-02\n",
      "  -2.64545903e-02  2.55592745e-02  1.41428607e-02 -2.76527507e-03\n",
      "   8.11810978e-03 -1.13292001e-02 -7.44842505e-03 -1.96967041e-03\n",
      "  -3.22146527e-02 -9.78323072e-03 -3.47625576e-02 -5.59490509e-02\n",
      "   1.56873111e-02  2.03017183e-02 -3.97867803e-03 -2.09238213e-02\n",
      "   2.14162539e-03 -1.97280645e-02 -9.13454778e-03  1.99643020e-02\n",
      "  -2.34794710e-02  2.10955366e-03 -1.17285736e-02 -1.92080066e-02\n",
      "  -2.87316237e-02  1.08782388e-02 -1.31360516e-02 -1.35731325e-03\n",
      "   2.02225894e-02 -1.28387678e-02  2.64974777e-02 -5.27792983e-03\n",
      "   1.53296413e-02 -1.00684976e-02  2.55294908e-02  9.95276216e-03\n",
      "   1.60313156e-02 -5.00711286e-03 -1.58458110e-02  6.89949282e-03\n",
      "   1.07249515e-02 -1.63776632e-02  4.10511158e-02 -1.54371206e-02\n",
      "   1.92388427e-02 -1.41014354e-02 -7.74470344e-03  2.43942142e-02\n",
      "  -7.22426083e-03 -2.81997002e-03  2.66365614e-02  3.34559777e-03\n",
      "   2.01429520e-02  9.57218464e-03 -4.60145390e-03 -1.31216878e-02\n",
      "  -1.44427000e-02 -7.05836620e-03 -1.17363911e-02 -2.03216486e-02\n",
      "   8.27584602e-03 -4.46243137e-02 -4.33054240e-03  3.11531196e-03\n",
      "   2.03401800e-02  1.59016270e-02  1.52083645e-02 -2.28173677e-02\n",
      "  -3.90772801e-03 -2.03860141e-02  1.03335297e-02 -2.96294931e-02\n",
      "  -1.43338181e-02 -3.92714841e-03  1.37914941e-02 -1.00910221e-03\n",
      "   1.12598378e-03  1.85500681e-02 -3.72787938e-02 -1.01428255e-02\n",
      "   1.79638248e-02 -9.82061401e-03  1.07748797e-02 -2.29335185e-02\n",
      "  -1.12263560e-02  1.36865340e-02  1.35463895e-03  1.46946786e-02\n",
      "   1.18043087e-02 -1.43809523e-02  5.09761088e-03 -2.92755105e-02\n",
      "   6.06404152e-03  1.36359055e-02 -1.37444576e-02 -4.25633555e-03\n",
      "   2.67624576e-02 -2.72810459e-04 -2.02571112e-03  2.15971638e-02\n",
      "  -7.61932973e-03  2.50991946e-03 -4.76737432e-02  3.78130213e-03\n",
      "  -5.54347504e-03  1.24297710e-02  8.17251764e-03  4.36700694e-02\n",
      "  -1.22653916e-02  1.46410298e-02 -7.08687119e-03 -5.97165199e-03\n",
      "  -1.36839673e-02 -3.44773084e-02 -2.89095063e-02  1.28083369e-02\n",
      "  -2.75433287e-02  7.24075921e-03 -2.11737137e-02  7.83983991e-03\n",
      "  -1.34931430e-02  1.94495742e-03  1.99123975e-02 -6.73308037e-03\n",
      "  -4.85471543e-03 -5.40525327e-03 -1.09768119e-02 -1.19478221e-03\n",
      "  -4.10323851e-02  1.64964218e-02 -4.57620062e-02 -3.68526690e-02\n",
      "   1.54782794e-02 -1.68868508e-02 -9.63481050e-03 -3.18732597e-02\n",
      "   1.11603867e-02 -2.32689548e-02  7.19509041e-03  2.11451165e-02\n",
      "  -1.26925344e-02  9.07161925e-03  2.86289025e-03  4.31704000e-02\n",
      "  -4.03420776e-02  3.75176519e-02  9.94483102e-03 -1.70999095e-02\n",
      "   1.63594652e-02 -1.13690738e-02 -3.97883169e-03 -3.44002284e-02\n",
      "   2.86523551e-02  3.21597420e-03 -1.40843298e-02  1.61410365e-02\n",
      "   2.25899578e-03  5.65635832e-03 -1.14905685e-02 -2.18176041e-02\n",
      "   1.24244206e-02  2.52212733e-02  1.72412768e-02 -6.25458406e-03\n",
      "   9.92884859e-03 -5.39564108e-03 -2.45827693e-03  3.38184051e-02\n",
      "   9.41888150e-03 -5.66444546e-03  1.12206973e-02 -2.86639407e-02]]\n",
      "[[-0.01694224 -0.00405122 -0.00593675  0.02510395  0.0187013  -0.00310902\n",
      "   0.00286019  0.00886322  0.00123183  0.00423659  0.0340391   0.0138992\n",
      "  -0.00960407  0.00691283  0.02163897 -0.02609316  0.00612212 -0.02800633\n",
      "  -0.01705072 -0.02827645  0.00033099 -0.01233691  0.00841077  0.00211779\n",
      "  -0.01559359  0.04824047  0.02861483  0.03190327 -0.0071392   0.00184695\n",
      "   0.01010855 -0.05247454 -0.01217801  0.01052525 -0.07153146 -0.00511211\n",
      "   0.01582637 -0.01835139  0.04461434 -0.0018805   0.00652188 -0.01256838\n",
      "   0.02839128 -0.01819167 -0.00389272  0.00889376  0.00322599  0.01257017\n",
      "   0.00154251  0.01533303 -0.01412323  0.02465647 -0.02991804  0.02160534\n",
      "   0.00687935  0.01976543  0.02365899  0.00653285  0.03177975 -0.02379381\n",
      "  -0.02311611 -0.00065475  0.00825509 -0.00126626 -0.00011474 -0.01089889\n",
      "  -0.00747104  0.02691828  0.00792886 -0.00472288 -0.01776712  0.01515341\n",
      "  -0.01482308 -0.01562465  0.01383735 -0.01340624 -0.00634196 -0.02092399\n",
      "  -0.04718525 -0.02135839 -0.00677828  0.01772529 -0.00328949  0.00461165\n",
      "  -0.00856631 -0.02654462  0.0189578   0.01564191  0.03178912  0.02478061\n",
      "  -0.01222777 -0.0255981   0.04195606 -0.02975673  0.03336084 -0.04516063\n",
      "   0.01007679  0.00417581 -0.00383519  0.0197746   0.00353563  0.01817568\n",
      "  -0.00939513  0.05569331 -0.03868031 -0.01051248 -0.00841459  0.02003623\n",
      "  -0.00252583 -0.00021226 -0.01510233  0.00776489  0.00241232 -0.01275717\n",
      "   0.03564059  0.01642754  0.03176268 -0.02381185 -0.01608288  0.03368562\n",
      "  -0.00720163  0.0087488   0.00609287 -0.02584225  0.00753505 -0.00354319\n",
      "   0.04033113  0.00601881 -0.0005695  -0.01345264  0.01239784  0.02429084\n",
      "  -0.01278008 -0.02742109  0.02378818  0.01975685  0.00120002 -0.00590898\n",
      "  -0.02572959  0.00997597 -0.04187031  0.0201166   0.02045862 -0.01974443\n",
      "  -0.01284364 -0.00338174 -0.01003783 -0.00080013  0.03466778 -0.01350268\n",
      "  -0.01159091  0.00877791 -0.01368933 -0.0136491  -0.02826444 -0.00730341\n",
      "  -0.00249066  0.02676773  0.00285536 -0.00233243 -0.00331255  0.02792673\n",
      "  -0.00807015  0.03086429  0.00165771  0.00310014 -0.01125957  0.00204279\n",
      "  -0.01262713 -0.00022511 -0.01671607  0.02369952  0.03261947  0.00833101\n",
      "   0.0487345   0.0163123  -0.01082608 -0.02149864  0.0026668  -0.02509527\n",
      "   0.00547344  0.01886651 -0.02090445  0.04567153  0.02532568 -0.02706184\n",
      "  -0.00617488 -0.01973232 -0.01190738 -0.00125736 -0.06758429 -0.00908587\n",
      "  -0.01878022  0.01261553  0.03248326 -0.02602175 -0.00341823  0.02340605\n",
      "  -0.03712355 -0.01529377  0.03044646  0.01606194  0.00387583 -0.01986778\n",
      "  -0.01322853  0.01389055  0.0632601  -0.03553996 -0.00337179  0.03536824\n",
      "  -0.01809105 -0.00514357 -0.03014531  0.00440818  0.02024036  0.00197225\n",
      "  -0.00101243 -0.00288562  0.00354534  0.01798862 -0.00460193 -0.032996\n",
      "   0.01712851  0.02392921  0.00514046  0.0129094   0.01329427  0.05422468\n",
      "  -0.02093323 -0.00406713 -0.00936391 -0.02116702  0.02580595  0.02135194\n",
      "   0.0159337   0.00424627  0.04265314 -0.03997754 -0.00637669  0.0301577\n",
      "  -0.02102301 -0.00108282  0.02476027 -0.04093427 -0.00701644 -0.01704459\n",
      "  -0.00798332  0.00283914 -0.01444467 -0.03285281 -0.016259    0.01601033\n",
      "   0.0156619  -0.00041051 -0.02015091  0.01742598  0.00333929 -0.00540988\n",
      "  -0.01575381 -0.03311392 -0.02011407 -0.03336786  0.02130272 -0.00434098\n",
      "  -0.00877877 -0.00580257  0.00598124 -0.00946689 -0.01112091 -0.01451615\n",
      "   0.01280095  0.03889959  0.01189652  0.0104924  -0.01109163 -0.0073873\n",
      "   0.03131831  0.01481456 -0.04049798  0.00596982  0.01371988  0.0042608\n",
      "  -0.00192676  0.02279004 -0.0195975  -0.0156157   0.02347256  0.03120846\n",
      "  -0.01142236  0.00188869  0.00548442  0.02576046 -0.00666295  0.01512228\n",
      "   0.01010307  0.01163531  0.00799165  0.02154521  0.01992888 -0.00333112\n",
      "  -0.00311383  0.00391146  0.02236975 -0.00563322 -0.00952967  0.0119847\n",
      "   0.03263571 -0.02436131 -0.00293831 -0.05626423 -0.00573628  0.01135979\n",
      "  -0.00103558  0.00346787  0.00075194  0.02295061 -0.00259228 -0.02694044\n",
      "  -0.04529727 -0.02846237  0.01007712 -0.00714847 -0.0112708   0.0155807\n",
      "   0.01101272  0.01687398 -0.01988741  0.00078218  0.00585885  0.03055021\n",
      "  -0.00910104  0.00032632  0.00719333 -0.03307876 -0.01518858 -0.02027919\n",
      "  -0.0052808  -0.03063425  0.00356254 -0.0255257  -0.02506233  0.02852638\n",
      "   0.0084296   0.00016326  0.00754957 -0.01581132 -0.01198931 -0.00671559\n",
      "  -0.03740057 -0.00768329 -0.0347567  -0.0598553   0.01080397  0.01317549\n",
      "  -0.00644069 -0.0273101   0.00918057 -0.01548069 -0.00946419  0.01398527\n",
      "  -0.02410742  0.00336377 -0.01449506 -0.02378083 -0.03385846  0.00619388\n",
      "  -0.01495064  0.00397046  0.01404924 -0.00853495  0.03463725 -0.01248865\n",
      "   0.01726437 -0.00487279  0.02976211  0.005405    0.01973153 -0.01036777\n",
      "  -0.01259472  0.01554734  0.01568881 -0.01111837  0.04112788 -0.02233168\n",
      "   0.02390664 -0.01911989 -0.00127838  0.02274147 -0.01187371  0.00338917\n",
      "   0.02516969  0.01094658  0.01920673  0.00121204 -0.00737421 -0.0113675\n",
      "  -0.01167305 -0.01091507 -0.01378021 -0.02030459  0.00220102 -0.04692077\n",
      "  -0.00057178  0.00564152  0.0274104   0.01798751  0.02180706 -0.01866381\n",
      "  -0.00055671 -0.02242161  0.01108853 -0.03570716 -0.02136104  0.00113314\n",
      "   0.01524283 -0.00560861  0.00089921  0.0198606  -0.03106941 -0.01019308\n",
      "   0.01533324 -0.00375427  0.01275715 -0.02688466 -0.01046095  0.017978\n",
      "  -0.0013937   0.01787417  0.01125553 -0.0126714   0.01043447 -0.02617306\n",
      "   0.00482047  0.01252028 -0.01703933 -0.00436448  0.03074686 -0.00158678\n",
      "  -0.0008445   0.03255386 -0.01112744 -0.00219797 -0.05465644  0.00855563\n",
      "  -0.00397846  0.01970105  0.00528926  0.04516313 -0.00660443  0.01249328\n",
      "   0.00063561 -0.00539258 -0.0189753  -0.03882869 -0.02485544  0.0089665\n",
      "  -0.03418939  0.00592111 -0.02559945  0.00229875 -0.01350451  0.00092807\n",
      "   0.02212067 -0.01264085 -0.00098437 -0.00011108 -0.01391291  0.00369435\n",
      "  -0.04933026  0.01361467 -0.03521927 -0.03825259  0.01759111 -0.01293342\n",
      "  -0.00715186 -0.02937352  0.01295367 -0.02786735  0.01213957  0.01811315\n",
      "  -0.0176966   0.00906227  0.0024281   0.03621202 -0.0363614   0.03871436\n",
      "   0.01530532 -0.02043647  0.01549808 -0.01261063 -0.00982533 -0.03329486\n",
      "   0.02739526  0.0098375  -0.01785292  0.02240126 -0.004886    0.00743088\n",
      "  -0.01463692 -0.02187888  0.00615355  0.02633564  0.02117498 -0.00962398\n",
      "   0.0130868   0.00142725  0.00095478  0.03282389  0.01702229 -0.00091052\n",
      "   0.01255497 -0.03579088]]\n",
      "[[-0.01233357 -0.00273762 -0.0119966   0.02395986  0.0290156  -0.00241008\n",
      "   0.00344093  0.00836779 -0.00135449  0.01110217  0.0261868   0.02109737\n",
      "  -0.00951488  0.02366357  0.03497674 -0.03274137  0.00896315 -0.02720818\n",
      "  -0.0253295  -0.02291319 -0.00090755 -0.0189261   0.00935741  0.00595777\n",
      "  -0.0130166   0.061715    0.04124181  0.02307917 -0.01402367  0.00453033\n",
      "   0.01541056 -0.06685007 -0.01301228  0.01899255 -0.08367334 -0.01531852\n",
      "   0.00717636 -0.02450337  0.05589841 -0.01387486  0.0172803  -0.0210968\n",
      "   0.03440886 -0.02034324  0.01833515  0.00707778 -0.00720784  0.02209636\n",
      "  -0.00301488  0.02321855 -0.00723619  0.03065389 -0.03859886  0.03068115\n",
      "   0.00267241  0.01915663  0.03603328 -0.00252954  0.03874556 -0.03359941\n",
      "  -0.02252477  0.00312133  0.01398318  0.0154408  -0.00178705 -0.01365286\n",
      "  -0.01460325  0.02830576  0.0222964  -0.01609844 -0.01932598  0.00403369\n",
      "  -0.0056017  -0.01205018  0.01746077 -0.00165714 -0.01880054 -0.0345229\n",
      "  -0.0574966  -0.0379212  -0.01458517  0.02885096  0.00164494  0.01282711\n",
      "   0.00344511 -0.03806042  0.01094602  0.01253937  0.03125257  0.03417252\n",
      "  -0.0173242  -0.02448509  0.04927813 -0.04261823  0.02399628 -0.04948784\n",
      "   0.00614864 -0.00384922 -0.00793021  0.02370092  0.01032455  0.02154403\n",
      "  -0.01421965  0.06203904 -0.050139   -0.00581931 -0.01623443  0.0311725\n",
      "  -0.00918446  0.0103174  -0.02174103 -0.00430606  0.00394987 -0.01062228\n",
      "   0.04371813  0.00465193  0.02855383 -0.04202146 -0.02691961  0.04475785\n",
      "  -0.02106364  0.01155421 -0.00154069 -0.01300033 -0.00133118 -0.01304102\n",
      "   0.04756187  0.00638844  0.00637739  0.00062306  0.02321375  0.02166933\n",
      "  -0.00429241 -0.02621875  0.02142919  0.02756529 -0.01013354  0.0075093\n",
      "  -0.02613407  0.01546046 -0.05721337  0.01882904  0.02361771 -0.02978271\n",
      "  -0.02535206 -0.00155668 -0.01270267  0.00515841  0.04422557 -0.02457446\n",
      "  -0.01949649  0.00865752 -0.02218258 -0.02396008 -0.03783311  0.00351241\n",
      "  -0.0072146   0.03343523 -0.00499441  0.00442489 -0.00931378  0.03658041\n",
      "  -0.01381962  0.03330775  0.00667724 -0.00282193 -0.02221424 -0.0102479\n",
      "  -0.02292131 -0.01080632 -0.0170333   0.0268907   0.0312087   0.0077139\n",
      "   0.05441774  0.02489546 -0.02184815 -0.01640569  0.00032948 -0.02999583\n",
      "   0.00985953  0.01271035 -0.03015744  0.04324358  0.03481154 -0.01955298\n",
      "  -0.00962896 -0.0271548  -0.00413554 -0.00743527 -0.07530614 -0.00589802\n",
      "  -0.03100843  0.02490867  0.04152578 -0.03054001  0.00165155  0.02159017\n",
      "  -0.04410475 -0.00254024  0.03810709  0.01346262 -0.00950491 -0.03135386\n",
      "  -0.02124531  0.02251077  0.07734036 -0.04541022 -0.00242208  0.0424752\n",
      "  -0.01664585 -0.00119659 -0.03932685  0.00010853  0.02942463  0.00426121\n",
      "   0.0028486   0.00211427 -0.00914643  0.0076028  -0.0120298  -0.0446118\n",
      "   0.02539155  0.03185038  0.01244849  0.00699641  0.02067136  0.04108716\n",
      "  -0.01764248 -0.00941036 -0.01632128 -0.01892062  0.04006462  0.0196551\n",
      "   0.02248171  0.014382    0.0341582  -0.04807418 -0.01369469  0.0300992\n",
      "  -0.03148962 -0.0038454   0.03610647 -0.05392625 -0.01793801 -0.02476369\n",
      "  -0.01495796 -0.00613002 -0.02213321 -0.02643183 -0.02093925  0.02210203\n",
      "   0.00680102 -0.00623098 -0.03217583  0.01026875  0.01416496 -0.0147163\n",
      "  -0.00954448 -0.04600805 -0.02707105 -0.03045704  0.03162497 -0.0025907\n",
      "  -0.01676792 -0.01501269  0.00209173 -0.02010623 -0.02232078 -0.02031832\n",
      "   0.0203042   0.05094247  0.01673241  0.02337171 -0.01399278 -0.01650423\n",
      "   0.04359789  0.0187062  -0.04917132  0.01787904 -0.00739696  0.00925566\n",
      "  -0.00841773  0.03111691 -0.02101915 -0.02516649  0.02185672  0.03955824\n",
      "  -0.02124401  0.00554498 -0.0088285   0.03264434 -0.00106254  0.00446566\n",
      "   0.01640407  0.01915125  0.01126157  0.03113851  0.02580516 -0.0101479\n",
      "  -0.00538633  0.00852962  0.01557215 -0.02028167 -0.00566619  0.01096432\n",
      "   0.04551006 -0.02598811  0.00742375 -0.06559685 -0.01494354  0.02085437\n",
      "   0.0078818   0.00283174  0.00835547  0.0262838   0.00069337 -0.03235669\n",
      "  -0.04083451 -0.03325801  0.01752861 -0.00110082 -0.02487541  0.02661594\n",
      "   0.01045215  0.02380136 -0.02241208  0.01700378  0.02210944  0.02992311\n",
      "   0.00311782 -0.00268006  0.00647446 -0.03589507 -0.00834785 -0.02056918\n",
      "  -0.01369153 -0.03677253 -0.00631044 -0.03645315 -0.02257763  0.03402703\n",
      "  -0.00212414  0.00543944  0.00657846 -0.02375863 -0.01984265 -0.014746\n",
      "  -0.04657213 -0.00387187 -0.03474375 -0.06716791  0.00213956  0.00042572\n",
      "  -0.01076699 -0.03863348  0.02144267 -0.00748863 -0.01005363  0.00353212\n",
      "  -0.02526044  0.00571354 -0.0196261  -0.03217347 -0.04253755 -0.00173194\n",
      "  -0.01840806  0.01357421  0.0032505  -0.00071269  0.04977311 -0.02514885\n",
      "   0.02042598  0.00458042  0.03688111 -0.00244188  0.02581862 -0.02042854\n",
      "  -0.00671395  0.03096069  0.02460222 -0.00140355  0.04081444 -0.03503679\n",
      "   0.03152459 -0.02852027  0.01007755  0.01994892 -0.02013741  0.01402618\n",
      "   0.02254365  0.02525597  0.01755266 -0.01428041 -0.01233101 -0.00829699\n",
      "  -0.00689648 -0.01744468 -0.01753811 -0.02025014 -0.00801998 -0.05107837\n",
      "   0.0063591   0.01019341  0.04020427  0.02176436  0.03364765 -0.01148907\n",
      "   0.00572353 -0.02610153  0.0124856  -0.04672109 -0.03423646  0.01022652\n",
      "   0.0176545  -0.01427763  0.00049324  0.02213297 -0.01925604 -0.0102832\n",
      "   0.01017293  0.00647476  0.01621029 -0.03339468 -0.00907749  0.02543706\n",
      "  -0.0065521   0.02373257  0.01022242 -0.00942511  0.02002569 -0.02048951\n",
      "   0.00251659  0.01051883 -0.0228193  -0.00456229  0.0382305  -0.00398077\n",
      "   0.00134259  0.05186022 -0.01751374 -0.01078154 -0.06685381  0.01721969\n",
      "  -0.00102443  0.03226266  0.00018223  0.04783015  0.00369499  0.00860737\n",
      "   0.01410081 -0.00436592 -0.02834451 -0.04654869 -0.01743598  0.0020314\n",
      "  -0.04635371  0.00358425 -0.03377989 -0.00800995 -0.01352451 -0.00080785\n",
      "   0.02603755 -0.02322447  0.00610492  0.00953102 -0.01899187  0.0125492\n",
      "  -0.06447375  0.00861236 -0.01770593 -0.04034021  0.02137954 -0.00568589\n",
      "  -0.00263738 -0.0249269   0.01621852 -0.03605644  0.02094826  0.01277056\n",
      "  -0.02689482  0.00904514  0.00168745  0.02318425 -0.02929552  0.04084525\n",
      "   0.02476727 -0.02634686  0.01397842 -0.01486171 -0.02033444 -0.03122292\n",
      "   0.02515581  0.02173611 -0.02437353  0.03321318 -0.01763818  0.01062017\n",
      "  -0.02039869 -0.02198747 -0.00491239  0.02832908  0.02796176 -0.01511283\n",
      "   0.01838328  0.01360783  0.0070533   0.03088545  0.03047124  0.00760897\n",
      "   0.01482807 -0.04867725]]\n",
      "[[-0.00367919 -0.00053424 -0.02205329  0.02195086  0.04742357 -0.00107737\n",
      "   0.00446542  0.00751938 -0.00557021  0.02302928  0.01238811  0.03389906\n",
      "  -0.00934495  0.05494509  0.05509868 -0.04347826  0.0146986  -0.02585851\n",
      "  -0.04094864 -0.01227704 -0.00329481 -0.03139413  0.01108223  0.01291938\n",
      "  -0.00832102  0.08567366  0.06272074  0.00765273 -0.0256963   0.00940573\n",
      "   0.02416217 -0.09027262 -0.01436353  0.03357703 -0.10358719 -0.03268984\n",
      "  -0.00939181 -0.03506074  0.07452166 -0.03539322  0.03572163 -0.03653886\n",
      "   0.04551034 -0.02386946  0.06369992  0.00397246 -0.02443421  0.03692186\n",
      "  -0.01119325  0.03649892  0.0051217   0.04221597 -0.05371297  0.04574882\n",
      "  -0.00502971  0.01804904  0.05590586 -0.01832678  0.05082631 -0.04980203\n",
      "  -0.02140925  0.00923829  0.02343818  0.0474603  -0.00460139 -0.0186311\n",
      "  -0.02607883  0.03065719  0.04707278 -0.03750474 -0.02187061 -0.01444776\n",
      "   0.00982623 -0.00598379  0.02365039  0.02091897 -0.0404664  -0.05962482\n",
      "  -0.07601792 -0.06422796 -0.0293539   0.05037646  0.00983446  0.02738538\n",
      "   0.02340811 -0.0577336  -0.00166193  0.00651209  0.03018405  0.05187835\n",
      "  -0.02663807 -0.02253485  0.06216853 -0.06508505  0.00574392 -0.05739985\n",
      "  -0.00075191 -0.01912311 -0.01561822  0.03058845  0.02155722  0.02752241\n",
      "  -0.02351994  0.07347369 -0.06947953  0.0029129  -0.03066075  0.05127909\n",
      "  -0.02087374  0.02880607 -0.03345188 -0.02517132  0.0064894  -0.00701195\n",
      "   0.05796138 -0.01646523  0.02303632 -0.07886944 -0.04614986  0.0632457\n",
      "  -0.04577333  0.01622261 -0.0155495   0.00973006 -0.01550891 -0.02708985\n",
      "   0.05969264  0.00704036  0.01857485  0.02522985  0.04194421  0.01681406\n",
      "   0.01246711 -0.02399977  0.0173867   0.04120825 -0.02977105  0.03067452\n",
      "  -0.02681093  0.02382442 -0.08433482  0.01630871  0.02917653 -0.04691146\n",
      "  -0.04711503  0.00140841 -0.01784124  0.01476303  0.06221597 -0.04283709\n",
      "  -0.03353734  0.00846326 -0.03690664 -0.04276988 -0.05539514  0.02087025\n",
      "  -0.01464001  0.0456211  -0.0219512   0.01753353 -0.01874946  0.05167332\n",
      "  -0.02299084  0.03687738  0.01610584 -0.01338646 -0.04039549 -0.03314679\n",
      "  -0.04054868 -0.02890864 -0.0175795   0.03195704  0.02841318  0.00668578\n",
      "   0.06410687  0.04064195 -0.03982206 -0.00706567 -0.00353071 -0.03699548\n",
      "   0.01657548  0.00106398 -0.04523283  0.03852237  0.05136945 -0.0058868\n",
      "  -0.01640126 -0.03958328  0.00964463 -0.01805587 -0.08787823  0.00021568\n",
      "  -0.05245071  0.04681244  0.05812811 -0.03800227  0.01092227  0.01842997\n",
      "  -0.05735884  0.02170964  0.05120863  0.0091732  -0.03393307 -0.05255217\n",
      "  -0.0348172   0.03877896  0.10063329 -0.06311272 -0.00061192  0.05543135\n",
      "  -0.01417819  0.00575351 -0.05560985 -0.00732945  0.04544096  0.00842084\n",
      "   0.00979866  0.01080255 -0.03222726 -0.00806782 -0.0263836  -0.06474245\n",
      "   0.03881134  0.04639486  0.02522459 -0.00412371  0.03292731  0.01809379\n",
      "  -0.01151381 -0.01864636 -0.03001517 -0.01517579  0.06339893  0.01601285\n",
      "   0.03310624  0.03332467  0.01860069 -0.06284969 -0.02807255  0.02994993\n",
      "  -0.04947851 -0.00937095  0.05701732 -0.077771   -0.03833064 -0.03986872\n",
      "  -0.02700844 -0.02256752 -0.03440844 -0.0154391  -0.02874862  0.03301229\n",
      "  -0.00736782 -0.0159718  -0.05147836 -0.00293239  0.03346907 -0.03062368\n",
      "   0.00097101 -0.06772305 -0.03892961 -0.02560851  0.04870942  0.00068968\n",
      "  -0.03059103 -0.03086574 -0.00477658 -0.03930593 -0.04199382 -0.02935087\n",
      "   0.03237448  0.07200688  0.0249517   0.04599226 -0.01844668 -0.03225273\n",
      "   0.0635061   0.02586759 -0.06533612  0.03828765 -0.04345244  0.01806224\n",
      "  -0.01995719  0.04615426 -0.02346415 -0.04207378  0.01905166  0.05300303\n",
      "  -0.03916619  0.011455   -0.03369731  0.04510316  0.00853723 -0.01584905\n",
      "   0.02749172  0.03252603  0.01762699  0.04784447  0.03607402 -0.02194557\n",
      "  -0.00881608  0.01695555  0.00393635 -0.04561036  0.00089331  0.00931418\n",
      "   0.06946158 -0.02851185  0.02316425 -0.08233192 -0.02972729  0.03915224\n",
      "   0.02263867  0.00185619  0.02170288  0.03186439  0.00696901 -0.04161825\n",
      "  -0.03213967 -0.04158079  0.02902384  0.00890575 -0.04846901  0.04565881\n",
      "   0.00934826  0.03641352 -0.02725993  0.04541077  0.04869206  0.02804851\n",
      "   0.02251066 -0.00787729  0.00501904 -0.03943542  0.00374557 -0.02102685\n",
      "  -0.02815745 -0.04800655 -0.02366487 -0.05484925 -0.01820127  0.04436369\n",
      "  -0.021743    0.01487112  0.0050021  -0.03747838 -0.0328267  -0.02747629\n",
      "  -0.06237902  0.0030352  -0.03471296 -0.08113282 -0.01288876 -0.02187398\n",
      "  -0.0181682  -0.05818897  0.04191165  0.00787853 -0.01109522 -0.0140769\n",
      "  -0.02739499  0.01021471 -0.02928003 -0.04764447 -0.05630033 -0.01428135\n",
      "  -0.02520538  0.03068395 -0.01492711  0.01342834  0.07809794 -0.04658254\n",
      "   0.0251091   0.02170996  0.04803801 -0.0153699   0.03494065 -0.03964699\n",
      "   0.00387474  0.05767366  0.0404021   0.01666079  0.03868081 -0.05848967\n",
      "   0.04273511 -0.04643927  0.02931674  0.01552269 -0.03452289  0.03125871\n",
      "   0.01789503  0.05258789  0.01469495 -0.04307362 -0.02107252 -0.00307907\n",
      "   0.00098941 -0.02781978 -0.02447962 -0.02008161 -0.02401126 -0.05857858\n",
      "   0.01922629  0.0183387   0.06307604  0.02858624  0.05452488  0.0003703\n",
      "   0.01773702 -0.03272966  0.01510252 -0.06652061 -0.05778646  0.02636879\n",
      "   0.02135525 -0.03098488 -0.00022566  0.02592329  0.00364085 -0.01044341\n",
      "  -0.0002887   0.02254589  0.0220225  -0.04313285 -0.00658775  0.03790949\n",
      "  -0.01645008  0.03460586  0.00822792 -0.00307228  0.03702817 -0.01004337\n",
      "  -0.001813    0.00696149 -0.03266472 -0.00492627  0.05259278 -0.00834543\n",
      "   0.00545036  0.08465363 -0.02913793 -0.02639179 -0.08728264  0.0328535\n",
      "   0.00469129  0.05286847 -0.00866912  0.05248393  0.02233423  0.00160033\n",
      "   0.03655711 -0.0025795  -0.04452206 -0.05996114 -0.00383698 -0.01040281\n",
      "  -0.0685667  -0.00047078 -0.04903387 -0.02732547 -0.0135588  -0.0036199\n",
      "   0.03284571 -0.04187011  0.01913467  0.02703191 -0.02741711  0.02847782\n",
      "  -0.0918867   0.00025026  0.00871736 -0.04280258  0.02810106  0.00765747\n",
      "   0.00557304 -0.01716635  0.02217324 -0.05036151  0.03631114  0.00359685\n",
      "  -0.04387135  0.00901358  0.00049255 -0.00148707 -0.01702488  0.0445425\n",
      "   0.04096605 -0.03659749  0.0113599  -0.01893556 -0.03893279 -0.02725231\n",
      "   0.02123166  0.04278224 -0.03519154  0.05101374 -0.03987553  0.0163073\n",
      "  -0.03098293 -0.02217627 -0.02386198  0.03185554  0.03914487 -0.02315836\n",
      "   0.02662575  0.03487166  0.0177809   0.02694943  0.05352452  0.02266154\n",
      "   0.01849709 -0.07169073]]\n",
      "[[ 1.26311565e-02  2.88932933e-03 -3.72522548e-02  1.85549557e-02\n",
      "   7.86777437e-02  1.50882208e-03  6.20876299e-03  6.14858791e-03\n",
      "  -1.16552245e-02  4.26506624e-02 -1.04989940e-02  5.58041371e-02\n",
      "  -9.01739020e-03  1.11777544e-01  7.89979696e-02 -5.88035211e-02\n",
      "   2.67605670e-02 -2.37263329e-02 -7.04659000e-02  9.46343970e-03\n",
      "  -7.99622014e-03 -5.51025644e-02  1.41904466e-02  2.53350884e-02\n",
      "   1.13954098e-04  1.25731036e-01  9.60443318e-02 -1.76504329e-02\n",
      "  -4.38163429e-02  1.81013830e-02  3.70744877e-02 -1.23932555e-01\n",
      "  -1.63195971e-02  5.71093224e-02 -1.32428452e-01 -5.98469600e-02\n",
      "  -4.12461124e-02 -5.20572029e-02  1.01949528e-01 -7.22933263e-02\n",
      "   6.49561360e-02 -6.37516230e-02  6.57550693e-02 -2.90793199e-02\n",
      "   1.54195875e-01 -1.02378463e-03 -4.98413034e-02  5.60298301e-02\n",
      "  -2.54770666e-02  5.70416972e-02  2.66566388e-02  6.48993179e-02\n",
      "  -7.85474554e-02  6.81346878e-02 -1.89747307e-02  1.60597060e-02\n",
      "   8.33080485e-02 -4.42825779e-02  7.06299692e-02 -7.34993145e-02\n",
      "  -1.92811545e-02  1.80449001e-02  3.75838727e-02  1.07304201e-01\n",
      "  -8.99043400e-03 -2.74605881e-02 -4.23054062e-02  3.43524925e-02\n",
      "   8.61804858e-02 -7.72551745e-02 -2.56127361e-02 -4.21056971e-02\n",
      "   3.31995413e-02  3.60824703e-03  3.35619934e-02  6.41965643e-02\n",
      "  -7.57685304e-02 -1.04538351e-01 -1.07993677e-01 -9.90342051e-02\n",
      "  -5.73052727e-02  9.21941698e-02  2.22423114e-02  5.19667007e-02\n",
      "   5.30090034e-02 -8.84338915e-02 -1.86454672e-02 -5.46867773e-03\n",
      "   2.80002560e-02  8.47785473e-02 -4.33673225e-02 -1.92509163e-02\n",
      "   8.38804021e-02 -1.01717651e-01 -3.00816614e-02 -7.16278329e-02\n",
      "  -1.23682115e-02 -4.83192801e-02 -2.99882982e-02  4.21763994e-02\n",
      "   3.83577570e-02  3.77642103e-02 -4.17937674e-02  9.34632495e-02\n",
      "  -9.89937410e-02  1.91303305e-02 -5.69002293e-02  8.61655921e-02\n",
      "  -4.03942354e-02  5.95493950e-02 -5.32225668e-02 -5.83193079e-02\n",
      "   1.03152655e-02 -1.31874264e-03  8.19616914e-02 -5.25771677e-02\n",
      "   1.41251339e-02 -1.53046802e-01 -7.83769488e-02  9.11000818e-02\n",
      "  -8.72740597e-02  2.33384036e-02 -4.08139341e-02  4.72129323e-02\n",
      "  -3.51874046e-02 -4.33679186e-02  7.83693194e-02  8.15181248e-03\n",
      "   3.90082784e-02  6.50488287e-02  7.22192675e-02  7.93683063e-03\n",
      "   4.59748134e-02 -1.99157521e-02  1.08674718e-02  6.37488663e-02\n",
      "  -6.13651164e-02  6.73391521e-02 -2.78410632e-02  3.43144052e-02\n",
      "  -1.29074633e-01  1.12591470e-02  3.85770872e-02 -7.39110187e-02\n",
      "  -8.23532939e-02  5.71592944e-03 -2.79522128e-02  2.84522362e-02\n",
      "   9.59579647e-02 -6.95927888e-02 -5.74074648e-02  8.18116870e-03\n",
      "  -6.06095418e-02 -7.60668814e-02 -8.68201852e-02  4.50127013e-02\n",
      "  -2.46183760e-02  6.74948245e-02 -5.95102124e-02  4.34269384e-02\n",
      "  -3.14801671e-02  7.65673965e-02 -3.56772020e-02  4.06939164e-02\n",
      "   3.38965841e-02 -3.16179022e-02 -6.72417358e-02 -7.49203637e-02\n",
      "  -6.84727579e-02 -5.73416166e-02 -1.84634943e-02  3.89775746e-02\n",
      "   2.27380339e-02  5.10775205e-03  7.95416012e-02  6.89965114e-02\n",
      "  -6.55456111e-02  9.81009379e-03 -9.32970271e-03 -4.42182869e-02\n",
      "   2.49933936e-02 -2.10908800e-02 -6.69577196e-02  3.00345123e-02\n",
      "   7.85489306e-02  1.79870334e-02 -2.99974270e-02 -5.86006306e-02\n",
      "   3.30578759e-02 -3.51919383e-02 -1.05953783e-01  1.21526327e-02\n",
      "  -8.77016708e-02  8.37633312e-02  8.80367905e-02 -4.91474569e-02\n",
      "   2.76288744e-02  1.31711746e-02 -8.27181265e-02  6.71753213e-02\n",
      "   7.21448138e-02  2.72014760e-03 -7.66397119e-02 -9.06863734e-02\n",
      "  -5.59425764e-02  6.94403276e-02  1.34751409e-01 -9.36748460e-02\n",
      "   2.89721345e-03  7.84630328e-02 -1.02173239e-02  1.75053012e-02\n",
      "  -8.30799043e-02 -1.94411166e-02  7.17266127e-02  1.58657171e-02\n",
      "   2.20444091e-02  2.50725951e-02 -7.25524947e-02 -2.71515511e-02\n",
      "  -5.43472767e-02 -9.72345769e-02  5.80860302e-02  7.26419911e-02\n",
      "   4.63935100e-02 -2.50683893e-02  5.12999892e-02 -1.92879587e-02\n",
      "  -1.13554612e-04 -3.36597450e-02 -5.76510578e-02 -9.42508411e-03\n",
      "   9.64834467e-02  7.86626246e-03  4.84111048e-02  6.83975741e-02\n",
      "  -9.28351190e-03 -8.92612711e-02 -5.65361753e-02  2.95670405e-02\n",
      "  -7.80204982e-02 -2.08153818e-02  9.46702957e-02 -1.20240413e-01\n",
      "  -7.57880360e-02 -6.99437931e-02 -4.64803539e-02 -5.20728678e-02\n",
      "  -5.13803475e-02  2.14205007e-03 -4.07023765e-02  5.18965721e-02\n",
      "  -2.70397812e-02 -3.09376996e-02 -7.80417174e-02 -2.68787239e-02\n",
      "   6.63698837e-02 -5.57501428e-02  1.74694583e-02 -1.00596584e-01\n",
      "  -5.76632544e-02 -1.81811042e-02  7.39792883e-02  6.86884671e-03\n",
      "  -5.29647581e-02 -5.62827811e-02 -1.64821781e-02 -7.25808144e-02\n",
      "  -7.43230954e-02 -4.11370322e-02  4.94049117e-02  1.06472015e-01\n",
      "   3.79830375e-02  8.30322802e-02 -2.40711775e-02 -5.75380549e-02\n",
      "   9.14821774e-02  3.88902836e-02 -9.52110365e-02  7.04168677e-02\n",
      "  -9.85054597e-02  3.29923667e-02 -3.97263244e-02  7.25646690e-02\n",
      "  -2.74359360e-02 -7.05368519e-02  1.45526333e-02  7.19407648e-02\n",
      "  -7.08592907e-02  1.99625753e-02 -7.33893141e-02  6.70759231e-02\n",
      "   2.39880420e-02 -5.44310696e-02  4.60823029e-02  5.53462803e-02\n",
      "   3.03539187e-02  7.52446204e-02  5.31073362e-02 -4.11925353e-02\n",
      "  -1.28723308e-02  3.20316255e-02 -1.46371862e-02 -8.53916034e-02\n",
      "   1.12691820e-02  6.92219567e-03  1.12965122e-01 -3.18316147e-02\n",
      "   4.25562896e-02 -1.11038812e-01 -5.03844656e-02  7.47394860e-02\n",
      "   4.45728227e-02  6.07573020e-04  4.40126695e-02  4.04403321e-02\n",
      "   1.91363897e-02 -5.64029962e-02 -1.49307782e-02 -5.52507602e-02\n",
      "   4.38032672e-02  2.39300448e-02 -8.61352533e-02  7.62200430e-02\n",
      "   7.10090576e-03  5.89251406e-02 -3.67292352e-02  9.12596509e-02\n",
      "   8.61345083e-02  2.33440325e-02  4.86538373e-02 -1.64070148e-02\n",
      "   1.93797436e-03 -4.17432003e-02  2.41540242e-02 -2.16222424e-02\n",
      "  -5.14065959e-02 -6.80242628e-02 -5.25583029e-02 -8.29182118e-02\n",
      "  -1.07637849e-02  6.38572797e-02 -5.75741604e-02  3.12168803e-02\n",
      "   2.70527927e-03 -5.93840294e-02 -5.21162301e-02 -4.48531732e-02\n",
      "  -8.78121555e-02  1.53135536e-02 -3.46360430e-02 -1.08115442e-01\n",
      "  -3.73706967e-02 -5.79994321e-02 -3.00644729e-02 -8.95460322e-02\n",
      "   7.25858733e-02  3.77395041e-02 -1.28744822e-02 -4.10970896e-02\n",
      "  -3.13405395e-02  1.90042760e-02 -4.75458726e-02 -7.57495761e-02\n",
      "  -7.50550702e-02 -3.12984809e-02 -3.89372595e-02  5.97960241e-02\n",
      "  -4.25986797e-02  3.81023139e-02  1.29737228e-01 -7.95175880e-02\n",
      "   3.06220781e-02  5.17642424e-02  6.29024804e-02 -3.45028229e-02\n",
      "   4.59184349e-02 -7.63563067e-02  2.24641897e-02  1.00053996e-01\n",
      "   6.71867281e-02  4.96893115e-02  2.97115557e-02 -1.00465588e-01\n",
      "   5.55851161e-02 -8.06654990e-02  5.89469932e-02  9.42130014e-03\n",
      "  -5.81583939e-02  5.56942634e-02  9.93889570e-03  1.04009070e-01\n",
      "   1.00189550e-02 -9.48419496e-02 -3.58813927e-02  5.21761505e-03\n",
      "   1.28135663e-02 -4.20900695e-02 -3.70902680e-02 -1.95951797e-02\n",
      "  -4.51158211e-02 -7.17997327e-02  4.28106561e-02  3.25127169e-02\n",
      "   1.01862483e-01  4.06772234e-02  8.91908780e-02  1.80995688e-02\n",
      "   4.10026610e-02 -4.44218926e-02  2.00119372e-02 -1.00682363e-01\n",
      "  -9.94354263e-02  5.38138859e-02  2.61038709e-02 -6.35220408e-02\n",
      "  -1.45843963e-03  3.17209214e-02  4.73932065e-02 -1.07203256e-02\n",
      "  -2.14309860e-02  4.39416096e-02  3.10792793e-02 -5.45966923e-02\n",
      "  -2.20986782e-03  5.69674857e-02 -3.57464291e-02  5.45894131e-02\n",
      "   4.28763777e-03  9.71382484e-03  6.57352507e-02  8.85507185e-03\n",
      "  -1.00084106e-02  8.24550632e-04 -4.83066663e-02 -5.59342233e-03\n",
      "   8.05111229e-02 -1.61682665e-02  1.32286912e-02  1.34958416e-01\n",
      "  -4.98995781e-02 -5.39257303e-02 -1.17906787e-01  6.01164587e-02\n",
      "   1.59481205e-02  8.25372636e-02 -2.31732558e-02  6.00144863e-02\n",
      "   5.48401065e-02 -1.07701709e-02  6.98367059e-02  3.88825487e-04\n",
      "  -7.06105828e-02 -8.20036605e-02  2.05744542e-02 -3.20316888e-02\n",
      "  -1.07754320e-01 -7.16233626e-03 -7.72169307e-02 -6.26384467e-02\n",
      "  -1.36144767e-02 -7.68624665e-03  4.40689474e-02 -7.29478598e-02\n",
      "   4.27213646e-02  5.77970929e-02 -4.01287004e-02  5.61380200e-02\n",
      "  -1.39019489e-01 -1.26027707e-02  3.99802327e-02 -4.38410118e-02\n",
      "   3.96314301e-02  3.18969116e-02  2.02782545e-02 -4.32224153e-03\n",
      "   3.29073928e-02 -7.39989951e-02  6.14406466e-02 -1.11883013e-02\n",
      "  -7.46450797e-02  8.95595644e-03 -1.22058811e-03 -4.73570973e-02\n",
      "   3.04124062e-03  5.04595153e-02  6.64725155e-02 -5.34403250e-02\n",
      "   7.09679816e-03 -2.61706021e-02 -7.00387582e-02 -1.95013657e-02\n",
      "   1.46627109e-02  7.79774562e-02 -5.15058488e-02  7.71502480e-02\n",
      "  -7.59032369e-02  2.61757392e-02 -5.01209646e-02 -2.24897321e-02\n",
      "  -5.37661761e-02  3.78975421e-02  5.57088144e-02 -3.22843157e-02\n",
      "   3.73843536e-02  6.94574565e-02  3.57734337e-02  1.86923705e-02\n",
      "   8.96731988e-02  4.80535477e-02  2.37684771e-02 -1.10672615e-01]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    output, h, c = inf_decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "    print(h)\n",
    "    decoded_sentence.append(output[0,0,0])\n",
    "      \n",
    "    # 更新目标序列（长度为1）。\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq = output\n",
    "      \n",
    "    # Update states\n",
    "    states_value = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef5f8610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 24, 12)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c872e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test...\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "print(\"test...\")\n",
    "for i in range(testX.shape[0]):\n",
    "  #print(\"test No.\" + str(i+1))\n",
    "  out.append(decode_predict(testX[i,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9a241b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97475707  0.92699723  0.85066644  0.75398047  0.64555546  0.53709401\n",
      "  0.43255995  0.33093848  0.23639528  0.1529586   0.08218984  0.02104576\n",
      " -0.02926854 -0.07000558 -0.10031152 -0.11863477 -0.12685242 -0.13249505\n",
      " -0.14221413 -0.15461119 -0.16507394 -0.1697786  -0.17384498 -0.18082392]\n"
     ]
    }
   ],
   "source": [
    "cor = np.zeros((24))\n",
    "for i in range(24):\n",
    "    cor[i] = np.corrcoef(testY.reshape(384,24)[:,i],np.array(out)[:,i])[0,1]\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "592724ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.86499506],\n",
       "       [-0.88665622],\n",
       "       [-0.82672739],\n",
       "       [-0.62346232],\n",
       "       [-0.70335156],\n",
       "       [-0.9443807 ],\n",
       "       [-1.13839138],\n",
       "       [-1.15875304],\n",
       "       [-1.15833545],\n",
       "       [-1.18053198],\n",
       "       [-1.17541635],\n",
       "       [-1.00110507],\n",
       "       [-0.80188906],\n",
       "       [-0.6772756 ],\n",
       "       [-0.52563614],\n",
       "       [-0.43219197],\n",
       "       [-0.36762857],\n",
       "       [-0.33536553],\n",
       "       [-0.28663447],\n",
       "       [-0.24307424],\n",
       "       [-0.35159171],\n",
       "       [-0.53687763],\n",
       "       [-0.68450022],\n",
       "       [-0.63430518]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "846f2002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.6586767,\n",
       " -1.8683927,\n",
       " -4.0427427,\n",
       " -7.6841717,\n",
       " -12.690783,\n",
       " -17.296146,\n",
       " -19.81999,\n",
       " -20.749151,\n",
       " -21.032207,\n",
       " -21.112982,\n",
       " -21.135582,\n",
       " -21.141872,\n",
       " -21.143621,\n",
       " -21.14411,\n",
       " -21.144247,\n",
       " -21.144281,\n",
       " -21.14429,\n",
       " -21.144295,\n",
       " -21.144297,\n",
       " -21.144297,\n",
       " -21.144297,\n",
       " -21.144297,\n",
       " -21.144297,\n",
       " -21.144297]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c828c8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.71680033,\n",
       " -1.9435332,\n",
       " -3.9903831,\n",
       " -7.1546187,\n",
       " -11.189546,\n",
       " -14.781495,\n",
       " -16.84002,\n",
       " -17.680498,\n",
       " -17.969387,\n",
       " -18.062328,\n",
       " -18.091578,\n",
       " -18.100714,\n",
       " -18.103563,\n",
       " -18.10445,\n",
       " -18.104729,\n",
       " -18.10481,\n",
       " -18.104837,\n",
       " -18.104845,\n",
       " -18.104847,\n",
       " -18.104849,\n",
       " -18.10485,\n",
       " -18.10485,\n",
       " -18.10485,\n",
       " -18.10485]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_predict(trX[10,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddcabca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.599],\n",
       "       [-0.197],\n",
       "       [ 0.104],\n",
       "       [ 0.308],\n",
       "       [ 0.448],\n",
       "       [ 0.504],\n",
       "       [ 0.688],\n",
       "       [ 0.941],\n",
       "       [ 1.221],\n",
       "       [ 1.313],\n",
       "       [ 1.219],\n",
       "       [ 1.127],\n",
       "       [ 1.024],\n",
       "       [ 0.837],\n",
       "       [ 0.533],\n",
       "       [ 0.155],\n",
       "       [-0.3  ],\n",
       "       [-0.736],\n",
       "       [-1.032],\n",
       "       [-1.209],\n",
       "       [-1.302],\n",
       "       [-1.454],\n",
       "       [-1.539],\n",
       "       [-1.528]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trY[10,:,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
