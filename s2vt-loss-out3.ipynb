{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa131968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import pickle, functools, operator\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import random #shuffle\n",
    "#from tensorflow.keras.utils.vis_utils import plot_model\n",
    "import tensorflow as tf\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b4a5c5",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a6a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CMIPdata(Xdata, Ydata, out , myform):\n",
    "    inp1 = Dataset(Xdata,'r')\n",
    "    inp2 = Dataset(Ydata,'r')\n",
    "\n",
    "    #myform = 5\n",
    "    sst_1 = np.zeros((myform,140,12,24,72))\n",
    "    t300_1 = np.zeros((myform,140,12,24,72))\n",
    "\n",
    "    #首年序列\n",
    "    #i= 21个模式\n",
    "    for i in range(myform):\n",
    "        sst_1[i,:,:,:,:] = inp1.variables['sst1'][1+141*i:141+141*i,0:12,:,:]\n",
    "        t300_1[i,:,:,:,:] = inp1.variables['t300'][1+141*i:141+141*i,0:12,:,:]\n",
    "    #(21,140,12,24,72)\n",
    "    #首年序列\n",
    "\n",
    "    \n",
    "    #flatted扁平化                                             \n",
    "    sst_2 = np.zeros((myform,1680,24,72))\n",
    "    t300_2 = np.zeros((myform,1680,24,72))\n",
    "    for i in range(myform):\n",
    "        for j in range(140):\n",
    "            sst_2[i,j*12:(j+1)*12,:,:] = sst_1[i,j,:,:,:]\n",
    "            t300_2[i,j*12:(j+1)*12,:,:] = t300_1[i,j,:,:,:]\n",
    "        #(21,1680,24,72)\n",
    "        #每个模式 139*12+1 = 1680 个序列 序列长度为12\n",
    "\n",
    "    #丢弃1个月 1862.1\n",
    "    sst_2 = sst_2[:,1:,:,:]\n",
    "    t300_2 = t300_2[:,1:,:,:]#(form,1679,24,72)\n",
    "\n",
    "\n",
    "    winsize = 12\n",
    "    #winnum = 1679-winsize +1 #1668\n",
    "    \"\"\"跳跃六个月滑窗 = 6\"\"\"\n",
    "    ts=6 \n",
    "    #(1679-12)/6+1 =27\n",
    "    winnum=int((1679-12)/ts +1) \n",
    "\n",
    "    sst_3 = np.zeros((myform, winnum, winsize, 24, 72))#(form,1668, 3, 24, 72)\n",
    "    t300_3 = np.zeros((myform, winnum, winsize, 24, 72))\n",
    "\n",
    "    sst_3 = np.zeros((myform,winnum,12,24,72))\n",
    "    t300_3 = np.zeros((myform,winnum,12,24,72))\n",
    "    for i in range(myform):\n",
    "        for j in range(winnum):\n",
    "            #print(j)\n",
    "            sst_3[i,j,:,:,:] = sst_2[i,(j*ts):(j*ts)+winsize,:,:]\n",
    "            t300_3[i,j,:,:,:] = t300_2[i,(j*ts):(j*ts)+winsize,:,:]\n",
    "\n",
    "    sst_4 = np.zeros((int(winnum*myform),12,24,72))\n",
    "    t300_4 = np.zeros((int(winnum*myform),12,24,72))\n",
    "    for i in range(myform):\n",
    "        sst_4[i*winnum:(i+1)*winnum,:,:,:] = sst_3[i,:,:,:,:]\n",
    "        t300_4[i*winnum:(i+1)*winnum,:,:,:] = t300_3[i,:,:,:,:]\n",
    "\n",
    "    #channel = 2\n",
    "    #trX = np.zeros((int(winnum*myform),12,24,72,2))\n",
    "    #trX[:,:,:,:,0] = sst_4\n",
    "    #trX[:,:,:,:,1] = t300_4\n",
    "    trX = np.zeros((int(winnum*myform),12,1728,2))\n",
    "    trX[:,:,:,0] = sst_4.reshape(int(winnum*myform),12,1728)\n",
    "    trX[:,:,:,1] = t300_4.reshape(int(winnum*myform),12,1728)\n",
    "    del sst_1,t300_1,sst_2,t300_2,sst_3,t300_3,sst_4,t300_4\n",
    "    trX = trX.reshape(int(winnum*myform),12,3456)\n",
    "\n",
    "    #保存np数组\n",
    "    #(5859, 12, 24, 72, 2)\n",
    "    #np.save(\"./CMIPdata/CMIP_trX_21_ts6_out.npy\",trX) \n",
    "\n",
    "    #label\n",
    "    #平铺\n",
    "    pr_1 = np.zeros(((myform),1692))#21,1692\n",
    "    for i in range(myform):\n",
    "        for j in range(141):\n",
    "            pr_1[i,j*12:(j+1)*12] = inp2.variables['pr'][(141*i)+j,:,0,0]\n",
    "            \n",
    "    #out\n",
    "    pr_2 =  np.zeros((myform,winnum,out))#form,1668,1\n",
    "    \n",
    "    for i in range(myform):\n",
    "        for j in range(winnum):\n",
    "            pr_2[i,j,:] = pr_1[i,(j*ts):(j*ts)+out]\n",
    "\n",
    "    trY = np.zeros((int(winnum*myform),out))#form*1668,1\n",
    "    for i in range(myform):\n",
    "        trY[i*winnum:(i+1)*winnum,:] = pr_2[i,:,:]\n",
    "\n",
    "    #np.save(\"./CMIPdata/CMIP_trY_21_ts6_out%s.npy\"%out,trY)\n",
    "    trY_decoder_input = np.zeros((int(winnum*myform),out))\n",
    "    trY_decoder_input[:,1:] = trY[:,:-1]\n",
    "\n",
    "    #return trX[1:,:,:] ,trY.reshape(-1,24,1)[1:,:,:] ,trY_decoder_input\n",
    "    return trX ,trY.reshape(-1,24,1) ,trY_decoder_input.reshape(-1,24,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14fa223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SODAdata(Xdata, Ydata, out):\n",
    "    inp1 = Dataset(Xdata,'r')\n",
    "    inp2 = Dataset(Ydata,'r')\n",
    "\n",
    "    #time_step = 1 month                                               \n",
    "    sst_1 = np.zeros((99,12,24,72))\n",
    "    t300_1 = np.zeros((99,12,24,72))\n",
    "\n",
    "    sst_1[:,:,:,:] = inp1.variables['sst'][1:,0:12,:,:]\n",
    "    t300_1[:,:,:,:] = inp1.variables['t300'][1:,0:12,:,:]\n",
    "    #(99,12,24,72)\n",
    "    #1872-1970\n",
    "\n",
    "    sst_2 = np.zeros((1188,24,72))\n",
    "    t300_2 = np.zeros((1188,24,72))\n",
    "\n",
    "    for i in range(99):\n",
    "        sst_2[i*12:(i+1)*12,:,:] = sst_1[i,:,:,:]\n",
    "        t300_2[i*12:(i+1)*12,:,:] = t300_1[i,:,:,:]\n",
    "    #(1188,24,72)\n",
    "    #丢弃一个月，便于制作滑窗序列\n",
    "    sst_2 = sst_2[1:,:,:]\n",
    "    t300_2 = t300_2[1:,:,:]#(1187,24,72)\n",
    "\n",
    "    sst_3 = np.zeros((1176,12,24,72))\n",
    "    t300_3 = np.zeros((1176,12,24,72))\n",
    "    #滑窗\n",
    "    for i in range(1176):\n",
    "        sst_3[i:,:,:] = sst_2[i:i+12,:,:]\n",
    "        t300_3[i,:,:,:] = t300_2[i:i+12,:,:]\n",
    "    #(1176,12,24,72)\n",
    "\n",
    "\n",
    "    #channel = 2\n",
    "    #trX = np.zeros((1176,12,24,72,2))\n",
    "    #trX[:,:,:,:,0] = sst_3\n",
    "    #trX[:,:,:,:,1] = t300_3\n",
    "    trX = np.zeros((1176,12,1728,2))\n",
    "    trX[:,:,:,0] = sst_3.reshape(1176,12,1728)\n",
    "    trX[:,:,:,1] = t300_3.reshape(1176,12,1728)\n",
    "    del sst_1,sst_2,sst_3\n",
    "    del t300_1,t300_2,t300_3\n",
    "    trX = trX.reshape(1176,12,3456)\n",
    "\n",
    "    #保存np数组\n",
    "    #np.save(\"./SODAdata/SODA_trX_ts1_out.npy\",trX)\n",
    "\n",
    "    #label\n",
    "    inpv2 = np.zeros((1200))\n",
    "    for i in range(100):\n",
    "        inpv2[i*12:(i+1)*12] = inp2.variables['pr'][i,:,0,0]\n",
    "    #(1200)\n",
    "\n",
    "    #out = 1\n",
    "    trY = np.zeros((1176,out,1))\n",
    "    #滑窗\n",
    "    for i in range(1176):\n",
    "        trY[i,:,0] = inpv2[i:i+out]   \n",
    "    #(1176,out)\n",
    "    trY_decoder_input = np.zeros((1176,24,1))\n",
    "    trY_decoder_input[:,1:,:] = trY[:,:-1,:]\n",
    "\n",
    "    #np.save(\"./SODAdata/SODA_trY_ts1_out%s.npy\"%out,trY) \n",
    "    #(1176,12,24,72,2)\n",
    "    #(1176,out,1)\n",
    "    return trX ,trY, trY_decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c8dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GOSDAdata(Xdata, Ydata, out):\n",
    "    #test data\n",
    "    inp11 = Dataset(Xdata,'r')\n",
    "    inp22 = Dataset(Ydata,'r')\n",
    "                                              \n",
    "    sst_11 = np.zeros((33,12,24,72)) #1983-2015\n",
    "    t300_11 = np.zeros((33,12,24,72))\n",
    "\n",
    "    sst_11[:,:,:,:] = inp11.variables['sst'][3:,0:12,:,:]\n",
    "    t300_11[:,:,:,:] = inp11.variables['t300'][3:,0:12,:,:]\n",
    "    #(33,12,24,72)\n",
    "\n",
    "    sst_22 = np.zeros((396,24,72))\n",
    "    t300_22 = np.zeros((396,24,72))\n",
    "\n",
    "    for i in range(33):\n",
    "        sst_22[i*12:(i+1)*12,:,:] = sst_11[i,:,:,:]\n",
    "        t300_22[i*12:(i+1)*12,:,:] = t300_11[i,:,:,:]\n",
    "    #(396,24,72)\n",
    "    #丢弃一个月，便于制作滑窗序列\n",
    "    sst_22 = sst_22[1:,:,:]\n",
    "    t300_22 = t300_22[1:,:,:]#(395,24,72)\n",
    "\n",
    "    #滑窗\n",
    "    winsize = 12\n",
    "    winnum = 395-winsize + 1 #384\n",
    "    sst_33 = np.zeros((winnum, winsize, 24, 72))#(384, 12, 24, 72)\n",
    "    t300_33 = np.zeros((winnum, winsize, 24, 72))\n",
    "\n",
    "    for i in range(winnum):\n",
    "        sst_33[i,:,:,:] = sst_22[i:i+12,:,:]\n",
    "        t300_33[i,:,:,:] = t300_22[i:i+12,:,:]\n",
    "    #(384,12,24,72)\n",
    "\n",
    "\n",
    "    #channel = 2\n",
    "    #testX = np.zeros((winnum,12,24,72,2))#384\n",
    "    #testX[:,:,:,:,0] = sst_33\n",
    "    #testX[:,:,:,:,1] = t300_33\n",
    "\n",
    "    testX = np.zeros((winnum,12,1728,2))#384\n",
    "    testX[:,:,:,0] = sst_33.reshape(384,12,1728)\n",
    "    testX[:,:,:,1] = t300_33.reshape(384,12,1728)\n",
    "    del sst_11,sst_22,sst_33\n",
    "    del t300_11,t300_22,t300_33\n",
    "    testX = testX.reshape(384,12,3456)\n",
    "\n",
    "    #label\n",
    "    inpv22 = np.zeros((408))#2017-1984 +1=34\n",
    "    for i in range(34):\n",
    "        inpv22[i*12:(i+1)*12] = inp22.variables['pr'][i+2,:,0,0]# +2:从1984开始\n",
    "        #(408)\n",
    "\n",
    "    testY = np.zeros((winnum,out,1))\n",
    "    #out = 1\n",
    "    #滑窗\n",
    "    for i in range(winnum):\n",
    "        testY[i,:,0] = inpv22[i:i+out]#24\n",
    "    #(384,out,1)\n",
    "\n",
    "    return testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169cb4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#修改output长度\n",
    "def set_output(Y, startout, endout):\n",
    "    outlen = endout-startout+1\n",
    "    trY = np.zeros((Y.shape[0],outlen,1))\n",
    "    trY_decoder_input = np.zeros((Y.shape[0],outlen,1))\n",
    "    \n",
    "    trY = Y[:,(startout-1):endout,:]\n",
    "    trY_decoder_input[:,1:,:] = trY[:,:-1,:]\n",
    "    \n",
    "    return trY, trY_decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa07b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = 24\n",
    "trX, trY, trY_decoder_input= SODAdata('/home/d/Q/saconvlstm/SODA.input.36mon.1871_1970.nc', \\\n",
    "                                      '/home/d/Q/saconvlstm/SODA.label.12mon.1873_1972.nc', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6632bd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trX.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cde30642",
   "metadata": {},
   "outputs": [],
   "source": [
    "trY, trY_decoder_input = set_output(trY, 10, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76776224",
   "metadata": {},
   "outputs": [],
   "source": [
    "myform = 21\n",
    "trX1, trY1, trY_decoder_input1 = CMIPdata('/home/d/Q/saconvlstm/CMIP5.input.36mon.1861_2001.nc',\\\n",
    "                                          '/home/d/Q/saconvlstm/CMIP5.label.12mon.1863_2003.nc', out,  myform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e439748",
   "metadata": {},
   "outputs": [],
   "source": [
    "trY1, trY_decoder_input1 = set_output(trY1, 10, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c89c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "trX = np.append(trX, trX1, axis=0)\n",
    "trY = np.append(trY, trY1, axis=0)\n",
    "trY_decoder_input = np.append(trY_decoder_input, trY_decoder_input1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1bfa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX, testY = GOSDAdata('/home/d/Q/saconvlstm/GODAS.input.36mon.1980_2015.nc',\\\n",
    "                         '/home/d/Q/saconvlstm/GODAS.label.12mon.1982_2017.nc', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d02df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "testY,_ = set_output(testY, 10, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trX.shape)\n",
    "print(trY.shape)\n",
    "print(trY_decoder_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86edaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(trX.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "trX = trX[indices]\n",
    "trY = trY[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9971159a",
   "metadata": {},
   "source": [
    "## Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b511b897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5800603 ],\n",
       "       [0.73728704],\n",
       "       [0.99576062]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trY[2000,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2eaea5cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.5800603 ],\n",
       "       [0.73728704]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trY_decoder_input[2000,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408679a3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19153e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps_encoder=12\n",
    "num_encoder_tokens=3456\n",
    "latent_dim=512\n",
    "time_steps_decoder=24\n",
    "#time_steps_decoder=3\n",
    "num_decoder_tokens=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ac5e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custome_rmse_fn(y_true, y_pred):\n",
    "    \"\"\" custome loss function\n",
    "    The 24 series is not equally weighted, so log1p weighted is used.\n",
    "    This is just my initial try, still have further improvement space.\n",
    "\n",
    "    y_true: batch * 24\n",
    "    \"\"\" \n",
    "    #y_pred = y_pred.numpy().reshape(-1,24)\n",
    "    #y_true = y_true.numpy().reshape(-1,24)\n",
    "    diff = (y_pred - y_true) ** 2\n",
    "    predict_sequence_length = tf.shape(y_true)[-1]#获取序列长度\n",
    "    alpha = [np.log1p(i) for i in range(1, time_steps_decoder+1)]\n",
    "    #alpha = [np.log(i)*j for i,j in zip(range(1, predict_sequence_length+1), [0.65]*4+[1]*7+[1.2]*7+[1.5]*6)]\n",
    "    alpha = tf.reshape(tf.convert_to_tensor(alpha, tf.float32), (1, time_steps_decoder))\n",
    "    #reshape(1,24)\n",
    "    rmse = tf.sqrt(tf.reduce_mean(diff * alpha))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20ae9de5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_775\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, 12, 3456)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, 24, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "endcoder_lstm (LSTM)            [(None, 12, 512), (N 8128512     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 24, 512), (N 1052672     decoder_inputs[0][0]             \n",
      "                                                                 endcoder_lstm[0][1]              \n",
      "                                                                 endcoder_lstm[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_769 (Dense)               (None, 24, 512)      262656      decoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_out (Dense)             (None, 24, 1)        513         dense_769[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 9,444,353\n",
      "Trainable params: 9,444,353\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setting up the encoder\n",
    "encoder_inputs = Input(shape=(time_steps_encoder, num_encoder_tokens), name=\"encoder_inputs\")\n",
    "\"\"\"后为特征数，前为批次数量！！！\"\"\"\n",
    "encoder = LSTM(latent_dim, return_state=True,return_sequences=True, name='endcoder_lstm')\n",
    "# 需要取得LSTM的内部state, 因此设定\"return_state=True\"\n",
    "_, state_h, state_c = encoder(encoder_inputs)\n",
    "# 我们抛弃掉`encoder_outputs`因为我们只需要LSTM cell的内部state参数\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder\n",
    "decoder_inputs = Input(shape=(time_steps_decoder, num_decoder_tokens), name= \"decoder_inputs\")\n",
    "# 我们设定我们的解码器回传整个输出的序列同时也回传内部的states参数\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "# 我们使用`encoder_states`来做为初始值(initial state) <-- 重要\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense1 = Dense(latent_dim, activation ='tanh')\n",
    "decoder_outputs = decoder_dense1(decoder_outputs)\n",
    "decoder_dense = Dense(num_decoder_tokens, name='decoder_out')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 定义一个模型接收encoder_input_data` & `decoder_input_data`做为输入而输出`decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()\n",
    "#plot_model(model, to_file='model_train.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee76fb1d",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "656d3b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "99/99 [==============================] - 25s 252ms/step - loss: 1.0292 - mae: 0.5125 - val_loss: 0.9557 - val_mae: 0.4654\n",
      "Epoch 2/100\n",
      "99/99 [==============================] - 24s 246ms/step - loss: 0.8424 - mae: 0.4134 - val_loss: 0.8791 - val_mae: 0.4310\n",
      "Epoch 3/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.6900 - mae: 0.3412 - val_loss: 0.8155 - val_mae: 0.4000\n",
      "Epoch 4/100\n",
      "99/99 [==============================] - 24s 246ms/step - loss: 0.5922 - mae: 0.2924 - val_loss: 0.7785 - val_mae: 0.3760\n",
      "Epoch 5/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.5307 - mae: 0.2614 - val_loss: 0.7688 - val_mae: 0.3701\n",
      "Epoch 6/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.4713 - mae: 0.2332 - val_loss: 0.7381 - val_mae: 0.3539\n",
      "Epoch 7/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.4252 - mae: 0.2098 - val_loss: 0.7374 - val_mae: 0.3555\n",
      "Epoch 8/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.3972 - mae: 0.1957 - val_loss: 0.7214 - val_mae: 0.3433\n",
      "Epoch 9/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.3752 - mae: 0.1845 - val_loss: 0.7028 - val_mae: 0.3350\n",
      "Epoch 10/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.3454 - mae: 0.1699 - val_loss: 0.7027 - val_mae: 0.3344\n",
      "Epoch 11/100\n",
      "99/99 [==============================] - 24s 246ms/step - loss: 0.3244 - mae: 0.1596 - val_loss: 0.6991 - val_mae: 0.3329\n",
      "Epoch 12/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.3104 - mae: 0.1524 - val_loss: 0.6985 - val_mae: 0.3308\n",
      "Epoch 13/100\n",
      "99/99 [==============================] - 24s 246ms/step - loss: 0.2962 - mae: 0.1458 - val_loss: 0.6991 - val_mae: 0.3319\n",
      "Epoch 14/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.2831 - mae: 0.1393 - val_loss: 0.6887 - val_mae: 0.3276\n",
      "Epoch 15/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.2766 - mae: 0.1361 - val_loss: 0.6922 - val_mae: 0.3260\n",
      "Epoch 16/100\n",
      "99/99 [==============================] - 24s 246ms/step - loss: 0.2645 - mae: 0.1303 - val_loss: 0.6836 - val_mae: 0.3226\n",
      "Epoch 17/100\n",
      "99/99 [==============================] - 24s 246ms/step - loss: 0.2535 - mae: 0.1246 - val_loss: 0.6780 - val_mae: 0.3188\n",
      "Epoch 18/100\n",
      "99/99 [==============================] - 24s 246ms/step - loss: 0.2512 - mae: 0.1234 - val_loss: 0.6878 - val_mae: 0.3229\n",
      "Epoch 19/100\n",
      "99/99 [==============================] - 24s 246ms/step - loss: 0.2457 - mae: 0.1205 - val_loss: 0.6855 - val_mae: 0.3208\n",
      "Epoch 20/100\n",
      "99/99 [==============================] - 24s 246ms/step - loss: 0.2211 - mae: 0.1072 - val_loss: 0.6690 - val_mae: 0.3113\n",
      "Epoch 21/100\n",
      "99/99 [==============================] - 24s 246ms/step - loss: 0.1841 - mae: 0.0905 - val_loss: 0.6670 - val_mae: 0.3097\n",
      "Epoch 22/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.1648 - mae: 0.0815 - val_loss: 0.6661 - val_mae: 0.3090\n",
      "Epoch 23/100\n",
      "99/99 [==============================] - 24s 246ms/step - loss: 0.1524 - mae: 0.0756 - val_loss: 0.6659 - val_mae: 0.3087\n",
      "Epoch 24/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.1440 - mae: 0.0715 - val_loss: 0.6652 - val_mae: 0.3083\n",
      "Epoch 25/100\n",
      "99/99 [==============================] - 24s 246ms/step - loss: 0.1382 - mae: 0.0685 - val_loss: 0.6649 - val_mae: 0.3081\n",
      "Epoch 26/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.1337 - mae: 0.0663 - val_loss: 0.6658 - val_mae: 0.3083\n",
      "Epoch 27/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.1302 - mae: 0.0645 - val_loss: 0.6655 - val_mae: 0.3083\n",
      "Epoch 28/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.1269 - mae: 0.0627 - val_loss: 0.6661 - val_mae: 0.3085\n",
      "Epoch 29/100\n",
      "99/99 [==============================] - 24s 245ms/step - loss: 0.1265 - mae: 0.0625 - val_loss: 0.6661 - val_mae: 0.3085\n",
      "Epoch 30/100\n",
      "99/99 [==============================] - 24s 246ms/step - loss: 0.1261 - mae: 0.0624 - val_loss: 0.6661 - val_mae: 0.3085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcdc7867c70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=100\n",
    "# Early Stopping\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "# Tensorboard callback\n",
    "#logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "# Run training\n",
    "#opt = keras.optimizers.Adam(lr = 0.0001)\n",
    "#RMSprop\n",
    "#当学习停滞时，减少2倍或10倍的学习率常常能获得较好的效果。\n",
    "x = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,patience=2,verbose=0,mode=\"auto\")\n",
    "\n",
    "model.compile(metrics=['mae'], optimizer='Adam', loss='rmse')\n",
    "\n",
    "model.fit([trX, trY_decoder_input], trY, batch_size=64, epochs=epochs, validation_split=0.1, verbose=1, callbacks=[x,earlystopping, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdaffec",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4526a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_777\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_inputs (InputLayer)  [(None, 12, 3456)]        0         \n",
      "_________________________________________________________________\n",
      "endcoder_lstm (LSTM)         [(None, 12, 512), (None,  8128512   \n",
      "=================================================================\n",
      "Total params: 8,128,512\n",
      "Trainable params: 8,128,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"functional_779\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     [(None, 24, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1155 (InputLayer)         [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1156 (InputLayer)         [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 24, 512), (N 1052672     decoder_inputs[0][0]             \n",
      "                                                                 input_1155[0][0]                 \n",
      "                                                                 input_1156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_769 (Dense)               (None, 24, 512)      262656      decoder_lstm[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_out (Dense)             (None, 24, 1)        513         dense_769[1][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,315,841\n",
      "Trainable params: 1,315,841\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "save_model_path = 'model_final'\n",
    "if not os.path.exists(save_model_path):\n",
    "    os.makedirs(save_model_path)\n",
    "\n",
    "# Saving encoder as in training\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Saving decoder states and dense layer \n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense1(decoder_outputs)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "#modle(input, output)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "encoder_model.summary()\n",
    "decoder_model.summary()\n",
    "encoder_model.save(os.path.join(save_model_path, 'encoder_model.h5'))\n",
    "decoder_model.save_weights(os.path.join(save_model_path, 'decoder_model_weights.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1a820",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c84b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predict(input_seq):\n",
    "    # inference encoder model\n",
    "    save_model_path = 'model_final'\n",
    "    inf_encoder_model = load_model(os.path.join(save_model_path, 'encoder_model.h5'))\n",
    "    # inference decoder model\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))#注意是none (none, 1)\n",
    "    decoder_dense1 = Dense(latent_dim,activation ='tanh')\n",
    "    decoder_dense = Dense(num_decoder_tokens)\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense1(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    inf_decoder_model = Model(\n",
    "            [decoder_inputs] + decoder_states_inputs,\n",
    "            [decoder_outputs] + decoder_states)\n",
    "    inf_decoder_model.load_weights(os.path.join(save_model_path, 'decoder_model_weights.h5'))\n",
    "\n",
    "    # 将输入编码为状态向量，states_value用于decoder\n",
    "    states_value = inf_encoder_model.predict(input_seq.reshape(-1, 12, 3456))\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))#(1, 1, 1)\n",
    "    target_seq[0, 0, 0] = 0.\n",
    "    decoded_sentence = []\n",
    "    \n",
    "    for i in range(24):\n",
    "      #print(target_seq)\n",
    "      output, h, c = inf_decoder_model.predict([target_seq] + states_value)\n",
    "      #加入预测值\n",
    "      decoded_sentence.append(output[0,0,0])\n",
    "      \n",
    "      \n",
    "      # 更新目标序列（长度为1）。\n",
    "      target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "      target_seq = output\n",
    "      \n",
    "      # Update states\n",
    "      states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea87f8",
   "metadata": {},
   "source": [
    "## Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa698e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test No.1\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.2\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.3\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.4\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.5\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.6\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.7\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.8\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.9\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.10\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.11\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.12\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.13\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.14\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.15\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.16\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.17\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.18\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.19\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.20\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.21\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.22\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.23\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.24\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.25\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.26\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.27\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.28\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.29\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.30\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.31\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.32\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.33\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.34\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.35\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.36\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "test No.37\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "for i in range(testX.shape[0]):\n",
    "    print(\"test No.\" + str(i+1))\n",
    "    out.append(decode_predict(testX[i,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bfe0e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 3, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e445f6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49911288 0.49208205 0.46317112]\n"
     ]
    }
   ],
   "source": [
    "cor = np.zeros((4))\n",
    "for i in range(4):\n",
    "    cor[i] = np.corrcoef(testY.reshape(384,4)[:,i],np.array(out)[:,i])[0,1]\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203c4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = np.zeros((24))\n",
    "for i in range(24):\n",
    "    cor[i] = np.corrcoef(testY.reshape(384,24)[:,i],np.array(out)[:,i])[0,1]\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "603d7a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/d/Q/result.txt\",\"a\") as f:\n",
    "    f.write(str(cor) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2acee3",
   "metadata": {},
   "source": [
    "## Correlation coefficient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
