{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "修改channel 加入ht 长方形patch计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_index_soda:940\n",
      "split_index:800\n",
      "(1000, 12, 2, 24, 72)\n",
      "(1176, 12, 2, 24, 72)\n",
      "(384, 12, 2, 24, 72)\n",
      "(384, 1)\n",
      "384\n",
      "(1176, 12, 2, 24, 72)\n",
      "----------------------------Train Data----------------------------\n",
      "(800, 12, 2, 24, 72)\n",
      "(800, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import random #shuffle\n",
    "from netCDF4 import Dataset\n",
    "def CMIPdata(Xdata, Ydata, out , myform):\n",
    "    inp1 = Dataset(Xdata,'r')\n",
    "    inp2 = Dataset(Ydata,'r')\n",
    "    sst_1 = np.zeros((myform,140,12,24,72))\n",
    "    t300_1 = np.zeros((myform,140,12,24,72))\n",
    "\n",
    "    #首年序列\n",
    "    #i= 21个模式\n",
    "    for i in range(myform):\n",
    "        sst_1[i,:,:,:,:] = inp1.variables['sst1'][1+141*i:141+141*i,0:12,:,:]\n",
    "        t300_1[i,:,:,:,:] = inp1.variables['t300'][1+141*i:141+141*i,0:12,:,:]\n",
    "    #(21,140,12,24,72)\n",
    "    #首年序列\n",
    "\n",
    "    \n",
    "    #flatted扁平化                                             \n",
    "    sst_2 = np.zeros((myform,1680,24,72))\n",
    "    t300_2 = np.zeros((myform,1680,24,72))\n",
    "    for i in range(myform):\n",
    "        for j in range(140):\n",
    "            sst_2[i,j*12:(j+1)*12,:,:] = sst_1[i,j,:,:,:]\n",
    "            t300_2[i,j*12:(j+1)*12,:,:] = t300_1[i,j,:,:,:]\n",
    "    \n",
    "        #(21,1680,24,72)\n",
    "        #每个模式 139*12+1 = 1680 个序列 序列长度为12\n",
    "\n",
    "    #丢弃1个月 1862.1\n",
    "    sst_2 = sst_2[:,1:,:,:]\n",
    "    t300_2 = t300_2[:,1:,:,:]\n",
    "    #(form,1679,24,72)\n",
    "\n",
    "\n",
    "    winsize = 12\n",
    "    #winnum = 1679-winsize +1 #1668\n",
    "    \"\"\"跳跃六个月滑窗 = 6\"\"\"\n",
    "    ts=6 \n",
    "    #(1679-12)/6+1 =27\n",
    "    winnum=int((1679-12)/ts +1) \n",
    "    \n",
    "    sst_3 = np.zeros((myform,winnum, 12, 24, 72))\n",
    "    t300_3 = np.zeros((myform, winnum, 12, 24, 72))\n",
    "    for i in range(myform):\n",
    "        for j in range(winnum):\n",
    "            #print(j)\n",
    "            sst_3[i,j,:,:,:] = sst_2[i,(j*ts):(j*ts)+winsize,:,:]\n",
    "            t300_3[i,j,:,:,:] = t300_2[i,(j*ts):(j*ts)+winsize,:,:]\n",
    "            \n",
    "\n",
    "    sst_4 = np.zeros((int(winnum*myform),12,24,72))\n",
    "    t300_4 = np.zeros((int(winnum*myform),12,24,72))\n",
    "    for i in range(myform):\n",
    "        sst_4[i*winnum:(i+1)*winnum,:,:,:] = sst_3[i,:,:,:,:]\n",
    "        t300_4[i*winnum:(i+1)*winnum,:,:,:] = t300_3[i,:,:,:,:]\n",
    "\n",
    "\n",
    "    trX = np.zeros((int(winnum*myform),12, 2, 24, 72))\n",
    "    trX[:,:,0,:,:] = sst_4\n",
    "    trX[:,:,1,:,:] = t300_4\n",
    "    del sst_1,sst_2,sst_3,sst_4\n",
    "    del t300_1,t300_2,t300_3,t300_4\n",
    "\n",
    "    #保存np数组\n",
    "    #(5859, 12, 2, 24, 72 )\n",
    "    #np.save(\"./CMIPdata/CMIP_trX_21_ts6_out.npy\",trX) \n",
    "\n",
    "    #label\n",
    "    #平铺\n",
    "    pr_1 = np.zeros(((myform),1692))#21,1692\n",
    "    for i in range(myform):\n",
    "        for j in range(141):\n",
    "            pr_1[i,j*12:(j+1)*12] = inp2.variables['pr'][(141*i)+j,:,0,0]\n",
    "            \n",
    "    #out\n",
    "    pr_2 =  np.zeros((myform,winnum,out))#form,1668,1\n",
    "    for i in range(myform):\n",
    "        for j in range(winnum):\n",
    "            pr_2[i,j,:] = pr_1[i,(j*ts):(j*ts)+out]\n",
    "\n",
    "    trY = np.zeros((int(winnum*myform),out))#form*1668,1\n",
    "    for i in range(myform):\n",
    "        trY[i*winnum:(i+1)*winnum,:] = pr_2[i,:,:]\n",
    "\n",
    "    #np.save(\"./CMIPdata/CMIP_trY_21_ts6_out%s.npy\"%out,trY)\n",
    "    trY_decoder_input = np.zeros((int(winnum*myform),out))\n",
    "    trY_decoder_input[:,1:] = trY[:,:-1]\n",
    "\n",
    "    #return trX[1:,:,:] ,trY.reshape(-1,24,1)[1:,:,:] ,trY_decoder_input\n",
    "    return trX ,trY.reshape(-1,24,1) ,trY_decoder_input.reshape(-1,24,1)\n",
    "\n",
    "def GOSDAdata(Xdata, Ydata, out):\n",
    "    #test data\n",
    "    inp11 = Dataset(Xdata,'r')\n",
    "    inp22 = Dataset(Ydata,'r')\n",
    "                                              \n",
    "    sst_11 = np.zeros((33,12,24,72)) #1983-2015\n",
    "    t300_11 = np.zeros((33,12,24,72))\n",
    "\n",
    "    sst_11[:,:,:,:] = inp11.variables['sst'][3:,0:12,:,:]\n",
    "    t300_11[:,:,:,:] = inp11.variables['t300'][3:,0:12,:,:]\n",
    "    #(33,12,24,72)\n",
    "\n",
    "    sst_22 = np.zeros((396,24,72))\n",
    "    t300_22 = np.zeros((396,24,72))\n",
    "\n",
    "    for i in range(33):\n",
    "        sst_22[i*12:(i+1)*12,:,:] = sst_11[i,:,:,:]\n",
    "        t300_22[i*12:(i+1)*12,:,:] = t300_11[i,:,:,:]\n",
    "    #(396,24,72)\n",
    "    #丢弃一个月，便于制作滑窗序列\n",
    "    sst_22 = sst_22[1:,:,:]\n",
    "    t300_22 = t300_22[1:,:,:]#(395,24,72)\n",
    "\n",
    "    #滑窗\n",
    "    winsize = 12\n",
    "    winnum = 395-winsize + 1 #384\n",
    "    sst_33 = np.zeros((winnum, winsize, 24, 72))#(384, 12, 24, 72)\n",
    "    t300_33 = np.zeros((winnum, winsize, 24, 72))\n",
    "\n",
    "    for i in range(winnum):\n",
    "        sst_33[i,:,:,:] = sst_22[i:i+12,:,:]\n",
    "        t300_33[i,:,:,:] = t300_22[i:i+12,:,:]\n",
    "    #(384,12,24,72)\n",
    "\n",
    "\n",
    "    #channel = 2\n",
    "    #testX = np.zeros((winnum,12,24,72,2))#384\n",
    "    #testX[:,:,:,:,0] = sst_33\n",
    "    #testX[:,:,:,:,1] = t300_33\n",
    "    #sst_33 = sst_33.swapaxes(1, 3)\n",
    "\n",
    "    testX = np.zeros((winnum, 12, 2, 24, 72))#384\n",
    "    testX[:,:,0,:,:] = sst_33\n",
    "    testX[:,:,0,:,:] = t300_33\n",
    "    del sst_11,sst_22,sst_33\n",
    "    del t300_11,t300_22,t300_33\n",
    "    #testX = testX.reshape(384,12,3456)\n",
    "\n",
    "    #label\n",
    "    inpv22 = np.zeros((408))#2017-1984 +1=34\n",
    "    for i in range(34):\n",
    "        inpv22[i*12:(i+1)*12] = inp22.variables['pr'][i+2,:,0,0]# +2:从1984开始\n",
    "        #(408)\n",
    "\n",
    "    testY = np.zeros((winnum,out,1))\n",
    "    #out = 1\n",
    "    #滑窗\n",
    "    for i in range(winnum):\n",
    "        testY[i,:,0] = inpv22[i:i+out]#24\n",
    "    #(384,out,1)\n",
    "\n",
    "    return testX, testY\n",
    "\n",
    "def SODAdata(Xdata, Ydata, out):\n",
    "    inp1 = Dataset(Xdata,'r')\n",
    "    inp2 = Dataset(Ydata,'r')\n",
    "\n",
    "    #time_step = 1 month                                               \n",
    "    sst_1 = np.zeros((99,12,24,72))\n",
    "    t300_1 = np.zeros((99,12,24,72))\n",
    "\n",
    "    sst_1[:,:,:,:] = inp1.variables['sst'][1:,0:12,:,:]\n",
    "    t300_1[:,:,:,:] = inp1.variables['t300'][1:,0:12,:,:]\n",
    "    #(99,12,24,72)\n",
    "    #1872-1970\n",
    "\n",
    "    sst_2 = np.zeros((1188,24,72))\n",
    "    t300_2 = np.zeros((1188,24,72))\n",
    "\n",
    "    for i in range(99):\n",
    "        sst_2[i*12:(i+1)*12,:,:] = sst_1[i,:,:,:]\n",
    "        t300_2[i*12:(i+1)*12,:,:] = t300_1[i,:,:,:]\n",
    "    #(1188,24,72)\n",
    "    #丢弃一个月，便于制作滑窗序列\n",
    "    sst_2 = sst_2[1:,:,:]\n",
    "    t300_2 = t300_2[1:,:,:]#(1187,24,72)\n",
    "\n",
    "    sst_3 = np.zeros((1176,12,24,72))\n",
    "    t300_3 = np.zeros((1176,12,24,72))\n",
    "    #滑窗\n",
    "    for i in range(1176):\n",
    "        sst_3[i:,:,:] = sst_2[i:i+12,:,:]\n",
    "        t300_3[i,:,:,:] = t300_2[i:i+12,:,:]\n",
    "    #(1176,12,24,72)\n",
    "\n",
    "    #channel = 2\n",
    "    #trX = np.zeros((1176,12,24,72,2))\n",
    "    #trX[:,:,:,:,0] = sst_3\n",
    "    #trX[:,:,:,:,1] = t300_3\n",
    "    trX = np.zeros((1176,12,2,24,72))\n",
    "    trX[:,:,0,:,:] = sst_3\n",
    "    trX[:,:,1,:,:] = t300_3\n",
    "    del sst_1,sst_2,sst_3\n",
    "    del t300_1,t300_2,t300_3\n",
    "\n",
    "    #保存np数组\n",
    "    #np.save(\"./SODAdata/SODA_trX_ts1_out.npy\",trX)\n",
    "\n",
    "    #label\n",
    "    inpv2 = np.zeros((1200))\n",
    "    for i in range(100):\n",
    "        inpv2[i*12:(i+1)*12] = inp2.variables['pr'][i,:,0,0]\n",
    "    #(1200)\n",
    "\n",
    "    #out = 1\n",
    "    trY = np.zeros((1176,out,1))\n",
    "    #滑窗\n",
    "    for i in range(1176):\n",
    "        trY[i,:,0] = inpv2[i:i+out]#24\n",
    "    #(1176,24)\n",
    "    \n",
    "    \n",
    "    trY_decoder_input = np.zeros((1176,out,1))\n",
    "    trY_decoder_input[:,1:,:] = trY[:,:-1,:]\n",
    "\n",
    "    #np.save(\"./SODAdata/SODA_trY_ts1_out%s.npy\"%out,trY) \n",
    "    return trX , trY, trY_decoder_input\n",
    "\n",
    "trX, trY, trY_decoder_input = CMIPdata('/home/d/Q/liyanqiu/saconvlstm/CMIP5.input.36mon.1861_2001.nc',\\\n",
    "                                          '/home/d/Q/liyanqiu/saconvlstm/CMIP5.label.12mon.1863_2003.nc', 24,  21)\n",
    "\n",
    "testX, testY = GOSDAdata('/home/d/Q/liyanqiu/saconvlstm/GODAS.input.36mon.1980_2015.nc',\\\n",
    "                         '/home/d/Q/liyanqiu/saconvlstm/GODAS.label.12mon.1982_2017.nc', 24)\n",
    "\n",
    "trX_SODA, trY_SODA ,trY_decoder_input_SODA = SODAdata('/home/d/Q/liyanqiu/saconvlstm/SODA.input.36mon.1871_1970.nc',\n",
    "                              '/home/d/Q/liyanqiu/saconvlstm/SODA.label.12mon.1873_1972.nc',24)\n",
    "\n",
    "\n",
    "indices = np.arange(trX.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "trX = trX[indices]\n",
    "trY = trY[indices]\n",
    "trY_decoder_input = trY_decoder_input[indices]\n",
    "\n",
    "\n",
    "# shuffle\n",
    "indices_soda = np.arange(trX_SODA.shape[0])\n",
    "np.random.shuffle(indices_soda)\n",
    "trX_SODA = trX_SODA[indices_soda]\n",
    "trY_SODA = trY_SODA[indices_soda]\n",
    "trY_decoder_input_SODA = trY_decoder_input_SODA[indices_soda]\n",
    "class MyDataSet(Data.Dataset):\n",
    "    \"\"\"自定义DataLoader\"\"\"\n",
    "    def __init__(self, enc_inputs,  dec_outputs):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.enc_inputs = enc_inputs\n",
    "        #self.dec_inputs = dec_inputs\n",
    "        self.dec_outputs = dec_outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.enc_inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx],  self.dec_outputs[idx]\n",
    "    \n",
    "batch_size = 16\n",
    "trainsize = 1000 #选择cmip训练数量\n",
    "trX = trX[:trainsize]\n",
    "trY = trY[:trainsize]\n",
    "\n",
    "#train split\n",
    "split_index = int(trX.shape[0]*0.8)\n",
    "split_index_soda = int(trX_SODA.shape[0]*0.8)\n",
    "\n",
    "print(\"split_index_soda:\" + str(split_index_soda))\n",
    "print(\"split_index:\" + str(split_index))\n",
    "\n",
    "\n",
    "target_month = 3\n",
    "loader_train = Data.DataLoader(\n",
    "    MyDataSet(\n",
    "        trX[:split_index,].astype(np.float32), \n",
    "        trY[:split_index, target_month-1:target_month].astype(np.float32).reshape((-1,1))), \n",
    "    batch_size, shuffle = True)\n",
    "loader_valid = Data.DataLoader(\n",
    "    MyDataSet(\n",
    "        trX[split_index:,].astype(np.float32), \n",
    "        trY[split_index:, target_month-1:target_month].astype(np.float32).reshape((-1,1))), \n",
    "    batch_size, shuffle=False)\n",
    "\n",
    "loader_train_sado = Data.DataLoader(\n",
    "    MyDataSet(\n",
    "        trX_SODA[:split_index_soda,].astype(np.float32), \n",
    "        trY_SODA[:split_index_soda, target_month-1:target_month].astype(np.float32).reshape((-1,1))), \n",
    "    batch_size, shuffle = True)\n",
    "loader_valid_soda = Data.DataLoader(\n",
    "    MyDataSet(\n",
    "        trX_SODA[split_index_soda:,].astype(np.float32), \n",
    "        trY_SODA[split_index_soda:, target_month-1:target_month].astype(np.float32).reshape((-1,1))), \n",
    "    batch_size, shuffle=False)\n",
    "\n",
    "#testdataloader\n",
    "\n",
    "loader_test = Data.DataLoader(\n",
    "    MyDataSet(\n",
    "        testX[:,:].astype(np.float32), \n",
    "        testY[:, target_month-1:target_month].astype(np.float32).reshape((-1,1))), \n",
    "    batch_size, shuffle = False)\n",
    "\n",
    "testY = testY[:, target_month-1:target_month].astype(np.float32).reshape((-1,1))\n",
    "\n",
    "print(trX.shape)\n",
    "print(trX_SODA .shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)\n",
    "print(len(testX))\n",
    "print(trX_SODA.shape)\n",
    "print(\"----------------------------Train Data----------------------------\")\n",
    "print(trX[:split_index,].astype(np.float32).shape)\n",
    "print(trY[:split_index, target_month-1:target_month].reshape((-1,1)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if on cuda: True\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, *args, **kwargs)\n",
    "    \n",
    "#feed forward\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1) # 在给定的维度上将张量进行分块。沿着-1分成两块\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)#geglu大小除以2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "#attention\n",
    "def attn(q, k, v):\n",
    "    sim = einsum('b i d, b j d -> b i j', q, k)\n",
    "    attn = sim.softmax(dim = -1)\n",
    "    out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "    return out\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, einops_from, einops_to, **einops_dims):\n",
    "        h = self.heads\n",
    "        #print(x.shape)[16, 28, 64]\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)#参数在to_qkv里边\n",
    "        #print(q.shape)[16, 28, 256]\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))#qkv头参数\n",
    "        q *= self.scale\n",
    "\n",
    "        # splice out classification token at index 1\n",
    "        # 在索引1处拼接出分类标记\n",
    "        (cls_q, q_), (cls_k, k_), (cls_v, v_) = map(lambda t: (t[:, 0:1], t[:, 1:]), (q, k, v))\n",
    "        #[64, 1, 64]\n",
    "        #[64, 27, 64]\n",
    "        \n",
    "        \"\"\"【timesformer patch 2修改部分，对classtoken的融合机制进行修改】\"\"\"\n",
    "\n",
    "        # let classification token attend to key / values of all patches across time and space\n",
    "        # 让cla token关注跨时间和空间的所有patch的键/值\n",
    "        #cls_out = attn(cls_q, k, v)# ！！！对q的处理\n",
    "        #[64, 1, 64]\n",
    "\n",
    "        # rearrange across time or space\n",
    "        # 将空间/时间维度移动到批处理维度以处理时间/空间注意力\n",
    "        q_, k_, v_ = map(lambda t: rearrange(t, f'{einops_from} -> {einops_to}', **einops_dims), (q_, k_, v_))\n",
    "\n",
    "        # expand cls token keys and values across time or space and concat\n",
    "        # 跨越时间或空间扩展clstoken的键和值，并将其连接起来。\n",
    "        r = q_.shape[0] // cls_k.shape[0]# 控制时间或者空间 n or f\n",
    "        cls_q, cls_k, cls_v = map(lambda t: repeat(t, 'b () d -> (b r) () d', r = r), (cls_q, cls_k, cls_v))\n",
    "        #print(cls_q.shape)\n",
    "        \n",
    "        \n",
    "        q_ = torch.cat((cls_q, q_), dim = 1)\n",
    "        k_ = torch.cat((cls_k, k_), dim = 1)\n",
    "        v_ = torch.cat((cls_v, v_), dim = 1)\n",
    "        #[576, 3+1, 64] [192, 9+1, 64]\n",
    "\n",
    "\n",
    "        # attention\n",
    "        #q_ [576, 3, 64] 没有class token\n",
    "        out = attn(q_, k_, v_)\n",
    "        # out [576, 3, 64] /[192, 9, 64]\n",
    "        \n",
    "        \n",
    "        cls_out, out = out[:, 0:1], out[:, 1:]\n",
    "        # merge back time or space 合并时间或空间\n",
    "        out = rearrange(out, f'{einops_to} -> {einops_from}', **einops_dims)#恢复\n",
    "        # [64, 27, 64] 没有class token\n",
    "        cls_out = rearrange(cls_out, f'{einops_to} -> {einops_from}', **einops_dims)#恢复\n",
    "        \n",
    "        cls_out_new = torch.mean(cls_out, dim=1).reshape(cls_out.shape[0], 1, cls_out.shape[2])\n",
    "        #print(cls_out_new.shape)\n",
    "        \n",
    "\n",
    "        # concat back the cls token 连接回来的cls token\n",
    "        out = torch.cat((cls_out_new, out), dim = 1)#[16, , d]\n",
    "        #[64, 28, 64]\n",
    "\n",
    "        # merge back the heads 合并后的头\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)# [16, 109, inner_dim] inner_dim = dim_head * heads所有头的平铺\n",
    "        #  [16, 28, 256]\n",
    "\n",
    "        # combine heads out 合并head输出\n",
    "        return self.to_out(out) # batch * dim\n",
    "    \n",
    "    \n",
    "# main classes\n",
    "class TimeSformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        num_frames,\n",
    "        num_classes,\n",
    "        image_size = 72,\n",
    "        patch_size = 12,\n",
    "        channels = 2,\n",
    "        depth = 12,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        #符合则往下运行\n",
    "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_size // patch_size) ** 2 #平方个数 //向下取整\n",
    "        num_positions = num_frames * num_patches # 12*num_patches\n",
    "        patch_dim = channels * patch_size * (patch_size//3)# patch大小\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.to_patch_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.pos_emb = nn.Embedding(num_positions + 1, dim)# 总共出现num_positions + 1个词,多了一个cls_token patch 用多少维来表示一个符号\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, dim))# [1, dim] 一个可学习的参数\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)), # Time attention\n",
    "                PreNorm(dim, Attention(dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)), # Spatial attention\n",
    "                PreNorm(dim, FeedForward(dim, dropout = ff_dropout)) # Feed Forward\n",
    "            ]))\n",
    "            #append到layers中\n",
    "            \n",
    "        # 多尺度CNN部分\n",
    "        self.projs = nn.ModuleList()\n",
    "        kernel_size = [17, 9, 5]\n",
    "        out_chans = 10\n",
    "        for ps in kernel_size:\n",
    "            stride = 1\n",
    "            padding = (ps - stride) // 2\n",
    "            self.projs.append(nn.Conv2d(2, out_chans, kernel_size=ps, stride=stride, padding=padding))\n",
    "            \n",
    "        \"\"\"\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "        \"\"\"\n",
    "    \n",
    "        self.to_out = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, video):\n",
    "        # (batch x frames x channels x height x width)\n",
    "        b, f, _, h, w, *_, device, p = *video.shape, video.device, self.patch_size\n",
    "        video = rearrange(video, 'b f c h w -> (b f) c h w')\n",
    "        xs = []\n",
    "        for i in range(len(self.projs)):\n",
    "            tx = self.projs[i](video)\n",
    "            #print(i)\n",
    "            #print(tx.shape)#[192, 10, 23, 71]\n",
    "            tx = rearrange(tx, '(b1 f1) c h w -> b1 f1 c h w', b1 = b, f1 = f)\n",
    "            xs.append(tx)\n",
    "        msvideo = torch.cat(xs, dim=2) #按channel维度拼接\n",
    "        \n",
    "        #print(x.shape)#torch.Size([16, 12, 30, 24, 72])\n",
    "        h, w= msvideo.shape[3], msvideo.shape[4]\n",
    "        assert h % (p//3) == 0 and w % p == 0, f'height {h} and width {w} of video must be divisible by the patch size {p}'\n",
    "        #print(h) 24\n",
    "        #print(w) 72\n",
    "\n",
    "        n = (h // (p//3)) * (w // p)# patch个数 3*3个patch p = 24\n",
    "        #print(n)\n",
    "        # 将w*h*c图片切分为N个p1*p2*c的图像块，序列长度变成f*h*w/(p*p/3)\n",
    "        msvideo = rearrange(msvideo, 'b f c (h p1) (w p2) -> b (f h w) (p1 p2 c)', p1 = p//3, p2 = p)\n",
    "        #print(\"patch finish\")\n",
    "        #print(video.shape) [16, 27, 384]\n",
    "        # torch.Size([16, 108, 384]) 9*12 = 108\n",
    "        \n",
    "        #数据处理完毕\n",
    "        tokens = self.to_patch_embedding(msvideo)# 对每个patch进行嵌入\n",
    "\n",
    "        cls_token = repeat(self.cls_token, 'n d -> b n d', b = b)# [batch_size, 1, dim]\n",
    "        x =  torch.cat((cls_token, tokens), dim = 1)#按照frames维度进行拼接 [batch_size, (f h w)+1, dim] 对每一个样本增加了一个cls_token\n",
    "        x += self.pos_emb(torch.arange(x.shape[1], device = device)) #位置编码\n",
    "        #print(x.shape)[16, 28, 64]\n",
    "\n",
    "        for (time_attn, spatial_attn, ff) in self.layers:#layers 循环 [ [1, 2, 3], [1, 2, 3], [1, 2, 3] ] d二维list\n",
    "            x = time_attn(x, 'b (f n) d', '(b n) f d', n = n) + x\n",
    "            x = spatial_attn(x, 'b (f n) d', '(b f) n d', f = f) + x\n",
    "            x = ff(x) + x\n",
    "            \n",
    "        # print(x.shape) [16, 109, 64]\n",
    "\n",
    "        cls_token = x[:, 0] #把patch做一个融合 取cls_token！\n",
    "        #print(cls_token.shape)\n",
    "        \n",
    "        return self.to_out(cls_token)\n",
    "    \n",
    "    \n",
    "#Parameters\n",
    "device = 'cuda'\n",
    "DIM = 64\n",
    "IMAGE_SIZE = 72\n",
    "PATCH_SIZE = 18 #24 18 12 9 6\n",
    "NUM_CLASSES = 1\n",
    "NUM_FRAMES = 12\n",
    "DEPTH = 2 # block个数\n",
    "HEADS = 8 \n",
    "DIM_HEAD = 64\n",
    "ATTN_DROPOUT = 0.\n",
    "FF_DROPOUT = 0.\n",
    "\n",
    "# (batch x frames x channels x height x width)\n",
    "model = TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, \\\n",
    "        num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, \\\n",
    "            ff_dropout = FF_DROPOUT).to(device)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "#criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "print(\"if on cuda:\",next(model.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "================================================================================2022-08-14 19:59:35\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function MmBackward returned an invalid gradient at index 0 - got [3072, 216] but expected shape compatible with [3072, 3240]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/d/Q/liyanqiu/timesformer/timesformer patch 2.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.69.28/home/d/Q/liyanqiu/timesformer/timesformer%20patch%202.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_pred, dec_outputs) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.69.28/home/d/Q/liyanqiu/timesformer/timesformer%20patch%202.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# 反向\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.6.69.28/home/d/Q/liyanqiu/timesformer/timesformer%20patch%202.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.69.28/home/d/Q/liyanqiu/timesformer/timesformer%20patch%202.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.6.69.28/home/d/Q/liyanqiu/timesformer/timesformer%20patch%202.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# print\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/enso-q/lib/python3.8/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/enso-q/lib/python3.8/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    148\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    149\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function MmBackward returned an invalid gradient at index 0 - got [3072, 216] but expected shape compatible with [3072, 3240]"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import datetime\n",
    "dfhistory = pd.DataFrame(columns = [\"epoch\",\"loss\",\"val_loss\"]) \n",
    "print(\"Start Training...\")\n",
    "nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"==========\"*8 + \"%s\"%nowtime)\n",
    "bestloss = float(\"inf\")\n",
    "epoch = 15\n",
    "model_epoch = 0\n",
    "for epoch in range(1,epoch+1):\n",
    "    #train----------------------------------------------------\n",
    "  model.train()\n",
    "  loss_sum = 0.0\n",
    "  step = 1\n",
    "  for step,(enc_inputs, dec_outputs) in enumerate(loader_train, 1):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 正向\n",
    "      enc_inputs, dec_outputs = enc_inputs.to(device), dec_outputs.to(device)\n",
    "      y_pred = model(enc_inputs)\n",
    "      \n",
    "      loss = criterion(y_pred, dec_outputs) \n",
    "\n",
    "      # 反向\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # print\n",
    "      loss_sum += loss.item()\n",
    "      #if step%40 == 0:\n",
    "        #print((\"[step = %d] loss: %.3f, \") % (step, loss_sum/step))\n",
    "\n",
    "\n",
    "  #valid\n",
    "  model.eval()\n",
    "  val_loss_sum = 0.0\n",
    "  val_step = 1\n",
    "\n",
    "  for val_step, (enc_inputs, dec_outputs) in enumerate(loader_valid, 1):\n",
    "    with torch.no_grad():# 节点不进行求梯度\n",
    "      enc_inputs, dec_outputs = enc_inputs.to(device), dec_outputs.to(device)\n",
    "      outputs = model(enc_inputs)\n",
    "      #val_loss = criterion(outputs.view(-1), dec_outputs.view(-1)) \n",
    "      val_loss =criterion(outputs, dec_outputs) \n",
    "\n",
    "    val_loss_sum += val_loss.item()\n",
    "\n",
    "\n",
    "  # log\n",
    "  info = (epoch, loss_sum/step, val_loss_sum/val_step)\n",
    "  dfhistory.loc[epoch-1] = info # epoch从1开始 index -1\n",
    "\n",
    "  # print\n",
    "  print((\"\\nEPOCH = %d, loss = %.3f,\"+\" val_loss = %.3f, \")% info)\n",
    "  nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "  print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "\n",
    "  if val_loss_sum <= bestloss: #换成小于等于比小于效果更好\n",
    "        bestloss = val_loss_sum\n",
    "        torch.save(model,'bestmodel.pkl')\n",
    "        model_epoch = epoch\n",
    "        #print(\"bestmodel updata\")\n",
    "        \n",
    "\n",
    "print('Finished Training...')\n",
    "\n",
    "out = []\n",
    "# batch_size\n",
    "for step,(inputs, _) in enumerate(loader_test, 1):\n",
    "    pred = model(inputs.to(device))\n",
    "    out.append(pred.squeeze().cpu().detach().numpy())\n",
    "\n",
    "out = np.array(out)    \n",
    "print(out.shape)\n",
    "\n",
    "#——————————————cor————————————————\n",
    "cor = np.zeros((1))\n",
    "cor = np.corrcoef(testY[:,0],out.reshape((-1)))[0,1]\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.70335156, -0.9443807 , -1.1383914 , -1.158753  , -1.1583354 ,\n",
       "       -1.180532  , -1.1754164 , -1.0011051 , -0.80188906, -0.6772756 ,\n",
       "       -0.52563614, -0.43219197, -0.36762857, -0.33536553, -0.28663447,\n",
       "       -0.24307424, -0.3515917 , -0.53687763, -0.6845002 , -0.6343052 ,\n",
       "       -0.4556363 , -0.19504076,  0.00893648,  0.22462256,  0.45296106,\n",
       "        0.6640037 ,  0.8641342 ,  1.0855862 ,  1.2000219 ,  1.2075744 ,\n",
       "        1.0670185 ,  1.0259333 ,  1.0872009 ,  1.2204103 ,  1.3105447 ,\n",
       "        1.3004302 ,  1.2650048 ,  1.2087082 ,  1.0506958 ,  0.8041709 ,\n",
       "        0.4654007 ,  0.11666613, -0.30131388, -0.82851887, -1.3176688 ,\n",
       "       -1.5566994 , -1.4677974 , -1.3122475 , -1.5108223 , -1.9338089 ,\n",
       "       -2.2517302 , -2.160442  , -1.7987889 , -1.5098666 , -1.281021  ,\n",
       "       -1.0888195 , -0.82842743, -0.5189633 , -0.33908117, -0.2899837 ,\n",
       "       -0.32445404, -0.38313252, -0.33575767, -0.21028854, -0.01515023,\n",
       "        0.04208653,  0.03992243,  0.04767451,  0.04120988,  0.04660106,\n",
       "        0.06847439,  0.09040196,  0.18114355,  0.1812514 ,  0.26534826,\n",
       "        0.36903805,  0.41013163,  0.30415213,  0.21031226,  0.21679474,\n",
       "        0.40402007,  0.5503819 ,  0.6438831 ,  0.5948353 ,  0.6294395 ,\n",
       "        0.8198107 ,  1.2492845 ,  1.5894603 ,  1.8326094 ,  1.7257282 ,\n",
       "        1.5328956 ,  1.2242323 ,  0.8475843 ,  0.44252762,  0.01633326,\n",
       "       -0.17961842, -0.3276879 , -0.32344684, -0.24563648, -0.10196014,\n",
       "        0.06480847,  0.14906168,  0.3515992 ,  0.55135125,  0.6831155 ,\n",
       "        0.5906019 ,  0.3676512 ,  0.25734314,  0.30177748,  0.38777885,\n",
       "        0.37133902,  0.22842109,  0.10190971,  0.03309238,  0.06707333,\n",
       "        0.12203732,  0.16335227,  0.11474749,  0.18566208,  0.23604342,\n",
       "        0.5097363 ,  0.79877144,  1.0645586 ,  1.088005  ,  0.9265459 ,\n",
       "        0.6247551 ,  0.38645035,  0.05201136, -0.12530795, -0.25937837,\n",
       "       -0.36164966, -0.6138202 , -0.92388725, -1.0949565 , -1.1652986 ,\n",
       "       -1.048527  , -0.94049877, -0.7540356 , -0.61543447, -0.52186936,\n",
       "       -0.4892819 , -0.4161863 , -0.3857957 , -0.37010375, -0.45886707,\n",
       "       -0.5301347 , -0.6353683 , -0.6699885 , -0.5842528 , -0.39093214,\n",
       "       -0.09694561,  0.2641984 ,  0.73089784,  1.2152845 ,  1.6448338 ,\n",
       "        1.9806343 ,  2.2726915 ,  2.5032568 ,  2.6692483 ,  2.7015462 ,\n",
       "        2.5454466 ,  2.1473217 ,  1.531126  ,  0.9847365 ,  0.11833646,\n",
       "       -0.6775008 , -1.395592  , -1.4490016 , -1.3960948 , -1.4301504 ,\n",
       "       -1.7156339 , -1.8123902 , -1.6748219 , -1.3105297 , -1.0671539 ,\n",
       "       -0.917839  , -0.9359876 , -0.88732046, -0.99708223, -1.0613419 ,\n",
       "       -1.1331542 , -1.2884588 , -1.4589748 , -1.7087297 , -1.6699474 ,\n",
       "       -1.4557678 , -1.0676273 , -0.75376713, -0.6165181 , -0.53354704,\n",
       "       -0.44493052, -0.38769904, -0.4288796 , -0.6126198 , -0.759524  ,\n",
       "       -0.79941976, -0.6802346 , -0.53336734, -0.38754767, -0.2746736 ,\n",
       "       -0.12908633, -0.00794798,  0.07932987,  0.03291504, -0.02358929,\n",
       "       -0.07938771, -0.14648439, -0.11918377,  0.02364753,  0.16844782,\n",
       "        0.22359058,  0.21381576,  0.3894431 ,  0.55399066,  0.77294713,\n",
       "        0.8920591 ,  1.0958005 ,  1.3055097 ,  1.4293478 ,  1.4103024 ,\n",
       "        1.1708536 ,  0.88625175,  0.4712479 ,  0.02651861, -0.25915244,\n",
       "       -0.20680885, -0.03192539,  0.08953923,  0.18323517,  0.30901146,\n",
       "        0.38826942,  0.32974368,  0.23356986,  0.11729809,  0.0517125 ,\n",
       "        0.09012336,  0.11214868,  0.22077921,  0.38305962,  0.55153453,\n",
       "        0.6511543 ,  0.6348088 ,  0.68105847,  0.64272535,  0.56259394,\n",
       "        0.4326957 ,  0.352455  ,  0.33551723,  0.30353507,  0.26128498,\n",
       "        0.13060793, -0.01490457, -0.0583128 , -0.17529142, -0.36450717,\n",
       "       -0.66237193, -0.7520013 , -0.6993727 , -0.43811488, -0.19285955,\n",
       "        0.05265308,  0.1502771 ,  0.28915092,  0.44048345,  0.6237    ,\n",
       "        0.80931145,  0.94552165,  0.9129927 ,  0.61189896,  0.19821773,\n",
       "       -0.06471152, -0.19931163, -0.19981095, -0.29279733, -0.44169396,\n",
       "       -0.7932753 , -1.2005999 , -1.5126238 , -1.675661  , -1.7396185 ,\n",
       "       -1.7740879 , -1.6021128 , -1.3209245 , -0.97115153, -0.7539861 ,\n",
       "       -0.43802756, -0.20178173, -0.16489099, -0.2809429 , -0.41784844,\n",
       "       -0.5727153 , -0.7414788 , -0.80943024, -0.7142749 , -0.48118338,\n",
       "       -0.18726315,  0.14521702,  0.48162338,  0.6389315 ,  0.7414161 ,\n",
       "        0.80754364,  1.1077127 ,  1.4112077 ,  1.6441729 ,  1.5825492 ,\n",
       "        1.3938177 ,  1.0731653 ,  0.5647548 , -0.07987696, -0.7085675 ,\n",
       "       -1.1289892 , -1.4124571 , -1.6036965 , -1.702307  , -1.7723985 ,\n",
       "       -1.7224311 , -1.5088073 , -1.1923906 , -0.8366764 , -0.600947  ,\n",
       "       -0.3917556 , -0.2945591 , -0.40946317, -0.60994625, -0.78970015,\n",
       "       -0.9026362 , -0.96293205, -1.014787  , -0.8584938 , -0.6960107 ,\n",
       "       -0.46721888, -0.29995808, -0.07946605,  0.21504068,  0.5050157 ,\n",
       "        0.6277404 ,  0.51819664,  0.3411935 ,  0.1571199 , -0.01236591,\n",
       "       -0.15461549, -0.15956822, -0.08929047, -0.07055528, -0.16286613,\n",
       "       -0.23964879, -0.2790303 , -0.2256851 , -0.20165744, -0.09782308,\n",
       "       -0.08935171, -0.12207285, -0.2816823 , -0.28969327, -0.11949515,\n",
       "        0.15834051,  0.2929204 ,  0.24104798,  0.13719071,  0.15039702,\n",
       "        0.3177711 ,  0.5458561 ,  0.6584393 ,  0.67565155,  0.60720396,\n",
       "        0.56000954,  0.65479577,  0.8320646 ,  1.0681293 ,  1.3186493 ,\n",
       "        1.6239043 ,  1.9186337 ,  2.2061248 ,  2.48969   ,  2.7006493 ,\n",
       "        2.7815833 ,  2.6262853 ,  2.223589  ,  1.6830047 ,  1.0038587 ,\n",
       "        0.40352476, -0.09740558, -0.39688972, -0.54868346], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.3421562e-01, -2.4632950e-01, -1.7897381e-01,  7.7676371e-02,\n",
       "        1.1805372e-01,  7.6637939e-02,  1.2494746e-01,  1.4293577e-01,\n",
       "        7.9432011e-02,  1.6330704e-01,  2.4310900e-01,  9.5570646e-03,\n",
       "       -9.6604794e-02, -2.4112675e-01,  2.3673270e-02,  2.9296838e-03,\n",
       "       -3.1318478e-03,  7.5737089e-02,  8.5582614e-02,  3.4403542e-01,\n",
       "        3.7450570e-01,  2.3445462e-01,  1.8400829e-02,  1.9976622e-01,\n",
       "        3.5949492e-01,  1.4563480e-01, -1.3909495e-01, -2.9757306e-01,\n",
       "       -3.3816707e-01, -2.3372182e-01, -8.2806364e-02,  2.1430112e-01,\n",
       "        6.6312408e-01,  8.4901357e-01,  6.3849229e-01,  5.3906929e-01,\n",
       "        4.4473636e-01,  1.8007579e-01,  3.6751112e-01,  4.8600695e-01,\n",
       "        3.0203724e-01, -1.0829061e-01, -3.9648983e-01, -2.1320309e-01,\n",
       "       -2.1445838e-01, -2.8444424e-01, -3.8680834e-01, -5.2365291e-01,\n",
       "       -2.5605634e-01, -1.2606712e-01, -1.9090630e-01, -2.4754873e-01,\n",
       "       -2.3122174e-01, -4.5962575e-01, -5.2032799e-01, -5.2521116e-01,\n",
       "       -4.0580505e-01, -2.8183597e-01, -5.1671708e-01, -1.4049765e-01,\n",
       "       -2.2289513e-01, -3.2626984e-01, -2.6099238e-01, -1.8873368e-01,\n",
       "       -1.7763421e-01, -3.1173709e-01, -1.0142790e-01,  1.0162826e-01,\n",
       "        1.0545187e-01, -2.0099688e-01, -4.6102723e-01, -2.2383031e-01,\n",
       "       -4.0198618e-01, -6.5791297e-01, -4.7949368e-01, -4.2975736e-01,\n",
       "       -8.0180162e-01, -8.1224650e-01, -5.9087545e-01, -4.8219994e-01,\n",
       "       -4.0683174e-01, -3.7776521e-01, -2.9054534e-01, -1.6239962e-01,\n",
       "        1.8213507e-02, -5.1549386e-02,  1.7324392e-01, -2.0180669e-01,\n",
       "       -2.7423140e-01, -2.7369836e-01,  1.4012843e-01,  2.0419128e-01,\n",
       "        3.2525256e-01,  7.8693077e-02,  1.3726921e-01,  2.6939678e-01,\n",
       "        5.9893453e-01,  5.9827137e-01,  5.7784331e-01,  3.7109265e-01,\n",
       "        6.0827076e-01,  5.4920608e-01,  3.8789621e-01,  4.2718941e-01,\n",
       "        2.3708601e-01,  1.4333627e-01, -8.4808357e-03, -2.5977205e-02,\n",
       "       -5.2748900e-02, -2.9412144e-01, -4.2045344e-02, -1.2248348e-01,\n",
       "       -8.0138817e-04,  1.6657643e-01, -1.1769350e-01, -3.7906030e-01,\n",
       "       -2.0708095e-01,  2.7505034e-01,  1.5431528e-01,  2.0655441e-01,\n",
       "        4.3370631e-01,  2.6970938e-01,  8.6015955e-02,  2.1070705e-01,\n",
       "        7.3585972e-02, -3.2146420e-02, -3.1894469e-01, -5.0620610e-01,\n",
       "       -6.7057592e-01, -3.1660172e-01,  4.8343007e-02,  4.5741890e-02,\n",
       "       -1.3623063e-01, -7.5961612e-03, -5.4362321e-01, -6.4606708e-01,\n",
       "       -3.2589465e-01, -4.5504981e-01, -9.2588884e-01, -8.2886505e-01,\n",
       "       -6.0926670e-01, -1.0445893e+00, -8.8623470e-01, -9.0420610e-01,\n",
       "       -7.8568816e-01, -1.0547150e+00, -1.1715304e+00, -1.0676739e+00,\n",
       "       -4.4857329e-01, -9.3209481e-01, -7.4030560e-01, -9.0653354e-01,\n",
       "       -8.3166331e-01, -6.9296116e-01, -7.3619276e-01, -2.4511188e-01,\n",
       "        5.7392383e-01,  3.0476633e-01,  1.4984971e-01,  6.2896192e-01,\n",
       "        4.7221956e-01,  3.5499403e-01,  4.3693280e-01,  3.2986304e-01,\n",
       "        3.5549074e-01, -6.1393265e-02, -2.7408298e-02, -2.6674613e-01,\n",
       "       -3.4100178e-01,  1.6408412e-01,  3.8695887e-01,  2.9337171e-01,\n",
       "        4.5562629e-02, -2.8455641e-02, -3.1626073e-01, -5.0571156e-01,\n",
       "       -4.1387603e-01, -3.7176019e-01, -4.4790831e-01, -5.2205783e-01,\n",
       "       -5.4888093e-01, -6.2403488e-01, -5.5470812e-01, -7.0136315e-01,\n",
       "       -6.4600086e-01, -4.3976644e-01, -3.3127159e-01, -3.4402424e-01,\n",
       "       -5.9651607e-01, -4.8953801e-01, -5.4543942e-01, -5.4089981e-01,\n",
       "       -5.2357125e-01, -4.7653279e-01, -2.4222888e-01, -5.6183901e-02,\n",
       "       -1.1410202e-01,  8.4560104e-03,  6.8513840e-02, -1.9478257e-01,\n",
       "       -3.8707438e-01, -4.6398795e-01, -5.6922221e-01, -8.0218548e-01,\n",
       "       -5.2110839e-01, -5.6496859e-01, -4.7827175e-01, -1.3878225e-01,\n",
       "       -2.8405258e-01, -1.8014524e-02,  2.2109614e-01, -2.8385693e-01,\n",
       "       -4.0284431e-01, -7.8184128e-02, -2.3391223e-01, -3.2421649e-01,\n",
       "        6.6502854e-02, -5.8241087e-01, -6.4922667e-01, -3.2893896e-01,\n",
       "       -2.7673724e-01, -3.3184242e-01, -1.6771995e-01,  5.1428948e-02,\n",
       "        2.2736408e-01,  4.1964018e-01,  2.2564255e-01,  1.9117348e-01,\n",
       "        1.5906905e-01,  4.2236692e-01,  1.1030716e-01, -1.6138253e-01,\n",
       "       -5.2056175e-01, -5.3867382e-01, -5.4842955e-01, -6.1715430e-01,\n",
       "       -6.0822934e-01, -2.1656525e-01, -4.2585079e-02, -1.7918687e-01,\n",
       "       -2.9706621e-01, -3.6155012e-01, -2.7552190e-01, -4.0983689e-01,\n",
       "       -4.0940788e-01, -1.5804945e-01,  1.7585294e-01, -7.0216519e-01,\n",
       "       -4.0540671e-01, -4.1901848e-01,  3.9044324e-01,  3.2565323e-01,\n",
       "       -1.4867517e-01, -4.1886527e-02,  5.1538814e-02, -1.6847850e-01,\n",
       "       -2.8668535e-01, -3.5910538e-01, -5.8639348e-01, -6.4549804e-01,\n",
       "       -9.5648521e-01, -7.6707119e-01, -5.9265530e-01, -7.4363786e-01,\n",
       "       -6.9854867e-01, -3.5659206e-01, -8.5322881e-01, -7.2568780e-01,\n",
       "       -5.6919283e-01, -5.7936388e-01, -3.4504482e-01, -1.6130786e-01,\n",
       "       -2.1666880e-01, -1.6903085e-01, -1.6144175e-02, -2.1128255e-01,\n",
       "       -4.1110885e-01, -2.2216548e-01, -2.1326794e-01, -5.9222698e-01,\n",
       "       -4.5065206e-01, -1.7331998e-01, -1.7365953e-01, -7.4717361e-01,\n",
       "       -7.7300656e-01, -7.6496464e-01, -7.7231479e-01, -7.0275539e-01,\n",
       "       -9.3493038e-01, -1.2595707e+00, -1.1078171e+00, -7.1649939e-01,\n",
       "       -2.7311602e-01, -5.2809632e-01, -5.8378363e-01, -4.8516127e-01,\n",
       "       -3.7707692e-01, -4.3921328e-01, -2.8081012e-01, -3.4584820e-01,\n",
       "       -4.9661058e-01, -3.9501083e-01, -6.8006307e-01, -8.6957556e-01,\n",
       "       -8.8087368e-01, -9.1907495e-01, -5.8502465e-01, -7.5762963e-01,\n",
       "       -6.4192891e-01, -1.9537748e-01,  5.7556394e-02, -4.1584920e-02,\n",
       "        2.9984217e-02, -9.1669887e-02,  4.7378991e-02, -1.4608851e-01,\n",
       "       -1.4539398e-03, -5.7196128e-01, -8.6400378e-01, -1.1552508e+00,\n",
       "       -1.3469578e+00, -1.2080331e+00, -1.1323770e+00, -1.0725472e+00,\n",
       "       -8.7021315e-01, -7.0696640e-01, -7.9961717e-01, -7.6960206e-01,\n",
       "       -8.2038242e-01, -9.0517592e-01, -6.9801402e-01, -7.0372993e-01,\n",
       "       -7.9557914e-01, -1.0627414e+00, -1.0414087e+00, -8.4300172e-01,\n",
       "       -6.9724053e-01, -7.7587724e-01, -6.6550535e-01, -7.3795724e-01,\n",
       "       -6.0761827e-01, -5.6866318e-01, -5.8435184e-01, -7.6437491e-01,\n",
       "       -6.6885841e-01, -5.8992821e-01, -6.6525835e-01, -7.7477449e-01,\n",
       "       -9.2466044e-01, -9.6457827e-01, -7.9435974e-01, -5.2442425e-01,\n",
       "       -5.6923211e-01, -5.9705967e-01, -6.6875923e-01, -3.9435783e-01,\n",
       "       -4.5532197e-01, -4.8864529e-01, -4.6293300e-01, -5.7609493e-01,\n",
       "       -5.9256572e-01, -7.0674300e-01, -6.5788245e-01, -5.8518916e-01,\n",
       "       -5.8923936e-01, -5.6131053e-01, -7.9734534e-01, -7.1001816e-01,\n",
       "       -6.8388867e-01, -6.0482323e-01, -3.1780005e-01, -3.8047460e-01,\n",
       "       -4.0603366e-01, -7.3780343e-02,  1.1176579e-01,  7.1870655e-02,\n",
       "       -9.1580436e-02,  1.4195692e-02,  1.4529994e-01,  1.6807149e-01,\n",
       "        7.6160908e-02, -1.1327909e-01, -1.2209670e-01, -2.5792477e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.reshape((-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# soda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "================================================================================2022-04-01 17:48:55\n",
      "\n",
      "EPOCH = 1, loss = 0.218, val_loss = 0.207, \n",
      "\n",
      "================================================================================2022-04-01 17:48:56\n",
      "\n",
      "EPOCH = 2, loss = 0.120, val_loss = 0.090, \n",
      "\n",
      "================================================================================2022-04-01 17:48:56\n",
      "\n",
      "EPOCH = 3, loss = 0.057, val_loss = 0.070, \n",
      "\n",
      "================================================================================2022-04-01 17:48:56\n",
      "\n",
      "EPOCH = 4, loss = 0.034, val_loss = 0.056, \n",
      "\n",
      "================================================================================2022-04-01 17:48:57\n",
      "\n",
      "EPOCH = 5, loss = 0.020, val_loss = 0.037, \n",
      "\n",
      "================================================================================2022-04-01 17:48:57\n",
      "Finished Training...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import datetime\n",
    "dfhistory = pd.DataFrame(columns = [\"epoch\",\"loss\",\"val_loss\"]) \n",
    "print(\"Start Training...\")\n",
    "nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"==========\"*8 + \"%s\"%nowtime)\n",
    "bestloss = float(\"inf\")\n",
    "epoch = 5\n",
    "model_epoch = 0\n",
    "for epoch in range(1,epoch+1):\n",
    "    #train----------------------------------------------------\n",
    "  model.train()\n",
    "  loss_sum = 0.0\n",
    "  step = 1\n",
    "  for step,(enc_inputs, dec_outputs) in enumerate(loader_train_sado, 1):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 正向\n",
    "      enc_inputs, dec_outputs = enc_inputs.to(device), dec_outputs.to(device)\n",
    "      y_pred = model(enc_inputs)\n",
    "      \n",
    "      loss = criterion(y_pred, dec_outputs) \n",
    "\n",
    "      # 反向\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # print\n",
    "      loss_sum += loss.item()\n",
    "      #if step%40 == 0:\n",
    "        #print((\"[step = %d] loss: %.3f, \") % (step, loss_sum/step))\n",
    "\n",
    "\n",
    "  #valid\n",
    "  model.eval()\n",
    "  val_loss_sum = 0.0\n",
    "  val_step = 1\n",
    "\n",
    "  for val_step, (enc_inputs, dec_outputs) in enumerate(loader_valid_soda, 1):\n",
    "    with torch.no_grad():# 节点不进行求梯度\n",
    "      enc_inputs, dec_outputs = enc_inputs.to(device), dec_outputs.to(device)\n",
    "      outputs = model(enc_inputs)\n",
    "      #val_loss = criterion(outputs.view(-1), dec_outputs.view(-1)) \n",
    "      val_loss =criterion(outputs, dec_outputs) \n",
    "\n",
    "    val_loss_sum += val_loss.item()\n",
    "\n",
    "\n",
    "  # log\n",
    "  info = (epoch, loss_sum/step, val_loss_sum/val_step)\n",
    "  dfhistory.loc[epoch-1] = info # epoch从1开始 index -1\n",
    "\n",
    "  # print\n",
    "  print((\"\\nEPOCH = %d, loss = %.3f,\"+\" val_loss = %.3f, \")% info)\n",
    "  nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "  print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "\n",
    "  if val_loss_sum <= bestloss: #换成小于等于比小于效果更好\n",
    "        bestloss = val_loss_sum\n",
    "        torch.save(model,'bestmodel_soda.pkl')\n",
    "        model_epoch = epoch\n",
    "        #print(\"bestmodel updata\")\n",
    "        \n",
    "\n",
    "print('Finished Training...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 16)\n",
      "-0.011562084762578915\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "# batch_size\n",
    "for step,(inputs, _) in enumerate(loader_test, 1):\n",
    "    pred = model(inputs.to(device))\n",
    "    out.append(pred.squeeze().cpu().detach().numpy())\n",
    "\n",
    "out = np.array(out)    \n",
    "print(out.shape)\n",
    "\n",
    "#——————————————cor————————————————\n",
    "cor = np.zeros((1))\n",
    "cor = np.corrcoef(testY[:,0],out.reshape((-1)))[0,1]\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 16)\n",
      "0.7452393533846223\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "# batch_size\n",
    "model=torch.load('bestmodel_soda.pkl')\n",
    "for step,(inputs, _) in enumerate(loader_test, 1):\n",
    "    pred = model(inputs.to(device))\n",
    "    out.append(pred.squeeze().cpu().detach().numpy())\n",
    "\n",
    "out = np.array(out)    \n",
    "print(out.shape)\n",
    "\n",
    "#——————————————cor————————————————\n",
    "cor = np.zeros((1))\n",
    "cor = np.corrcoef(testY[:,0],out.reshape((-1)))[0,1]\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 16)\n",
      "0.5400710080447104\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "# batch_size\n",
    "model=torch.load('bestmodel.pkl')\n",
    "for step,(inputs, _) in enumerate(loader_test, 1):\n",
    "    pred = model(inputs.to(device))\n",
    "    out.append(pred.squeeze().cpu().detach().numpy())\n",
    "\n",
    "out = np.array(out)    \n",
    "print(out.shape)\n",
    "\n",
    "#——————————————cor————————————————\n",
    "cor = np.zeros((1))\n",
    "cor = np.corrcoef(testY[:,0],out.reshape((-1)))[0,1]\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(model_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7828678 , -0.64715624, -0.68216705, -0.86499506, -0.8866562 ,\n",
       "       -0.8267274 , -0.6234623 , -0.70335156, -0.9443807 , -1.1383914 ,\n",
       "       -1.158753  , -1.1583354 , -1.180532  , -1.1754164 , -1.0011051 ,\n",
       "       -0.80188906], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY[:16,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3945299 ,  0.21358556, -0.55354697, -0.40999395, -0.8203296 ,\n",
       "       -0.46780396, -0.0918841 , -0.05698049, -0.14935665,  0.16960537,\n",
       "        0.2032471 ,  0.13740295,  0.19149709,  0.08952218,  0.1406549 ,\n",
       "        0.03880312], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = model(torch.from_numpy(testX[:10].astype(np.float32)).to(device))\n",
    "pre.cpu().detach().numpy()\n",
    "pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[0,1,2], [3,4,5]])\n",
    "a.resize(3, 3)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0,1,2], [3,4,5]])\n",
    "a.reshape((-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('enso-q')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "9bba8c7d9c8496bd6992670a29c4e9a9a45ed2b8a8c8d7c9266649db153117bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
