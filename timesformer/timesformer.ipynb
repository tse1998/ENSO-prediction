{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_index_soda:940\n",
      "split_index:2000\n",
      "(2500, 12, 1, 72, 72)\n",
      "(1176, 12, 1, 72, 72)\n",
      "(384, 1)\n",
      "384\n",
      "(1176, 12, 1, 72, 72)\n",
      "----------------------------Train Data----------------------------\n",
      "(2000, 12, 1, 72, 72)\n",
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import random #shuffle\n",
    "from netCDF4 import Dataset\n",
    "def CMIPdata(Xdata, Ydata, out , myform):\n",
    "    inp1 = Dataset(Xdata,'r')\n",
    "    inp2 = Dataset(Ydata,'r')\n",
    "    sst_1 = np.zeros((myform,140,12,24,72))\n",
    "\n",
    "    #首年序列\n",
    "    #i= 21个模式\n",
    "    for i in range(myform):\n",
    "        sst_1[i,:,:,:,:] = inp1.variables['sst1'][1+141*i:141+141*i,0:12,:,:]\n",
    "    #(21,140,12,24,72)\n",
    "    #首年序列\n",
    "\n",
    "    \n",
    "    #flatted扁平化                                             \n",
    "    sst_2 = np.zeros((myform,1680,24,72))\n",
    "    for i in range(myform):\n",
    "        for j in range(140):\n",
    "            sst_2[i,j*12:(j+1)*12,:,:] = sst_1[i,j,:,:,:]\n",
    "    \n",
    "        #(21,1680,24,72)\n",
    "        #每个模式 139*12+1 = 1680 个序列 序列长度为12\n",
    "\n",
    "    #丢弃1个月 1862.1\n",
    "    sst_2 = sst_2[:,1:,:,:]\n",
    "    #(form,1679,24,72)\n",
    "\n",
    "\n",
    "    winsize = 12\n",
    "    #winnum = 1679-winsize +1 #1668\n",
    "    \"\"\"跳跃六个月滑窗 = 6\"\"\"\n",
    "    ts=6 \n",
    "    #(1679-12)/6+1 =27\n",
    "    winnum=int((1679-12)/ts +1) \n",
    "    \n",
    "    sst_3 = np.zeros((myform,winnum,12,24,72))\n",
    "    for i in range(myform):\n",
    "        for j in range(winnum):\n",
    "            #print(j)\n",
    "            sst_3[i,j,:,:,:] = sst_2[i,(j*ts):(j*ts)+winsize,:,:]\n",
    "            \n",
    "\n",
    "    sst_4 = np.zeros((int(winnum*myform),12,24,72))\n",
    "    #t300_4 = np.zeros((int(winnum*myform),12,24,72))\n",
    "    for i in range(myform):\n",
    "        sst_4[i*winnum:(i+1)*winnum,:,:,:] = sst_3[i,:,:,:,:]\n",
    "\n",
    "\n",
    "    #sst_4 = sst_4.swapaxes(1, 3)\n",
    "    trX = sst_4\n",
    "    del sst_1,sst_2,sst_3,sst_4\n",
    "    #trX = trX.reshape(int(winnum*myform),12,1728)\n",
    "\n",
    "    #保存np数组\n",
    "    #(5859, 12, 24, 72, 2)\n",
    "    #np.save(\"./CMIPdata/CMIP_trX_21_ts6_out.npy\",trX) \n",
    "\n",
    "    #label\n",
    "    #平铺\n",
    "    pr_1 = np.zeros(((myform),1692))#21,1692\n",
    "    for i in range(myform):\n",
    "        for j in range(141):\n",
    "            pr_1[i,j*12:(j+1)*12] = inp2.variables['pr'][(141*i)+j,:,0,0]\n",
    "            \n",
    "    #out\n",
    "    pr_2 =  np.zeros((myform,winnum,out))#form,1668,1\n",
    "    for i in range(myform):\n",
    "        for j in range(winnum):\n",
    "            pr_2[i,j,:] = pr_1[i,(j*ts):(j*ts)+out]\n",
    "\n",
    "    trY = np.zeros((int(winnum*myform),out))#form*1668,1\n",
    "    for i in range(myform):\n",
    "        trY[i*winnum:(i+1)*winnum,:] = pr_2[i,:,:]\n",
    "\n",
    "    #np.save(\"./CMIPdata/CMIP_trY_21_ts6_out%s.npy\"%out,trY)\n",
    "    trY_decoder_input = np.zeros((int(winnum*myform),out))\n",
    "    trY_decoder_input[:,1:] = trY[:,:-1]\n",
    "\n",
    "    #return trX[1:,:,:] ,trY.reshape(-1,24,1)[1:,:,:] ,trY_decoder_input\n",
    "    return trX ,trY.reshape(-1,24,1) ,trY_decoder_input.reshape(-1,24,1)\n",
    "\n",
    "def GOSDAdata(Xdata, Ydata, out):\n",
    "    #test data\n",
    "    inp11 = Dataset(Xdata,'r')\n",
    "    inp22 = Dataset(Ydata,'r')\n",
    "                                              \n",
    "    sst_11 = np.zeros((33,12,24,72)) #1983-2015\n",
    "    #t300_11 = np.zeros((33,12,24,72))\n",
    "\n",
    "    sst_11[:,:,:,:] = inp11.variables['sst'][3:,0:12,:,:]\n",
    "    #t300_11[:,:,:,:] = inp11.variables['t300'][3:,0:12,:,:]\n",
    "    #(33,12,24,72)\n",
    "\n",
    "    sst_22 = np.zeros((396,24,72))\n",
    "    #t300_22 = np.zeros((396,24,72))\n",
    "\n",
    "    for i in range(33):\n",
    "        sst_22[i*12:(i+1)*12,:,:] = sst_11[i,:,:,:]\n",
    "        #t300_22[i*12:(i+1)*12,:,:] = t300_11[i,:,:,:]\n",
    "    #(396,24,72)\n",
    "    #丢弃一个月，便于制作滑窗序列\n",
    "    sst_22 = sst_22[1:,:,:]\n",
    "    #t300_22 = t300_22[1:,:,:]#(395,24,72)\n",
    "\n",
    "    #滑窗\n",
    "    winsize = 12\n",
    "    winnum = 395-winsize + 1 #384\n",
    "    sst_33 = np.zeros((winnum, winsize, 24, 72))#(384, 12, 24, 72)\n",
    "    #t300_33 = np.zeros((winnum, winsize, 24, 72))\n",
    "\n",
    "    for i in range(winnum):\n",
    "        sst_33[i,:,:,:] = sst_22[i:i+12,:,:]\n",
    "        #t300_33[i,:,:,:] = t300_22[i:i+12,:,:]\n",
    "    #(384,12,24,72)\n",
    "\n",
    "\n",
    "    #channel = 2\n",
    "    #testX = np.zeros((winnum,12,24,72,2))#384\n",
    "    #testX[:,:,:,:,0] = sst_33\n",
    "    #testX[:,:,:,:,1] = t300_33\n",
    "    #sst_33 = sst_33.swapaxes(1, 3)\n",
    "\n",
    "    testX = sst_33#384\n",
    "    #testX[:,:,:,1] = t300_33.reshape(384,12,1728)\n",
    "    del sst_11,sst_22,sst_33\n",
    "    #del t300_11,t300_22,t300_33\n",
    "    #testX = testX.reshape(384,12,3456)\n",
    "\n",
    "    #label\n",
    "    inpv22 = np.zeros((408))#2017-1984 +1=34\n",
    "    for i in range(34):\n",
    "        inpv22[i*12:(i+1)*12] = inp22.variables['pr'][i+2,:,0,0]# +2:从1984开始\n",
    "        #(408)\n",
    "\n",
    "    testY = np.zeros((winnum,out,1))\n",
    "    #out = 1\n",
    "    #滑窗\n",
    "    for i in range(winnum):\n",
    "        testY[i,:,0] = inpv22[i:i+out]#24\n",
    "    #(384,out,1)\n",
    "\n",
    "    return testX, testY\n",
    "\n",
    "def SODAdata(Xdata, Ydata, out):\n",
    "    inp1 = Dataset(Xdata,'r')\n",
    "    inp2 = Dataset(Ydata,'r')\n",
    "\n",
    "    #time_step = 1 month                                               \n",
    "    sst_1 = np.zeros((99,12,24,72))\n",
    "    #t300_1 = np.zeros((99,12,24,72))\n",
    "\n",
    "    sst_1[:,:,:,:] = inp1.variables['sst'][1:,0:12,:,:]\n",
    "    #sst_1[:,:,:,:] = inp1.variables['t300'][1:,0:12,:,:]\n",
    "    #(99,12,24,72)\n",
    "    #1872-1970\n",
    "\n",
    "    sst_2 = np.zeros((1188,24,72))\n",
    "    #t300_2 = np.zeros((1188,24,72))\n",
    "\n",
    "    for i in range(99):\n",
    "        sst_2[i*12:(i+1)*12,:,:] = sst_1[i,:,:,:]\n",
    "        #t300_2[i*12:(i+1)*12,:,:] = t300_1[i,:,:,:]\n",
    "    #(1188,24,72)\n",
    "    #丢弃一个月，便于制作滑窗序列\n",
    "    sst_2 = sst_2[1:,:,:]\n",
    "    #t300_2 = t300_2[1:,:,:]#(1187,24,72)\n",
    "\n",
    "    sst_3 = np.zeros((1176,12,24,72))\n",
    "    #t300_3 = np.zeros((1176,12,24,72))\n",
    "    #滑窗\n",
    "    for i in range(1176):\n",
    "        sst_3[i:,:,:] = sst_2[i:i+12,:,:]\n",
    "        #t300_3[i,:,:,:] = sst_2[i:i+12,:,:]\n",
    "    #(1176,12,24,72)\n",
    "    trX = sst_3\n",
    "\n",
    "    #channel = 2\n",
    "    #trX = np.zeros((1176,12,24,72,2))\n",
    "    #trX[:,:,:,:,0] = sst_3\n",
    "    #trX[:,:,:,:,1] = t300_3\n",
    "\n",
    "    #保存np数组\n",
    "    #np.save(\"./SODAdata/SODA_trX_ts1_out.npy\",trX)\n",
    "\n",
    "    #label\n",
    "    inpv2 = np.zeros((1200))\n",
    "    for i in range(100):\n",
    "        inpv2[i*12:(i+1)*12] = inp2.variables['pr'][i,:,0,0]\n",
    "    #(1200)\n",
    "\n",
    "    #out = 1\n",
    "    trY = np.zeros((1176,out,1))\n",
    "    #滑窗\n",
    "    for i in range(1176):\n",
    "        trY[i,:,0] = inpv2[i:i+out]#24\n",
    "    #(1176,24)\n",
    "    \n",
    "    \n",
    "    trY_decoder_input = np.zeros((1176,out,1))\n",
    "    trY_decoder_input[:,1:,:] = trY[:,:-1,:]\n",
    "\n",
    "    #np.save(\"./SODAdata/SODA_trY_ts1_out%s.npy\"%out,trY) \n",
    "    return trX , trY, trY_decoder_input\n",
    "\n",
    "trX, trY, trY_decoder_input = CMIPdata('/home/d/Q/liyanqiu/saconvlstm/CMIP5.input.36mon.1861_2001.nc',\\\n",
    "                                          '/home/d/Q/liyanqiu/saconvlstm/CMIP5.label.12mon.1863_2003.nc', 24,  21)\n",
    "\n",
    "testX, testY = GOSDAdata('/home/d/Q/liyanqiu/saconvlstm/GODAS.input.36mon.1980_2015.nc',\\\n",
    "                         '/home/d/Q/liyanqiu/saconvlstm/GODAS.label.12mon.1982_2017.nc', 24)\n",
    "\n",
    "trX_SODA, trY_SODA ,trY_decoder_input_SODA = SODAdata('/home/d/Q/liyanqiu/saconvlstm/SODA.input.36mon.1871_1970.nc',\n",
    "                              '/home/d/Q/liyanqiu/saconvlstm/SODA.label.12mon.1873_1972.nc',24)\n",
    "\n",
    "# resize 72*72\n",
    "trX.resize((trX.shape[0], trX.shape[1], 72, 72))\n",
    "trX_SODA.resize((trX_SODA.shape[0], trX_SODA.shape[1], 72, 72))\n",
    "testX.resize((testX.shape[0], testX.shape[1], 72, 72))\n",
    "\n",
    "indices = np.arange(trX.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "trX = trX[indices]\n",
    "trY = trY[indices]\n",
    "trY_decoder_input = trY_decoder_input[indices]\n",
    "\n",
    "\n",
    "# shuffle\n",
    "indices_soda = np.arange(trX_SODA.shape[0])\n",
    "np.random.shuffle(indices_soda)\n",
    "trX_SODA = trX_SODA[indices_soda]\n",
    "trY_SODA = trY_SODA[indices_soda]\n",
    "trY_decoder_input_SODA = trY_decoder_input_SODA[indices_soda]\n",
    "\n",
    "\n",
    "class MyDataSet(Data.Dataset):\n",
    "    \"\"\"自定义DataLoader\"\"\"\n",
    "    def __init__(self, enc_inputs,  dec_outputs):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.enc_inputs = enc_inputs\n",
    "        #self.dec_inputs = dec_inputs\n",
    "        self.dec_outputs = dec_outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.enc_inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx],  self.dec_outputs[idx]\n",
    "    \n",
    "batch_size = 16\n",
    "trainsize = 2500 #选择cmip训练数量\n",
    "trX = trX[:trainsize]\n",
    "trY = trY[:trainsize]\n",
    "\n",
    "\n",
    "\n",
    "# expand channel\n",
    "trX = np.expand_dims(trX, axis=2)\n",
    "trX_SODA = np.expand_dims(trX_SODA, axis=2)\n",
    "testX = np.expand_dims(testX, axis=2)\n",
    "\n",
    "#train split\n",
    "split_index = int(trX.shape[0]*0.8)\n",
    "split_index_soda = int(trX_SODA.shape[0]*0.8)\n",
    "\n",
    "print(\"split_index_soda:\" + str(split_index_soda))\n",
    "print(\"split_index:\" + str(split_index))\n",
    "\n",
    "\n",
    "target_month = 1\n",
    "loader_train = Data.DataLoader(\n",
    "    MyDataSet(\n",
    "        trX[:split_index].astype(np.float32), \n",
    "        trY[:split_index, target_month-1:target_month].astype(np.float32).reshape((-1,1))), \n",
    "    batch_size, shuffle = True)\n",
    "loader_valid = Data.DataLoader(\n",
    "    MyDataSet(\n",
    "        trX[split_index:].astype(np.float32), \n",
    "        trY[split_index:, target_month-1:target_month].astype(np.float32).reshape((-1,1))), \n",
    "    batch_size, shuffle=False)\n",
    "\n",
    "loader_train_sado = Data.DataLoader(\n",
    "    MyDataSet(\n",
    "        trX_SODA[:split_index_soda].astype(np.float32), \n",
    "        trY_SODA[:split_index_soda, target_month-1:target_month].astype(np.float32).reshape((-1,1))), \n",
    "    batch_size, shuffle = True)\n",
    "loader_valid_soda = Data.DataLoader(\n",
    "    MyDataSet(\n",
    "        trX_SODA[split_index_soda:].astype(np.float32), \n",
    "        trY_SODA[split_index_soda:, target_month-1:target_month].astype(np.float32).reshape((-1,1))), \n",
    "    batch_size, shuffle=False)\n",
    "\n",
    "#testdataloader\n",
    "\n",
    "loader_test = Data.DataLoader(\n",
    "    MyDataSet(\n",
    "        testX.astype(np.float32), \n",
    "        testY[:, target_month-1:target_month].astype(np.float32).reshape((-1,1))), \n",
    "    batch_size, shuffle = False)\n",
    "\n",
    "testY = testY[:, target_month-1:target_month].astype(np.float32).reshape((-1,1))\n",
    "\n",
    "print(trX.shape)\n",
    "print(trX_SODA .shape)\n",
    "print(testY.shape)\n",
    "print(len(testX))\n",
    "print(trX_SODA.shape)\n",
    "print(\"----------------------------Train Data----------------------------\")\n",
    "print(trX[:split_index].astype(np.float32).shape)\n",
    "print(trY[:split_index, target_month-1:target_month].reshape((-1,1)).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if on cuda: True\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, *args, **kwargs)\n",
    "    \n",
    "#feed forward\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1) # 在给定的维度上将张量进行分块。沿着-1分成两块\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)#geglu大小除以2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "#attention\n",
    "def attn(q, k, v):\n",
    "    sim = einsum('b i d, b j d -> b i j', q, k)\n",
    "    attn = sim.softmax(dim = -1)\n",
    "    out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "    return out\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, einops_from, einops_to, **einops_dims):\n",
    "        h = self.heads\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "\n",
    "        q *= self.scale\n",
    "\n",
    "        # splice out classification token at index 1\n",
    "        (cls_q, q_), (cls_k, k_), (cls_v, v_) = map(lambda t: (t[:, 0:1], t[:, 1:]), (q, k, v))\n",
    "\n",
    "        # let classification token attend to key / values of all patches across time and space\n",
    "        cls_out = attn(cls_q, k, v)\n",
    "\n",
    "        # rearrange across time or space\n",
    "        q_, k_, v_ = map(lambda t: rearrange(t, f'{einops_from} -> {einops_to}', **einops_dims), (q_, k_, v_))\n",
    "\n",
    "        # expand cls token keys and values across time or space and concat\n",
    "        r = q_.shape[0] // cls_k.shape[0]\n",
    "        cls_k, cls_v = map(lambda t: repeat(t, 'b () d -> (b r) () d', r = r), (cls_k, cls_v))\n",
    "\n",
    "        k_ = torch.cat((cls_k, k_), dim = 1)\n",
    "        v_ = torch.cat((cls_v, v_), dim = 1)\n",
    "\n",
    "        # attention\n",
    "        out = attn(q_, k_, v_)\n",
    "\n",
    "        # merge back time or space 合并时间或空间\n",
    "        out = rearrange(out, f'{einops_to} -> {einops_from}', **einops_dims)\n",
    "\n",
    "        # concat back the cls token 连接回来的cls token\n",
    "        out = torch.cat((cls_out, out), dim = 1)\n",
    "\n",
    "        # merge back the heads\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "\n",
    "        # combine heads out 合并head输出\n",
    "        return self.to_out(out) # batch * dim\n",
    "    \n",
    "    \n",
    "# main classes\n",
    "class TimeSformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        num_frames,\n",
    "        num_classes,\n",
    "        image_size = 72,\n",
    "        patch_size = 12,\n",
    "        channels = 1,\n",
    "        depth = 12,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        #符合则往下运行\n",
    "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_size // patch_size) ** 2 #平方个数 //向下取整\n",
    "        num_positions = num_frames * num_patches # 12*num_patches\n",
    "        patch_dim = channels * patch_size ** 2 # patch大小\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.to_patch_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.pos_emb = nn.Embedding(num_positions + 1, dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, dim))# [1, dim]\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)), # Time attention\n",
    "                PreNorm(dim, Attention(dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)), # Spatial attention\n",
    "                PreNorm(dim, FeedForward(dim, dropout = ff_dropout)) # Feed Forward\n",
    "            ]))\n",
    "\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "    \n",
    "        #self.to_out = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, video):\n",
    "        b, f, _, h, w, *_, device, p = *video.shape, video.device, self.patch_size\n",
    "        assert h % p == 0 and w % p == 0, f'height {h} and width {w} of video must be divisible by the patch size {p}'\n",
    "\n",
    "        n = (h // p) * (w // p)\n",
    "        video = rearrange(video, 'b f c (h p1) (w p2) -> b (f h w) (p1 p2 c)', p1 = p, p2 = p)\n",
    "        \n",
    "        tokens = self.to_patch_embedding(video)\n",
    "\n",
    "        cls_token = repeat(self.cls_token, 'n d -> b n d', b = b)# [batch, 1, dim]\n",
    "        x =  torch.cat((cls_token, tokens), dim = 1)\n",
    "        x += self.pos_emb(torch.arange(x.shape[1], device = device))\n",
    "\n",
    "        for (time_attn, spatial_attn, ff) in self.layers:\n",
    "            x = time_attn(x, 'b (f n) d', '(b n) f d', n = n) + x\n",
    "            x = spatial_attn(x, 'b (f n) d', '(b f) n d', f = f) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        cls_token = x[:, 0]\n",
    "        \n",
    "        return self.to_out(cls_token)\n",
    "    \n",
    "    \n",
    "#Parameters\n",
    "device = 'cuda'\n",
    "DIM = 128\n",
    "IMAGE_SIZE = 72\n",
    "PATCH_SIZE = 12 #12 24 8\n",
    "NUM_CLASSES = 1\n",
    "NUM_FRAMES = 12\n",
    "DEPTH = 1 # block个数\n",
    "HEADS = 8\n",
    "DIM_HEAD = 64\n",
    "ATTN_DROPOUT = 0.1\n",
    "FF_DROPOUT = 0.1\n",
    "#ITERATIONS = 20# epoch\n",
    "\n",
    "\n",
    "# (batch x frames x channels x height x width)\n",
    "model = TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, \\\n",
    "        num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, \\\n",
    "            ff_dropout = FF_DROPOUT).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "print(\"if on cuda:\",next(model.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "================================================================================2022-03-10 20:46:50\n",
      "\n",
      "EPOCH = 1, loss = 0.693, val_loss = 0.624, \n",
      "\n",
      "================================================================================2022-03-10 20:46:54\n",
      "\n",
      "EPOCH = 2, loss = 0.678, val_loss = 0.630, \n",
      "\n",
      "================================================================================2022-03-10 20:46:57\n",
      "\n",
      "EPOCH = 3, loss = 0.667, val_loss = 0.672, \n",
      "\n",
      "================================================================================2022-03-10 20:46:59\n",
      "\n",
      "EPOCH = 4, loss = 0.671, val_loss = 0.622, \n",
      "\n",
      "================================================================================2022-03-10 20:47:01\n",
      "\n",
      "EPOCH = 5, loss = 0.674, val_loss = 0.650, \n",
      "\n",
      "================================================================================2022-03-10 20:47:03\n",
      "\n",
      "EPOCH = 6, loss = 0.665, val_loss = 0.628, \n",
      "\n",
      "================================================================================2022-03-10 20:47:06\n",
      "\n",
      "EPOCH = 7, loss = 0.669, val_loss = 0.636, \n",
      "\n",
      "================================================================================2022-03-10 20:47:08\n",
      "\n",
      "EPOCH = 8, loss = 0.669, val_loss = 0.624, \n",
      "\n",
      "================================================================================2022-03-10 20:47:10\n",
      "\n",
      "EPOCH = 9, loss = 0.664, val_loss = 0.620, \n",
      "\n",
      "================================================================================2022-03-10 20:47:12\n",
      "\n",
      "EPOCH = 10, loss = 0.665, val_loss = 0.616, \n",
      "\n",
      "================================================================================2022-03-10 20:47:15\n",
      "\n",
      "EPOCH = 11, loss = 0.665, val_loss = 0.621, \n",
      "\n",
      "================================================================================2022-03-10 20:47:17\n",
      "\n",
      "EPOCH = 12, loss = 0.659, val_loss = 0.650, \n",
      "\n",
      "================================================================================2022-03-10 20:47:19\n",
      "\n",
      "EPOCH = 13, loss = 0.665, val_loss = 0.651, \n",
      "\n",
      "================================================================================2022-03-10 20:47:21\n",
      "\n",
      "EPOCH = 14, loss = 0.669, val_loss = 0.638, \n",
      "\n",
      "================================================================================2022-03-10 20:47:23\n",
      "\n",
      "EPOCH = 15, loss = 0.663, val_loss = 0.633, \n",
      "\n",
      "================================================================================2022-03-10 20:47:26\n",
      "\n",
      "EPOCH = 16, loss = 0.656, val_loss = 0.715, \n",
      "\n",
      "================================================================================2022-03-10 20:47:28\n",
      "\n",
      "EPOCH = 17, loss = 0.660, val_loss = 0.630, \n",
      "\n",
      "================================================================================2022-03-10 20:47:30\n",
      "\n",
      "EPOCH = 18, loss = 0.655, val_loss = 0.625, \n",
      "\n",
      "================================================================================2022-03-10 20:47:33\n",
      "\n",
      "EPOCH = 19, loss = 0.660, val_loss = 0.632, \n",
      "\n",
      "================================================================================2022-03-10 20:47:35\n",
      "\n",
      "EPOCH = 20, loss = 0.650, val_loss = 0.648, \n",
      "\n",
      "================================================================================2022-03-10 20:47:38\n",
      "Finished Training...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import datetime\n",
    "dfhistory = pd.DataFrame(columns = [\"epoch\",\"loss\",\"val_loss\"]) \n",
    "print(\"Start Training...\")\n",
    "nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"==========\"*8 + \"%s\"%nowtime)\n",
    "bestloss = float(\"inf\")\n",
    "epoch = 20\n",
    "for epoch in range(1,epoch+1):\n",
    "    #train----------------------------------------------------\n",
    "  model.train()\n",
    "  loss_sum = 0.0\n",
    "  step = 1\n",
    "  for step,(enc_inputs, dec_outputs) in enumerate(loader_train, 1):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 正向\n",
    "      enc_inputs, dec_outputs = enc_inputs.to(device), dec_outputs.to(device)\n",
    "      y_pred = model(enc_inputs)\n",
    "      \n",
    "      loss = criterion(y_pred, dec_outputs) \n",
    "\n",
    "      # 反向\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # print\n",
    "      loss_sum += loss.item()\n",
    "      #if step%40 == 0:\n",
    "        #print((\"[step = %d] loss: %.3f, \") % (step, loss_sum/step))\n",
    "\n",
    "\n",
    "  #valid\n",
    "  model.eval()\n",
    "  val_loss_sum = 0.0\n",
    "  val_step = 1\n",
    "\n",
    "  for val_step, (enc_inputs, dec_outputs) in enumerate(loader_valid, 1):\n",
    "    with torch.no_grad():# 节点不进行求梯度\n",
    "      enc_inputs, dec_outputs = enc_inputs.to(device), dec_outputs.to(device)\n",
    "      outputs = model(enc_inputs)\n",
    "      #val_loss = criterion(outputs.view(-1), dec_outputs.view(-1)) \n",
    "      val_loss =criterion(outputs, dec_outputs) \n",
    "\n",
    "    val_loss_sum += val_loss.item()\n",
    "\n",
    "\n",
    "  # log\n",
    "  info = (epoch, loss_sum/step, val_loss_sum/val_step)\n",
    "  dfhistory.loc[epoch-1] = info # epoch从1开始 index -1\n",
    "\n",
    "  # print\n",
    "  print((\"\\nEPOCH = %d, loss = %.3f,\"+\" val_loss = %.3f, \")% info)\n",
    "  nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "  print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "  \n",
    "  if val_loss_sum <= bestloss: #换成小于等于比小于效果更好\n",
    "        bestloss = val_loss_sum\n",
    "        bestmodel = model\n",
    "        \n",
    "\n",
    "print('Finished Training...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04751934806165872\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "# batch_size\n",
    "for step,(inputs, _) in enumerate(loader_test, 1):\n",
    "    pred = model(inputs.to(device))\n",
    "    out.append(pred.squeeze().cpu().detach().numpy())\n",
    "\n",
    "out = np.array(out)    \n",
    "out.shape\n",
    "\n",
    "#——————————————cor————————————————\n",
    "cor = np.zeros((1))\n",
    "cor = np.corrcoef(testY[:,0],out.reshape((-1)))[0,1]\n",
    "print(cor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = model(torch.from_numpy(testX[:10].astype(np.float32)).to(device))\n",
    "pre.cpu().detach().numpy()\n",
    "pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[0,1,2], [3,4,5]])\n",
    "a.resize(3, 3)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0,1,2], [3,4,5]])\n",
    "a.reshape((-1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9bba8c7d9c8496bd6992670a29c4e9a9a45ed2b8a8c8d7c9266649db153117bd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
