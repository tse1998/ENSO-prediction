{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_index_soda:940\n",
      "split_index:2000\n",
      "(2500, 12, 24, 72)\n",
      "(2500, 24, 1)\n",
      "(2500, 24, 1)\n",
      "(384, 12, 24, 72)\n",
      "384\n",
      "(1176, 12, 24, 72)\n",
      "(1176, 24, 1)\n",
      "(1176, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import random #shuffle\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "def CMIPdata(Xdata, Ydata, out , myform):\n",
    "    inp1 = Dataset(Xdata,'r')\n",
    "    inp2 = Dataset(Ydata,'r')\n",
    "    sst_1 = np.zeros((myform,140,12,24,72))\n",
    "\n",
    "    #首年序列\n",
    "    #i= 21个模式\n",
    "    for i in range(myform):\n",
    "        sst_1[i,:,:,:,:] = inp1.variables['sst1'][1+141*i:141+141*i,0:12,:,:]\n",
    "    #(21,140,12,24,72)\n",
    "    #首年序列\n",
    "\n",
    "    \n",
    "    #flatted扁平化                                             \n",
    "    sst_2 = np.zeros((myform,1680,24,72))\n",
    "    for i in range(myform):\n",
    "        for j in range(140):\n",
    "            sst_2[i,j*12:(j+1)*12,:,:] = sst_1[i,j,:,:,:]\n",
    "    \n",
    "        #(21,1680,24,72)\n",
    "        #每个模式 139*12+1 = 1680 个序列 序列长度为12\n",
    "\n",
    "    #丢弃1个月 1862.1\n",
    "    sst_2 = sst_2[:,1:,:,:]\n",
    "    #(form,1679,24,72)\n",
    "\n",
    "\n",
    "    winsize = 12\n",
    "    #winnum = 1679-winsize +1 #1668\n",
    "    \"\"\"跳跃六个月滑窗 = 6\"\"\"\n",
    "    ts=6 \n",
    "    #(1679-12)/6+1 =27\n",
    "    winnum=int((1679-12)/ts +1) \n",
    "    \n",
    "    sst_3 = np.zeros((myform,winnum,12,24,72))\n",
    "    for i in range(myform):\n",
    "        for j in range(winnum):\n",
    "            #print(j)\n",
    "            sst_3[i,j,:,:,:] = sst_2[i,(j*ts):(j*ts)+winsize,:,:]\n",
    "            \n",
    "\n",
    "    sst_4 = np.zeros((int(winnum*myform),12,24,72))\n",
    "    #t300_4 = np.zeros((int(winnum*myform),12,24,72))\n",
    "    for i in range(myform):\n",
    "        sst_4[i*winnum:(i+1)*winnum,:,:,:] = sst_3[i,:,:,:,:]\n",
    "\n",
    "\n",
    "    #sst_4 = sst_4.swapaxes(1, 3)\n",
    "    trX = sst_4\n",
    "    del sst_1,sst_2,sst_3,sst_4\n",
    "    #trX = trX.reshape(int(winnum*myform),12,1728)\n",
    "\n",
    "    #保存np数组\n",
    "    #(5859, 12, 24, 72, 2)\n",
    "    #np.save(\"./CMIPdata/CMIP_trX_21_ts6_out.npy\",trX) \n",
    "\n",
    "    #label\n",
    "    #平铺\n",
    "    pr_1 = np.zeros(((myform),1692))#21,1692\n",
    "    for i in range(myform):\n",
    "        for j in range(141):\n",
    "            pr_1[i,j*12:(j+1)*12] = inp2.variables['pr'][(141*i)+j,:,0,0]\n",
    "            \n",
    "    #out\n",
    "    pr_2 =  np.zeros((myform,winnum,out))#form,1668,1\n",
    "    for i in range(myform):\n",
    "        for j in range(winnum):\n",
    "            pr_2[i,j,:] = pr_1[i,(j*ts):(j*ts)+out]\n",
    "\n",
    "    trY = np.zeros((int(winnum*myform),out))#form*1668,1\n",
    "    for i in range(myform):\n",
    "        trY[i*winnum:(i+1)*winnum,:] = pr_2[i,:,:]\n",
    "\n",
    "    #np.save(\"./CMIPdata/CMIP_trY_21_ts6_out%s.npy\"%out,trY)\n",
    "    trY_decoder_input = np.zeros((int(winnum*myform),out))\n",
    "    trY_decoder_input[:,1:] = trY[:,:-1]\n",
    "\n",
    "    #return trX[1:,:,:] ,trY.reshape(-1,24,1)[1:,:,:] ,trY_decoder_input\n",
    "    return trX ,trY.reshape(-1,24,1) ,trY_decoder_input.reshape(-1,24,1)\n",
    "\n",
    "def GOSDAdata(Xdata, Ydata, out):\n",
    "    #test data\n",
    "    inp11 = Dataset(Xdata,'r')\n",
    "    inp22 = Dataset(Ydata,'r')\n",
    "                                              \n",
    "    sst_11 = np.zeros((33,12,24,72)) #1983-2015\n",
    "    #t300_11 = np.zeros((33,12,24,72))\n",
    "\n",
    "    sst_11[:,:,:,:] = inp11.variables['sst'][3:,0:12,:,:]\n",
    "    #t300_11[:,:,:,:] = inp11.variables['t300'][3:,0:12,:,:]\n",
    "    #(33,12,24,72)\n",
    "\n",
    "    sst_22 = np.zeros((396,24,72))\n",
    "    #t300_22 = np.zeros((396,24,72))\n",
    "\n",
    "    for i in range(33):\n",
    "        sst_22[i*12:(i+1)*12,:,:] = sst_11[i,:,:,:]\n",
    "        #t300_22[i*12:(i+1)*12,:,:] = t300_11[i,:,:,:]\n",
    "    #(396,24,72)\n",
    "    #丢弃一个月，便于制作滑窗序列\n",
    "    sst_22 = sst_22[1:,:,:]\n",
    "    #t300_22 = t300_22[1:,:,:]#(395,24,72)\n",
    "\n",
    "    #滑窗\n",
    "    winsize = 12\n",
    "    winnum = 395-winsize + 1 #384\n",
    "    sst_33 = np.zeros((winnum, winsize, 24, 72))#(384, 12, 24, 72)\n",
    "    #t300_33 = np.zeros((winnum, winsize, 24, 72))\n",
    "\n",
    "    for i in range(winnum):\n",
    "        sst_33[i,:,:,:] = sst_22[i:i+12,:,:]\n",
    "        #t300_33[i,:,:,:] = t300_22[i:i+12,:,:]\n",
    "    #(384,12,24,72)\n",
    "\n",
    "\n",
    "    #channel = 2\n",
    "    #testX = np.zeros((winnum,12,24,72,2))#384\n",
    "    #testX[:,:,:,:,0] = sst_33\n",
    "    #testX[:,:,:,:,1] = t300_33\n",
    "    #sst_33 = sst_33.swapaxes(1, 3)\n",
    "\n",
    "    testX = sst_33#384\n",
    "    #testX[:,:,:,1] = t300_33.reshape(384,12,1728)\n",
    "    del sst_11,sst_22,sst_33\n",
    "    #del t300_11,t300_22,t300_33\n",
    "    #testX = testX.reshape(384,12,3456)\n",
    "\n",
    "    #label\n",
    "    inpv22 = np.zeros((408))#2017-1984 +1=34\n",
    "    for i in range(34):\n",
    "        inpv22[i*12:(i+1)*12] = inp22.variables['pr'][i+2,:,0,0]# +2:从1984开始\n",
    "        #(408)\n",
    "\n",
    "    testY = np.zeros((winnum,out,1))\n",
    "    #out = 1\n",
    "    #滑窗\n",
    "    for i in range(winnum):\n",
    "        testY[i,:,0] = inpv22[i:i+out]#24\n",
    "    #(384,out,1)\n",
    "\n",
    "    return testX, testY\n",
    "\n",
    "def SODAdata(Xdata, Ydata, out):\n",
    "    inp1 = Dataset(Xdata,'r')\n",
    "    inp2 = Dataset(Ydata,'r')\n",
    "\n",
    "    #time_step = 1 month                                               \n",
    "    sst_1 = np.zeros((99,12,24,72))\n",
    "    #t300_1 = np.zeros((99,12,24,72))\n",
    "\n",
    "    sst_1[:,:,:,:] = inp1.variables['sst'][1:,0:12,:,:]\n",
    "    #sst_1[:,:,:,:] = inp1.variables['t300'][1:,0:12,:,:]\n",
    "    #(99,12,24,72)\n",
    "    #1872-1970\n",
    "\n",
    "    sst_2 = np.zeros((1188,24,72))\n",
    "    #t300_2 = np.zeros((1188,24,72))\n",
    "\n",
    "    for i in range(99):\n",
    "        sst_2[i*12:(i+1)*12,:,:] = sst_1[i,:,:,:]\n",
    "        #t300_2[i*12:(i+1)*12,:,:] = t300_1[i,:,:,:]\n",
    "    #(1188,24,72)\n",
    "    #丢弃一个月，便于制作滑窗序列\n",
    "    sst_2 = sst_2[1:,:,:]\n",
    "    #t300_2 = t300_2[1:,:,:]#(1187,24,72)\n",
    "\n",
    "    sst_3 = np.zeros((1176,12,24,72))\n",
    "    #t300_3 = np.zeros((1176,12,24,72))\n",
    "    #滑窗\n",
    "    for i in range(1176):\n",
    "        sst_3[i:,:,:] = sst_2[i:i+12,:,:]\n",
    "        #t300_3[i,:,:,:] = sst_2[i:i+12,:,:]\n",
    "    #(1176,12,24,72)\n",
    "    trX = sst_3\n",
    "\n",
    "    #channel = 2\n",
    "    #trX = np.zeros((1176,12,24,72,2))\n",
    "    #trX[:,:,:,:,0] = sst_3\n",
    "    #trX[:,:,:,:,1] = t300_3\n",
    "\n",
    "    #保存np数组\n",
    "    #np.save(\"./SODAdata/SODA_trX_ts1_out.npy\",trX)\n",
    "\n",
    "    #label\n",
    "    inpv2 = np.zeros((1200))\n",
    "    for i in range(100):\n",
    "        inpv2[i*12:(i+1)*12] = inp2.variables['pr'][i,:,0,0]\n",
    "    #(1200)\n",
    "\n",
    "    #out = 1\n",
    "    trY = np.zeros((1176,out,1))\n",
    "    #滑窗\n",
    "    for i in range(1176):\n",
    "        trY[i,:,0] = inpv2[i:i+out]#24\n",
    "    #(1176,24)\n",
    "    \n",
    "    \n",
    "    trY_decoder_input = np.zeros((1176,out,1))\n",
    "    trY_decoder_input[:,1:,:] = trY[:,:-1,:]\n",
    "\n",
    "    #np.save(\"./SODAdata/SODA_trY_ts1_out%s.npy\"%out,trY) \n",
    "    return trX , trY, trY_decoder_input\n",
    "\n",
    "trX, trY, trY_decoder_input = CMIPdata('/home/d/Q/liyanqiu/saconvlstm/CMIP5.input.36mon.1861_2001.nc',\\\n",
    "                                          '/home/d/Q/liyanqiu/saconvlstm/CMIP5.label.12mon.1863_2003.nc', 24,  21)\n",
    "\n",
    "testX, testY = GOSDAdata('/home/d/Q/liyanqiu/saconvlstm/GODAS.input.36mon.1980_2015.nc',\\\n",
    "                         '/home/d/Q/liyanqiu/saconvlstm/GODAS.label.12mon.1982_2017.nc', 24)\n",
    "\n",
    "trX_SODA, trY_SODA ,trY_decoder_input_SODA = SODAdata('/home/d/Q/liyanqiu/saconvlstm/SODA.input.36mon.1871_1970.nc',\n",
    "                              '/home/d/Q/liyanqiu/saconvlstm/SODA.label.12mon.1873_1972.nc',24)\n",
    "\n",
    "indices = np.arange(trX.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "trX = trX[indices]\n",
    "trY = trY[indices]\n",
    "trY_decoder_input = trY_decoder_input[indices]\n",
    "\n",
    "# shuffle\n",
    "indices_soda = np.arange(trX_SODA.shape[0])\n",
    "np.random.shuffle(indices_soda)\n",
    "trX_SODA = trX_SODA[indices_soda]\n",
    "trY_SODA = trY_SODA[indices_soda]\n",
    "trY_decoder_input_SODA = trY_decoder_input_SODA[indices_soda]\n",
    "\n",
    "class MyDataSet(Data.Dataset):\n",
    "    \"\"\"自定义DataLoader\"\"\"\n",
    "\n",
    "    def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.enc_inputs = enc_inputs\n",
    "        self.dec_inputs = dec_inputs\n",
    "        self.dec_outputs = dec_outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.enc_inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    "    \n",
    "batch_size = 128\n",
    "trainsize = 2500 #选择cmip训练数量\n",
    "trX = trX[:trainsize]\n",
    "trY = trY[:trainsize]\n",
    "trY_decoder_input = trY_decoder_input[:trainsize]\n",
    "\n",
    "#trY = trY[:, 5:8, :]\n",
    "#trY_decoder_input = trY_decoder_input[:, 5:8, :]\n",
    "\n",
    "split_index = int(trX.shape[0]*0.8)\n",
    "split_index_soda = int(trX_SODA.shape[0]*0.8)\n",
    "print(\"split_index_soda:\" + str(split_index_soda))\n",
    "\n",
    "print(\"split_index:\" + str(split_index))\n",
    "loader_train = Data.DataLoader(\n",
    "    MyDataSet(\n",
    "        trX[:split_index].astype(np.float32), \n",
    "        trY_decoder_input[:split_index].astype(np.float32), \n",
    "        trY[:split_index].astype(np.float32)), \n",
    "    batch_size, shuffle = True)\n",
    "loader_valid = Data.DataLoader(\n",
    "    MyDataSet(\n",
    "        trX[split_index:].astype(np.float32), \n",
    "        trY_decoder_input[split_index:].astype(np.float32), \n",
    "        trY[split_index:].astype(np.float32)), \n",
    "    batch_size, shuffle=False)\n",
    "\n",
    "loader_train_sado = Data.DataLoader(\n",
    "    MyDataSet(\n",
    "        trX_SODA[:split_index_soda].astype(np.float32), \n",
    "        trY_decoder_input_SODA[:split_index_soda].astype(np.float32), \n",
    "        trY_SODA[:split_index_soda].astype(np.float32)), \n",
    "    batch_size, shuffle = True)\n",
    "loader_valid_soda = Data.DataLoader(\n",
    "    MyDataSet(\n",
    "        trX_SODA[split_index_soda:].astype(np.float32), \n",
    "        trY_decoder_input_SODA[split_index_soda:].astype(np.float32), \n",
    "        trY_SODA[split_index_soda:].astype(np.float32)), \n",
    "    batch_size, shuffle=False)\n",
    "\n",
    "print(trX.shape)\n",
    "print(trY_decoder_input.shape)\n",
    "print(trY.shape)\n",
    "print(testX.shape)\n",
    "print(len(testX))\n",
    "print(trX_SODA.shape)\n",
    "print(trY_decoder_input_SODA.shape)\n",
    "print(trY_SODA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if on cuda: True\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "src_len = 12\n",
    "tgt_len = 24\n",
    "tgt_vocab_size = 1 # out unit\n",
    "\n",
    "filters = 30 #50\n",
    "d_model = 540 #900  # Embedding Size（token embedding和position编码的维度） 加入CNN！！！\n",
    "#d_model = 24*72\n",
    "d_ff = 2048  # FeedForward dimension (两次线性层中的隐藏层 512->2048->512，线性层是用来做特征提取的），当然最后会再接一个projection层\n",
    "d_k = d_v = 64  # dimension of K(=Q), V（Q和K的维度需要相同，这里为了方便让K=V）\n",
    "encoder_n_layers = 1  # number of Encoder  Layer（Block的个数）\n",
    "decoder_n_layers = 1  # # number of  Decoder Layer（Block的个数）\n",
    "n_heads = 1 # number of heads in Multi-Head Attention（有几套头）\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    # pad mask的作用：在对value向量加权平均的时候，可以让pad对应的alpha_ij=0，这样注意力就不会考虑到pad向量\n",
    "    \"\"\"这里的q,k表示的是两个序列（跟注意力机制的q,k没有关系），例如encoder_inputs (x1,x2,..xm)和encoder_inputs (x1,x2..xm)\n",
    "    encoder和decoder都可能调用这个函数，所以seq_len视情况而定\n",
    "    seq_q: [batch_size, seq_len]\n",
    "    seq_k: [batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    batch_size, len_q, = seq_q.size(0), seq_q.size(1)  # 这个seq_q只是用来expand维度的\n",
    "    batch_size, len_k, = seq_k.size(0), seq_k.size(1)\n",
    "    # eq(zero) is PAD token\n",
    "    a = torch.ones(batch_size, 1 ,len_k)\n",
    "    pad_attn_mask = a.data.eq(0)\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k).to(device)  # [batch_size, len_q, len_k] 构成一个立方体(batch_size个这样的矩阵)\n",
    "\n",
    "def get_attn_subsequence_mask(seq):\n",
    "    \"\"\"\n",
    "    seq: [batch_size, tgt_len]\n",
    "    \"\"\"\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    # attn_shape: [batch_size, tgt_len, tgt_len]\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1)  # 生成一个上三角矩阵\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\n",
    "    # print(subsequence_mask.data)\n",
    "    return subsequence_mask.to(device)  # [batch_size, tgt_len, tgt_len]\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K: [batch_size, n_heads, len_k, d_k]\n",
    "        V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "        说明：在encoder-decoder的Attention层中len_q(q1,..qt)和len_k(k1,...km)可能不同\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)  # scores : [batch_size, n_heads, len_q, len_k]\n",
    "        # mask矩阵填充scores（用-1e9填充scores中与attn_mask中值为1位置相对应的元素）\n",
    "        scores.masked_fill_(attn_mask, -1e9)  # Fills elements of self tensor with value where mask is True.\n",
    "\n",
    "        attn = nn.Softmax(dim=-1)(scores)  # 对最后一个维度(v)做softmax\n",
    "        # scores : [batch_size, n_heads, len_q, len_k] * V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        context = torch.matmul(attn, V)  # context: [batch_size, n_heads, len_q, d_v]\n",
    "        # context：[[z1,z2,...],[...]]向量, attn注意力稀疏矩阵（用于可视化的）\n",
    "        return context, attn\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"这个Attention类可以实现:\n",
    "    Encoder的Self-Attention\n",
    "    Decoder的Masked Self-Attention\n",
    "    Encoder-Decoder的Attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)  # q,k必须维度相同，不然无法做点积\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "\n",
    "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
    "        \"\"\"\n",
    "        input_Q: [batch_size, len_q, d_model]\n",
    "        input_K: [batch_size, len_k, d_model]\n",
    "        input_V: [batch_size, len_v(=len_k), d_model]\n",
    "        attn_mask: [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        residual, batch_size = input_Q, input_Q.size(0)\n",
    "        # 下面的多头的参数矩阵是放在一起做线性变换的，然后再拆成多个头，这是工程实现的技巧\n",
    "        # B: batch_size, S:seq_len, D: dim\n",
    "        # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, Head, W) -trans-> (B, Head, S, W)\n",
    "        #           线性变换               拆成多头\n",
    "\n",
    "        # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
    "        # K: [batch_size, n_heads, len_k, d_k] # K和V的长度一定相同，维度可以不同\n",
    "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
    "        # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1, 2)\n",
    "\n",
    "        # 因为是多头，所以mask矩阵要扩充成4维的\n",
    "        # attn_mask: [batch_size, seq_len, seq_len] -> [batch_size, n_heads, seq_len, seq_len]\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "\n",
    "        # context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\n",
    "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\n",
    "        # 下面将不同头的输出向量拼接在一起\n",
    "        # context: [batch_size, n_heads, len_q, d_v] -> [batch_size, len_q, n_heads * d_v]\n",
    "        context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v)\n",
    "        # 再做一个projection\n",
    "        output = self.fc(context)  # [batch_size, len_q, d_model]\n",
    "        return nn.LayerNorm(d_model).to(device)(output + residual), attn\n",
    "    \n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        residual = inputs\n",
    "        output = self.fc(inputs)\n",
    "        return nn.LayerNorm(d_model).to(device)(output + residual)  # [batch_size, seq_len, d_model]\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        \"\"\"E\n",
    "        enc_inputs: [batch_size, src_len, d_model]\n",
    "        enc_self_attn_mask: [batch_size, src_len, src_len]  mask矩阵(pad mask or sequence mask)\n",
    "        \"\"\"\n",
    "        # enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]\n",
    "        # 第一个enc_inputs * W_Q = Q\n",
    "        # 第二个enc_inputs * W_K = K\n",
    "        # 第三个enc_inputs * W_V = V\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs,\n",
    "                                               enc_self_attn_mask)  # enc_inputs to same Q,K,V（未线性变换前）\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        # enc_outputs: [batch_size, src_len, d_model]\n",
    "        return enc_outputs, attn\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
    "        \"\"\"\n",
    "        dec_inputs: [batch_size, tgt_len, d_model]\n",
    "        enc_outputs: [batch_size, src_len, d_model]\n",
    "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
    "        dec_enc_attn_mask: [batch_size, tgt_len, src_len]\n",
    "        \"\"\"\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)  # 这里的Q,K,V全是Decoder自己的输入\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
    "        #dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs,dec_enc_attn_mask)  # Attention层的Q(来自decoder) 和 K,V(来自encoder)\n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_inputs, enc_outputs, enc_outputs,dec_enc_attn_mask) \n",
    "       \n",
    "        dec_outputs = self.pos_ffn(dec_outputs)  # [batch_size, tgt_len, d_model]\n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn  # dec_self_attn, dec_enc_attn这两个是为了可视化的\n",
    "        \n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        #self.src_emb = nn.Embedding(src_vocab_size, d_model)  # token Embedding\n",
    "        self.conv1=nn.Conv2d(1,filters,(4,8))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2=nn.Conv2d(filters,filters,(4,8))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv3=nn.Conv2d(filters,filters,(2,4))\n",
    "        \n",
    "        self.pos_emb = PositionalEncoding(d_model)  # Transformer中位置编码时固定的，不需要学习\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(encoder_n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs):\n",
    "        \"\"\"\n",
    "        enc_inputs: [batch_size, src_len,24,72]\n",
    "        \"\"\"\n",
    "        #enc_outputs = self.src_emb(enc_inputs)  # [batch_size, src_len, d_model]\n",
    "        \n",
    "        enc_outputs = enc_inputs.reshape(-1, 24, 72).unsqueeze(1)\n",
    "        #print(enc_outputs.size())\n",
    "        enc_outputs = self.conv1(enc_outputs)\n",
    "        #print(enc_outputs.size())\n",
    "        enc_outputs = self.pool1(enc_outputs)\n",
    "        enc_outputs = self.conv2(enc_outputs)\n",
    "        #print(enc_outputs.size())\n",
    "        enc_outputs = self.pool2(enc_outputs)\n",
    "        enc_outputs = self.conv3(enc_outputs)\n",
    "        #print(enc_outputs.size())\n",
    "        enc_outputs = enc_outputs.reshape(-1, src_len,enc_outputs.size(-1) * enc_outputs.size(-2) * enc_outputs.size(-3))\n",
    "        #[-1, src_len,30 * 18 * 6]\n",
    "        #print(enc_outputs.size())\n",
    "        \n",
    "        #enc_outputs = enc_inputs.reshape(-1, 12,24*72)\n",
    "\n",
    "        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1)  # [batch_size, src_len, d_model]\n",
    "        # Encoder输入序列的pad mask矩阵\n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)  # [batch_size, src_len, src_len]\n",
    "        enc_self_attns = []  # 画热力图等\n",
    "        for layer in self.layers:  # for循环访问nn.ModuleList对象\n",
    "            # 上一个block的输出enc_outputs作为当前block的输入\n",
    "            # enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs,\n",
    "                                               enc_self_attn_mask)  # 传入的enc_outputs其实是input，传入mask矩阵是因为你要做self attention\n",
    "            enc_self_attns.append(enc_self_attn)  # 这个只是为了可视化\n",
    "        return enc_outputs, enc_self_attns\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        #self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)  # Decoder输入的embed词表\n",
    "        self.expand_linear = nn.Linear(tgt_vocab_size, d_model, bias=True)# 可学习query\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(decoder_n_layers)])  # Decoder的blocks\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        \"\"\"\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        enc_outputs: [batch_size, src_len, d_model]   # 用在Encoder-Decoder Attention层\n",
    "        \"\"\"\n",
    "        \n",
    "        dec_outputs = self.expand_linear(dec_inputs).to(device) # [batch_size, 24, d_model]\n",
    "        \n",
    "        #dec_outputs = dec_inputs.repeat(1,1,d_model) \n",
    "        dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1)).transpose(0, 1).to(device)  \n",
    "        # [batch_size, tgt_len, d_model]\n",
    "        # Decoder输入序列的pad mask矩阵（这个例子中decoder是没有加pad的，实际应用中都是有pad填充的）\n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).to(device)  # [batch_size, tgt_len, tgt_len]\n",
    "        # Masked Self_Attention：当前时刻是看不到未来的信息的\n",
    "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).to(\n",
    "            device)  # [batch_size, tgt_len, tgt_len]\n",
    "\n",
    "        # Decoder中把两种mask矩阵相加（既屏蔽了pad的信息，也屏蔽了未来时刻的信息）\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask),\n",
    "                                      0).to(device)  # [batch_size, tgt_len, tgt_len]; torch.gt比较两个矩阵的元素，大于则返回1，否则返回0\n",
    "\n",
    "        # 这个mask主要用于encoder-decoder attention层\n",
    "        # get_attn_pad_mask主要是enc_inputs的pad mask矩阵(因为enc是处理K,V的，求Attention时是用v1,v2,..vm去加权的，要把pad对应的v_i的相关系数设为0，这样注意力就不会关注pad向量)\n",
    "        #                       dec_inputs只是提供expand的size的\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)  # [batc_size, tgt_len, src_len]\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
    "            # Decoder的Block是上一个Block的输出dec_outputs（变化）和Encoder网络的输出enc_outputs（固定）\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model]\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder().to(device)\n",
    "        self.decoder = Decoder().to(device)\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False).to(device)\n",
    "\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        \"\"\"Transformers的输入：两个序列\n",
    "        enc_inputs: [batch_size, src_len]\n",
    "        修改\n",
    "        enc_inputs: [batch_size, src_len, 24,72]\n",
    "        dec_inputs: [batch_size, tgt_len,1]\n",
    "        \"\"\"\n",
    "        # tensor to store decoder outputs\n",
    "        # outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "\n",
    "        # enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]\n",
    "        # 经过Encoder网络后，得到的输出还是[batch_size, src_len, d_model]\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model] -> dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n",
    "        dec_logits = self.projection(dec_outputs)\n",
    "        #return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n",
    "        return dec_logits, enc_self_attns, dec_self_attns, dec_enc_attns\n",
    "\n",
    "\n",
    "model = Transformer().to(device)\n",
    "\n",
    "        \n",
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, pred, truth):\n",
    "        \"\"\"计算相关系数\n",
    "        Args:\n",
    "            pred ([type]): [batch_size, 24, 1]\n",
    "            truth ([type]): [batch_size, 24, 1]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [batch_size个相关系数的均值]\n",
    "        \"\"\"\n",
    "        mse = 0.\n",
    "        cost = torch.pow((truth.squeeze() - pred.squeeze()), 2)\n",
    "        #print(cost.size())\n",
    "        k = torch.arange(1, 25).unsqueeze(0).to(device)\n",
    "        loss = torch.mul(cost, k)\n",
    "        #print(loss.size())\n",
    "        \n",
    "        return  torch.mean(loss)\n",
    "\n",
    "#criterion = MyLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "print(\"if on cuda:\",next(model.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "================================================================================2022-03-01 22:12:13\n",
      "\n",
      "EPOCH = 1, loss = 0.014, val_loss = 0.014, \n",
      "\n",
      "================================================================================2022-03-01 22:12:13\n",
      "\n",
      "EPOCH = 2, loss = 0.014, val_loss = 0.013, \n",
      "\n",
      "================================================================================2022-03-01 22:12:14\n",
      "\n",
      "EPOCH = 3, loss = 0.013, val_loss = 0.013, \n",
      "\n",
      "================================================================================2022-03-01 22:12:14\n",
      "\n",
      "EPOCH = 4, loss = 0.013, val_loss = 0.013, \n",
      "\n",
      "================================================================================2022-03-01 22:12:15\n",
      "\n",
      "EPOCH = 5, loss = 0.013, val_loss = 0.014, \n",
      "\n",
      "================================================================================2022-03-01 22:12:16\n",
      "\n",
      "EPOCH = 6, loss = 0.013, val_loss = 0.013, \n",
      "\n",
      "================================================================================2022-03-01 22:12:16\n",
      "\n",
      "EPOCH = 7, loss = 0.013, val_loss = 0.013, \n",
      "\n",
      "================================================================================2022-03-01 22:12:17\n",
      "\n",
      "EPOCH = 8, loss = 0.013, val_loss = 0.013, \n",
      "\n",
      "================================================================================2022-03-01 22:12:17\n",
      "\n",
      "EPOCH = 9, loss = 0.013, val_loss = 0.013, \n",
      "\n",
      "================================================================================2022-03-01 22:12:18\n",
      "\n",
      "EPOCH = 10, loss = 0.013, val_loss = 0.013, \n",
      "\n",
      "================================================================================2022-03-01 22:12:18\n",
      "\n",
      "EPOCH = 11, loss = 0.014, val_loss = 0.014, \n",
      "\n",
      "================================================================================2022-03-01 22:12:19\n",
      "\n",
      "EPOCH = 12, loss = 0.014, val_loss = 0.013, \n",
      "\n",
      "================================================================================2022-03-01 22:12:19\n",
      "\n",
      "EPOCH = 13, loss = 0.013, val_loss = 0.013, \n",
      "\n",
      "================================================================================2022-03-01 22:12:20\n",
      "\n",
      "EPOCH = 14, loss = 0.013, val_loss = 0.013, \n",
      "\n",
      "================================================================================2022-03-01 22:12:20\n",
      "\n",
      "EPOCH = 15, loss = 0.013, val_loss = 0.013, \n",
      "\n",
      "================================================================================2022-03-01 22:12:21\n",
      "Finished Training...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import datetime\n",
    "dfhistory = pd.DataFrame(columns = [\"epoch\",\"loss\",\"val_loss\"]) \n",
    "print(\"Start Training...\")\n",
    "nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"==========\"*8 + \"%s\"%nowtime)\n",
    "bestloss = float(\"inf\")\n",
    "epoch = 15\n",
    "for epoch in range(1,epoch+1):\n",
    "    #train----------------------------------------------------\n",
    "  model.train()\n",
    "  loss_sum = 0.0\n",
    "  step = 1\n",
    "  for step,(enc_inputs, dec_inputs, dec_outputs) in enumerate(loader_train, 1):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 正向\n",
    "      enc_inputs, dec_inputs, dec_outputs = enc_inputs.to(device), dec_inputs.to(device), dec_outputs.to(device)\n",
    "      outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "      # print(outputs.size()) [32, 24, 1]\n",
    "      #loss = criterion(outputs.view(-1), dec_outputs.view(-1)) \n",
    "      loss = criterion(outputs, dec_outputs) \n",
    "\n",
    "      # 反向\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # print\n",
    "      loss_sum += loss.item()\n",
    "      if step%100 == 0:\n",
    "        print((\"[step = %d] loss: %.3f, \") % (step, loss_sum/step))\n",
    "\n",
    "\n",
    "  #valid\n",
    "  model.eval()\n",
    "  val_loss_sum = 0.0\n",
    "  val_step = 1\n",
    "\n",
    "  for val_step, (enc_inputs, dec_inputs, dec_outputs) in enumerate(loader_valid, 1):\n",
    "    with torch.no_grad():# 节点不进行求梯度\n",
    "      enc_inputs, dec_inputs, dec_outputs = enc_inputs.to(device), dec_inputs.to(device), dec_outputs.to(device)\n",
    "      outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "      #val_loss = criterion(outputs.view(-1), dec_outputs.view(-1)) \n",
    "      val_loss =criterion(outputs, dec_outputs) \n",
    "\n",
    "    val_loss_sum += val_loss.item()\n",
    "\n",
    "\n",
    "  # log\n",
    "  info = (epoch, loss_sum/step, val_loss_sum/val_step)\n",
    "  dfhistory.loc[epoch-1] = info # epoch从1开始 index -1\n",
    "\n",
    "  # print\n",
    "  print((\"\\nEPOCH = %d, loss = %.3f,\"+\" val_loss = %.3f, \")% info)\n",
    "  nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "  print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "  \n",
    "  if val_loss_sum <= bestloss: #换成小于等于比小于效果更好\n",
    "        bestloss = val_loss_sum\n",
    "        bestmodel = model\n",
    "        \n",
    "\n",
    "print('Finished Training...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# soda微调(没啥用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "================================================================================2022-03-01 22:12:23\n",
      "\n",
      "EPOCH = 1, loss = 0.018, val_loss = 0.017, \n",
      "\n",
      "================================================================================2022-03-01 22:12:23\n",
      "\n",
      "EPOCH = 2, loss = 0.019, val_loss = 0.017, \n",
      "\n",
      "================================================================================2022-03-01 22:12:23\n",
      "Finished Training...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import datetime\n",
    "dfhistory = pd.DataFrame(columns = [\"epoch\",\"loss\",\"val_loss\"]) \n",
    "print(\"Start Training...\")\n",
    "nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"==========\"*8 + \"%s\"%nowtime)\n",
    "epoch = 2\n",
    "for epoch in range(1,epoch+1):\n",
    "    #train----------------------------------------------------\n",
    "  model.train()\n",
    "  loss_sum = 0.0\n",
    "  step = 1\n",
    "  for step,(enc_inputs, dec_inputs, dec_outputs) in enumerate(loader_train_sado, 1):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 正向\n",
    "      enc_inputs, dec_inputs, dec_outputs = enc_inputs.to(device), dec_inputs.to(device), dec_outputs.to(device)\n",
    "      outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "      # print(outputs.size()) [32, 24, 1]\n",
    "      #loss = criterion(outputs.view(-1), dec_outputs.view(-1)) \n",
    "      loss = criterion(outputs, dec_outputs) \n",
    "\n",
    "      # 反向\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # print\n",
    "      loss_sum += loss.item()\n",
    "      if step%100 == 0:\n",
    "        print((\"[step = %d] loss: %.3f, \") % (step, loss_sum/step))\n",
    "\n",
    "\n",
    "  #valid\n",
    "  model.eval()\n",
    "  val_loss_sum = 0.0\n",
    "  val_step = 1\n",
    "\n",
    "  for val_step, (enc_inputs, dec_inputs, dec_outputs) in enumerate(loader_valid_soda, 1):\n",
    "    with torch.no_grad():# 节点不进行求梯度\n",
    "      enc_inputs, dec_inputs, dec_outputs = enc_inputs.to(device), dec_inputs.to(device), dec_outputs.to(device)\n",
    "      outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "      #val_loss = criterion(outputs.view(-1), dec_outputs.view(-1)) \n",
    "      val_loss =criterion(outputs, dec_outputs) \n",
    "\n",
    "    val_loss_sum += val_loss.item()\n",
    "\n",
    "  # log\n",
    "  info = (epoch, loss_sum/step, val_loss_sum/val_step)\n",
    "  dfhistory.loc[epoch-1] = info # epoch从1开始 index -1\n",
    "\n",
    "  # print\n",
    "  print((\"\\nEPOCH = %d, loss = %.3f,\"+\" val_loss = %.3f, \")% info)\n",
    "  nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "  print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "\n",
    "print('Finished Training...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test No.100\n",
      "test No.200\n",
      "test No.300\n",
      "(384, 24)\n",
      "[0.96358644 0.91629727 0.85234677 0.77180521 0.68166226 0.59048313\n",
      " 0.50725966 0.43788118 0.38418784 0.34469092 0.31310734 0.28147887\n",
      " 0.25307009 0.22828535 0.20364979 0.17649899 0.14840572 0.12301691\n",
      " 0.09899502 0.07415278 0.05103727 0.03477643 0.02939612 0.03578624]\n",
      "sum :8.501857600706028\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================================\n",
    "# 预测阶段\n",
    "# testX, testY\n",
    "def test_decoder(modle, enc_input):\n",
    "  enc_input = torch.from_numpy(enc_input.astype(np.float32)).to(device)\n",
    "    \n",
    "  enc_outputs, enc_self_attns = bestmodel.encoder(enc_input)\n",
    "  dec_input = torch.zeros(1, 0, 1).type_as(enc_input.data)# type转变\n",
    "  next_symbol = 0.\n",
    "  # print(enc_outputs.size())\n",
    "  # print(enc_input.size())\n",
    "    \n",
    "  for i in range(tgt_len):\n",
    "      # next_symbol两维\n",
    "      # print(i)\n",
    "      dec_input = torch.cat([dec_input.to(device), torch.tensor([[[next_symbol]]], dtype=enc_input.dtype).to(device)], -2)\n",
    "      # print(dec_input.size())\n",
    "      # print(dec_input.data)\n",
    "      dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n",
    "      # print(dec_outputs.size())\n",
    "      projected = model.projection(dec_outputs)\n",
    "      # print(projected.size())\n",
    "      prob = projected.squeeze(0)\n",
    "      # print(\"prob\")\n",
    "      # print(prob.data)\n",
    "      next_word = prob.data[-1].item()\n",
    "      next_symbol = next_word\n",
    "      # print(next_word)\n",
    "        \n",
    "  dec_input = torch.cat([dec_input.to(device), torch.tensor([[[next_symbol]]], dtype=enc_input.dtype).to(device)], -2)\n",
    "  dec_predict = dec_input[:, 1:]\n",
    "  return dec_predict\n",
    "\n",
    "# 每步预测的值都存入数组中\n",
    "seqout = []\n",
    "for i in range(len(testX)):\n",
    "  testout = test_decoder(model, testX[i].reshape(1, 12, 24, 72))\n",
    "  seqout.append(testout.squeeze().cpu().numpy())\n",
    "  if (i+1)%100 ==0:\n",
    "    print(\"test No.\" + str(i+1))\n",
    "\n",
    "seqout = np.array(seqout)\n",
    "print(seqout.shape)\n",
    "cor = np.zeros((tgt_len))\n",
    "\n",
    "for i in range(tgt_len):\n",
    "  cor[i] = np.corrcoef(testY.reshape(384,tgt_len)[:,i], np.array(seqout)[:,i])[0,1]\n",
    "print(cor)\n",
    "print(\"sum :\" + str(np.sum(cor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('result/seqout9',seqout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decoder(model, testX[0].reshape(1, 12, 24, 72))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 24)\n",
      "(384, 24)\n",
      "[0.97378202 0.9388711  0.8928915  0.83864513 0.77779451 0.71220014\n",
      " 0.6494573  0.59994692 0.57515166 0.57579443 0.59315061 0.61125964\n",
      " 0.61754696 0.60905918 0.58648416 0.55099123 0.50059509 0.43899815\n",
      " 0.37554191 0.32122516 0.27923136 0.25239781 0.24382802 0.25073651]\n",
      "sum :13.765580506731908\n"
     ]
    }
   ],
   "source": [
    "sum  = np.zeros((384, 24))\n",
    "num = 10\n",
    "for i in range(num):\n",
    "    tmp = np.load('result/seqout'+str(i+1) + '.npy')\n",
    "    #print(tmp)\n",
    "    sum =sum + tmp\n",
    "    \n",
    "print(sum.shape)\n",
    "seqout = sum/num \n",
    "print(seqout.shape)\n",
    "tgt_len=24\n",
    "cor = np.zeros((tgt_len))\n",
    "\n",
    "for i in range(tgt_len):\n",
    "  cor[i] = np.corrcoef(testY.reshape(384,tgt_len)[:,i], np.array(seqout)[:,i])[0,1]\n",
    "print(cor)\n",
    "print(\"sum :\" + str(np.sum(cor)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('enso-q')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "9bba8c7d9c8496bd6992670a29c4e9a9a45ed2b8a8c8d7c9266649db153117bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
