{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5e577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import pickle, functools, operator\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import random #shuffle\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "def CMIPdata(Xdata, Ydata, out , myform):\n",
    "    inp1 = Dataset(Xdata,'r')\n",
    "    inp2 = Dataset(Ydata,'r')\n",
    "    sst_1 = np.zeros((myform,140,12,24,72))\n",
    "\n",
    "    #首年序列\n",
    "    #i= 21个模式\n",
    "    for i in range(myform):\n",
    "        sst_1[i,:,:,:,:] = inp1.variables['sst1'][1+141*i:141+141*i,0:12,:,:]\n",
    "    #(21,140,12,24,72)\n",
    "    #首年序列\n",
    "\n",
    "    \n",
    "    #flatted扁平化                                             \n",
    "    sst_2 = np.zeros((myform,1680,24,72))\n",
    "    for i in range(myform):\n",
    "        for j in range(140):\n",
    "            sst_2[i,j*12:(j+1)*12,:,:] = sst_1[i,j,:,:,:]\n",
    "    \n",
    "        #(21,1680,24,72)\n",
    "        #每个模式 139*12+1 = 1680 个序列 序列长度为12\n",
    "\n",
    "    #丢弃1个月 1862.1\n",
    "    sst_2 = sst_2[:,1:,:,:]\n",
    "    #(form,1679,24,72)\n",
    "\n",
    "\n",
    "    winsize = 12\n",
    "    #winnum = 1679-winsize +1 #1668\n",
    "    \"\"\"跳跃六个月滑窗 = 6\"\"\"\n",
    "    ts=6 \n",
    "    #(1679-12)/6+1 =27\n",
    "    winnum=int((1679-12)/ts +1) \n",
    "    \n",
    "    sst_3 = np.zeros((myform,winnum,12,24,72))\n",
    "    for i in range(myform):\n",
    "        for j in range(winnum):\n",
    "            #print(j)\n",
    "            sst_3[i,j,:,:,:] = sst_2[i,(j*ts):(j*ts)+winsize,:,:]\n",
    "            \n",
    "\n",
    "    sst_4 = np.zeros((int(winnum*myform),12,24,72))\n",
    "    #t300_4 = np.zeros((int(winnum*myform),12,24,72))\n",
    "    for i in range(myform):\n",
    "        sst_4[i*winnum:(i+1)*winnum,:,:,:] = sst_3[i,:,:,:,:]\n",
    "\n",
    "\n",
    "    #sst_4 = sst_4.swapaxes(1, 3)\n",
    "    trX = sst_4\n",
    "    del sst_1,sst_2,sst_3,sst_4\n",
    "    #trX = trX.reshape(int(winnum*myform),12,1728)\n",
    "\n",
    "    #保存np数组\n",
    "    #(5859, 12, 24, 72, 2)\n",
    "    #np.save(\"./CMIPdata/CMIP_trX_21_ts6_out.npy\",trX) \n",
    "\n",
    "    #label\n",
    "    #平铺\n",
    "    pr_1 = np.zeros(((myform),1692))#21,1692\n",
    "    for i in range(myform):\n",
    "        for j in range(141):\n",
    "            pr_1[i,j*12:(j+1)*12] = inp2.variables['pr'][(141*i)+j,:,0,0]\n",
    "            \n",
    "    #out\n",
    "    pr_2 =  np.zeros((myform,winnum,out))#form,1668,1\n",
    "    for i in range(myform):\n",
    "        for j in range(winnum):\n",
    "            pr_2[i,j,:] = pr_1[i,(j*ts):(j*ts)+out]\n",
    "\n",
    "    trY = np.zeros((int(winnum*myform),out))#form*1668,1\n",
    "    for i in range(myform):\n",
    "        trY[i*winnum:(i+1)*winnum,:] = pr_2[i,:,:]\n",
    "\n",
    "    #np.save(\"./CMIPdata/CMIP_trY_21_ts6_out%s.npy\"%out,trY)\n",
    "    trY_decoder_input = np.zeros((int(winnum*myform),out))\n",
    "    trY_decoder_input[:,1:] = trY[:,:-1]\n",
    "\n",
    "    #return trX[1:,:,:] ,trY.reshape(-1,24,1)[1:,:,:] ,trY_decoder_input\n",
    "    return trX ,trY.reshape(-1,24,1) ,trY_decoder_input.reshape(-1,24,1)\n",
    "\n",
    "def GOSDAdata(Xdata, Ydata, out):\n",
    "    #test data\n",
    "    inp11 = Dataset(Xdata,'r')\n",
    "    inp22 = Dataset(Ydata,'r')\n",
    "                                              \n",
    "    sst_11 = np.zeros((33,12,24,72)) #1983-2015\n",
    "    #t300_11 = np.zeros((33,12,24,72))\n",
    "\n",
    "    sst_11[:,:,:,:] = inp11.variables['sst'][3:,0:12,:,:]\n",
    "    #t300_11[:,:,:,:] = inp11.variables['t300'][3:,0:12,:,:]\n",
    "    #(33,12,24,72)\n",
    "\n",
    "    sst_22 = np.zeros((396,24,72))\n",
    "    #t300_22 = np.zeros((396,24,72))\n",
    "\n",
    "    for i in range(33):\n",
    "        sst_22[i*12:(i+1)*12,:,:] = sst_11[i,:,:,:]\n",
    "        #t300_22[i*12:(i+1)*12,:,:] = t300_11[i,:,:,:]\n",
    "    #(396,24,72)\n",
    "    #丢弃一个月，便于制作滑窗序列\n",
    "    sst_22 = sst_22[1:,:,:]\n",
    "    #t300_22 = t300_22[1:,:,:]#(395,24,72)\n",
    "\n",
    "    #滑窗\n",
    "    winsize = 12\n",
    "    winnum = 395-winsize + 1 #384\n",
    "    sst_33 = np.zeros((winnum, winsize, 24, 72))#(384, 12, 24, 72)\n",
    "    #t300_33 = np.zeros((winnum, winsize, 24, 72))\n",
    "\n",
    "    for i in range(winnum):\n",
    "        sst_33[i,:,:,:] = sst_22[i:i+12,:,:]\n",
    "        #t300_33[i,:,:,:] = t300_22[i:i+12,:,:]\n",
    "    #(384,12,24,72)\n",
    "\n",
    "\n",
    "    #channel = 2\n",
    "    #testX = np.zeros((winnum,12,24,72,2))#384\n",
    "    #testX[:,:,:,:,0] = sst_33\n",
    "    #testX[:,:,:,:,1] = t300_33\n",
    "    #sst_33 = sst_33.swapaxes(1, 3)\n",
    "\n",
    "    testX = sst_33#384\n",
    "    #testX[:,:,:,1] = t300_33.reshape(384,12,1728)\n",
    "    del sst_11,sst_22,sst_33\n",
    "    #del t300_11,t300_22,t300_33\n",
    "    #testX = testX.reshape(384,12,3456)\n",
    "\n",
    "    #label\n",
    "    inpv22 = np.zeros((408))#2017-1984 +1=34\n",
    "    for i in range(34):\n",
    "        inpv22[i*12:(i+1)*12] = inp22.variables['pr'][i+2,:,0,0]# +2:从1984开始\n",
    "        #(408)\n",
    "\n",
    "    testY = np.zeros((winnum,out,1))\n",
    "    #out = 1\n",
    "    #滑窗\n",
    "    for i in range(winnum):\n",
    "        testY[i,:,0] = inpv22[i:i+out]#24\n",
    "    #(384,out,1)\n",
    "\n",
    "    return testX, testY\n",
    "\n",
    "\n",
    "trX, trY, trY_decoder_input = CMIPdata('/home/d/Q/saconvlstm/CMIP5.input.36mon.1861_2001.nc',\\\n",
    "                                          '/home/d/Q/saconvlstm/CMIP5.label.12mon.1863_2003.nc', 24,  21)\n",
    "\n",
    "testX, testY = GOSDAdata('/home/d/Q/saconvlstm/GODAS.input.36mon.1980_2015.nc',\\\n",
    "                         '/home/d/Q/saconvlstm/GODAS.label.12mon.1982_2017.nc', 24)\n",
    "\n",
    "indices = np.arange(trX.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "trX = trX[indices]\n",
    "trY = trY[indices]\n",
    "trY_decoder_input = trY_decoder_input[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d47488e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24, 72, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 24, 72, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 72, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 36, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 36, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 18, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 18, 256)        295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 6, 18, 256)        590080    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 18, 256)        590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6912)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              28315648  \n",
      "=================================================================\n",
      "Total params: 30,049,984\n",
      "Trainable params: 30,049,984\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-17 20:19:03.894921: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-17 20:19:03.914318: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3799900000 Hz\n",
      "2021-11-17 20:19:03.915067: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ca93c54500 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-17 20:19:03.915079: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "#vgg16\n",
    "input_cnn = Input(shape=(24,72,1))\n",
    "x = layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\")(input_cnn)\n",
    "x = layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.MaxPool2D(pool_size=(2,2),strides=(2,2))(x)\n",
    "\n",
    "x = layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.MaxPool2D(pool_size=(2,2),strides=(2,2))(x)\n",
    "\n",
    "x = layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.MaxPool2D(pool_size=(2,2),strides=(2,2))(x)\n",
    "\"\"\"\n",
    "x = layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.MaxPool2D(pool_size=(2,2),strides=(2,2))(x)\n",
    "\"\"\"\n",
    "\n",
    "#x = layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "#x = layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "#x = layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "#x = layers.MaxPool2D(pool_size=(2,2),strides=(2,2))(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = Dense(units=4096)(x)\n",
    "#Dense(units=4096,activation=\"relu\")\n",
    "outputs_cnn = x\n",
    "model_cnn = Model(input_cnn,outputs_cnn)\n",
    "model_cnn.summary()\n",
    "#plot_model(model_cnn, to_file='model_train.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f121fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = Input(shape=(12, 24, 72))\n",
    "_input1 = tf.reshape(_input, [-1, 24, 72])\n",
    "x = model_cnn(_input1)\n",
    "#x1 = model_cnn1(_input1)\n",
    "output = tf.reshape(x, [-1,12, 4096])\n",
    "#output1 = tf.reshape(x1, [-1,12, 50 * 18 * 6])\n",
    "#output = tf.concat([output,output1],2)#(None, 12, 6480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cec329bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>', U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 12, 24, 72)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 24, 72)]     0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "functional_1 (Functional)       (None, 4096)         30049984    tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 12, 4096)]   0           functional_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 12, 512), (N 9439232     tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 12, 512), (N 2099200     lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, 24, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, 12, 512), (N 2099200     lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, 24, 512), (N 1052672     decoder_inputs[0][0]             \n",
      "                                                                 lstm_5[0][1]                     \n",
      "                                                                 lstm_5[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, 24, 512), (N 524800      lstm_5[0][0]                     \n",
      "                                                                 decoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, 24, 1024)     0           decoder_lstm[0][0]               \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 24, 512)      524800      concat_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 24, 1)        513         time_distributed_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 45,790,401\n",
      "Trainable params: 45,790,401\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "25/44 [================>.............] - ETA: 42s - loss: 0.1168 - mae: 0.2195"
     ]
    }
   ],
   "source": [
    "time_steps_encoder=12\n",
    "#num_encoder_tokens=5400\n",
    "num_encoder_tokens=4096\n",
    "latent_dim=512\n",
    "time_steps_decoder=24\n",
    "#time_steps_decoder=3\n",
    "num_decoder_tokens=1\n",
    "\n",
    "encoder_inputs = output\n",
    "\"\"\"后为特征数，前为批次数量！！！\"\"\"\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(encoder_inputs)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder\n",
    "decoder_inputs = Input(shape=(time_steps_decoder, num_decoder_tokens), name= \"decoder_inputs\")\n",
    "# 我们设定我们的解码器回传整个输出的序列同时也回传内部的states参数\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "# 我们使用`encoder_states`来做为初始值(initial state) <-- 重要\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n",
    "\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "decoder_dense = TimeDistributed(Dense(latent_dim))\n",
    "decoder_outputs1 = decoder_dense(decoder_concat_input)\n",
    "\n",
    "decoder_dense2 = TimeDistributed(Dense(num_decoder_tokens))\n",
    "decoder_outputs2 = decoder_dense2(decoder_outputs1)\n",
    "\n",
    "# 定义一个模型接收encoder_input_data` & `decoder_input_data`做为输入而输出`decoder_target_data`\n",
    "model = Model([_input, decoder_inputs], decoder_outputs2)\n",
    "model.summary()\n",
    "#plot_model(model, to_file='model_train.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "\n",
    "epochs=100\n",
    "# Early Stopping\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "# Tensorboard callback\n",
    "#logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "# Run training\n",
    "#opt = keras.optimizers.Adam(lr = 0.0001)\n",
    "#RMSprop\n",
    "#当学习停滞时，减少2倍或10倍的学习率常常能获得较好的效果。\n",
    "x = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,patience=2,verbose=0,mode=\"auto\")\n",
    "\n",
    "model.compile(metrics=['mae'], optimizer='Adam', loss='mse') \n",
    "model.fit([trX[:2000], trY_decoder_input[:2000]], trY[:2000], batch_size=32, epochs=epochs, validation_split=0.3, verbose=1, callbacks=[earlystopping, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa86fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "# 对输入序列进行编码得到特征向量\n",
    "encoder_model = Model(_input,[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "# 下面的张量将保存上一个时间步的状态\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "#修改\n",
    "decoder_hidden_state_input = Input(shape=(time_steps_decoder,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "# 获取解码器序列的嵌入\n",
    "#dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))#注意是none (none, 1)\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "# 要预测序列中的下一个单词，请将初始状态设置为前一个时间步的状态\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_inputs, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "# 注意力 推理\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "# 用于生成概率分布的密集 softmax 层。 超过目标词汇\n",
    "decoder_outputs22 = decoder_dense(decoder_inf_concat)\n",
    "decoder_outputs33 = decoder_dense2(decoder_outputs22)\n",
    "\n",
    "# Final decoder model\n",
    "# 最终解码器模型\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs33] + [state_h2, state_c2])\n",
    "\n",
    "\n",
    "#预测\n",
    "def decode_predict(input_seq):\n",
    "    # 将输入编码为状态向量。\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq.reshape(-1, 12, 24, 72))\n",
    "    #print(input_seq.shape)\n",
    "    # 生成长度为 1 的空目标序列。\n",
    "    target_seq = np.zeros(( 1,1, num_decoder_tokens))#(1, 1, 1)\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    # 用起始词填充目标序列的第一个词。\n",
    "    target_seq[0, 0,0] = 0.\n",
    "\n",
    "    decoded_sentence = []\n",
    "    for i in range(24):\n",
    "        output, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        #print(output.shape)  (1,1,1)\n",
    "        \n",
    "        decoded_sentence.append(output[0,0,0])\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        # 更新目标序列（长度为 1）。\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0,0,0] = output\n",
    "        #print(target_seq)\n",
    "\n",
    "        # Update internal states\n",
    "        # 更新内部状态\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "out = []\n",
    "for i in range(testX.shape[0]):\n",
    "    print(\"test No.\" + str(i+1))\n",
    "    out.append(decode_predict(testX[i,:,:]))\n",
    "    \n",
    "cor = np.zeros((24))\n",
    "for i in range(24):\n",
    "    cor[i] = np.corrcoef(testY.reshape(384,24)[:,i],np.array(out)[:,i])[0,1]\n",
    "print(cor)\n",
    "print(\"sum :\" + str(np.sum(cor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165ac6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
